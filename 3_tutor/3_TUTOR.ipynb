{"cells":[{"cell_type":"markdown","metadata":{"id":"g3ivv9pWqOKZ"},"source":["#Put your Google Colab link here:\n","*your link here*"]},{"cell_type":"markdown","metadata":{"id":"yCS3Ib5GhmFP"},"source":["## Important notice: any use of generative AI for completing the assignment is strictly prohibited."]},{"cell_type":"markdown","metadata":{"id":"-yKoHry41pcF"},"source":["## Get access to a GPU:\n","To gain access to the GPUs on Colab, navigate to the `Runtime` tab above and select `Change runtime type`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"STrgYrSs1r61"},"outputs":[],"source":["# use if working in colab\n","import torch\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(device)\n"]},{"cell_type":"markdown","metadata":{"id":"JiW4BmCXpzkD"},"source":["## Import packages"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Chs3A5PHnA-r"},"outputs":[],"source":["import numpy as np\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import accuracy_score\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","metadata":{"id":"884ruHA4DqfR"},"source":["### Warning: to ensure the reproducibility of your results and to achieve the full grade, do not change or remove RANDOM_STATE variables and setting random seed statements. If you remove or change them, you may not get the full grade."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DtfXLAKk-mdY"},"outputs":[],"source":["random_state = 5"]},{"cell_type":"markdown","metadata":{"id":"cqXEcpCZh4pm"},"source":["## Part 1: Data Loading and Preprocessing (3 points)\n","\n","Objectives:\n","- Load and understand the dataset structure\n","- Implement data reduction strategy\n","- Prepare training/validation/test splits\n"]},{"cell_type":"markdown","metadata":{"id":"2agHdDzqEAbL"},"source":["### Part 1.1: Data Loading (1 point)\n","Load the dataset from Google Drive and check basic statistics\n","\n","- HINT: Use the provided dataloader function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fwh2EmjvFzX_"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# you should be added as viewer to shared Google drive \"ECE477 datasets\"\n","#  at https://drive.google.com/drive/u/0/folders/0ABIZHKB-QPnRUk9PVA\n","\n","\n","# the dataloader can load the original size data, and the reduced size data\n","def dataloader(file_path):\n","  data = np.load(file_path)\n","  X_train, y_train, X_validation, y_validation, X_test, y_test = (data['X_train'],data['y_train'],data['X_validation'],data['y_validation'],data['X_test'],data['y_test'])\n","\n","  return X_train, X_validation, X_test, y_train, y_validation, y_test\n","\n","file_path = '/content/drive/Shared drives/ECE477 datasets/Assignment3/Covertype_original_data.npz'\n","# Load the dataset\n","# YOUR CODE HERE\n","\n","\n","# check the shape of data\n","print(X_train.shape)\n","print(y_train.shape)\n","print(X_validation.shape)\n","print(y_validation.shape)\n","print(X_test.shape)\n","print(y_test.shape)\n"]},{"cell_type":"markdown","metadata":{"id":"Et2__rlVp-Le"},"source":["### Part 1.2: Data Reduction (2 point)\n","Implement data reduction based on compression ratio"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n5coI3fkF69C"},"outputs":[],"source":["# function to reduce the dataset size for our experiments\n","# We apply the dataset reduction to the training and validation set\n","# We randomly pick data points from the training and validation sets\n","def data_reduction(X_train, X_validation, y_train, y_validation, size_train, size_validation):\n","\n","  print('Data size reduction method')\n","\n","  # selecting random rows from the training set\n","  R = np.random.RandomState(random_state)\n","  rows = R.randint(X_train.shape[0], size=size_train)\n","\n","  # YOUR CODE HERE\n","\n","  print(f\"X_train.shape: {X_train.shape}\")\n","  print(f\"y_train.shape: {y_train.shape}\")\n","\n","  # selecting random rows from the validation set\n","  rows = R.randint(X_validation.shape[0], size=size_validation)\n","\n","  # YOUR CODE HERE\n","\n","  return X_train, X_validation, y_train, y_validation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NgyFqA33HRah"},"outputs":[],"source":["# We can use this part to generate reduced size datasets for various data compression ratios\n","# setting the data compression ratio\n","data_compression_ratio = 20\n","\n","# computing the size of the training and validation set based on the data compression ratio\n","size_train = int(X_train.shape[0]/data_compression_ratio)\n","size_validation = int(X_validation.shape[0]/data_compression_ratio)\n","\n","# Reduce the dataset size with the data_compression_ratio set above\n","# YOUR CODE HERE\n","\n","# Saving the reduced size data - we can name the data to include the data compression ratio\n","# Do not save to shared drive; save to your drive\n","reduced_file_path = # YOUR CODE HERE\n","np.savez(reduced_file_path,\n","    X_train=X_train_reduced,\n","    y_train=y_train_reduced,\n","    X_validation=X_validation_reduced,\n","    y_validation=y_validation_reduced,\n","    X_test=X_test, # The test set for the reduced size data is the same as the original data\n","    y_test=y_test\n","    )\n","print(\"reduced size data successfully saved.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dLKtdqXIU1cL"},"outputs":[],"source":["# load reduced data\n","X_train, X_validation, X_test, y_train, y_validation, y_test = dataloader(reduced_file_path)\n","print (\"Loading complete !\")"]},{"cell_type":"markdown","metadata":{"id":"Op7lfbM7HhE-"},"source":["## Part 2: Synthetic Data Generation (8 points)\n","Objectives:\n","- Implement Kernel Density Estimation (KDE) for synthetic data generation\n","- Validate synthetic data using semantic integrity classifer\n","- Label synthetic data with Random Forest"]},{"cell_type":"markdown","metadata":{"id":"2zsKn5JJH-3y"},"source":["### Part 2.1: KDE Implementation (4 points)\n","Complete the KDE sampling function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IbhrGgvWKnCz"},"outputs":[],"source":["from sklearn.neighbors import KernelDensity\n","\n","def KDE_sample_generation (X_train, X_validation):\n","\n","  # search over different bandwidth, find the best one\n","  bw_list = [0.5, 0.7, 1, 1.5, 2, 3]\n","\n","  log_like = np.zeros((len(bw_list)))\n","  for i, bw in enumerate(bw_list):\n","    print(f\"bw = {bw}\")\n","    # create KernelDensity model with: kernel='gaussian', bandwidth=bw\n","    # fit using training data\n","    # Compute the total log-likelihood of validation data under the model (check avalible functions in sklearn.neighbors.KernelDensity), and save in log_like\n","    # YOUR CODE HERE\n","\n","  bbw = bw_list[np.argmax(log_like)]\n","  print (f\"Best Bandwidth: {bbw}\")\n","\n","  # create model with Best Bandwidth, fit\n","  # YOUR CODE HERE\n","\n","  # sample 20000 samples from the model\n","  X_syn = # YOUR CODE HERE\n","  print(f\"X_syn.shape: {X_syn.shape}\")\n","\n","  for i in range(X_syn.shape[0]):\n","    wilderness_areas_sample = []\n","    max_value = float(\"-inf\")\n","    max_index = 0\n","    for j in range(10, 14):\n","      wilderness_areas_sample.append(X_syn[i,j])\n","      max_index = max (max_index, j)\n","\n","    for j in range(10, 14):\n","      if j == max_index:\n","        X_syn[i,j] = 1\n","      else:\n","        X_syn [i,j] = 0\n","\n","  return X_syn"]},{"cell_type":"markdown","metadata":{"id":"tReuLZxFgnKQ"},"source":["### Part 2.2: Semantic integrity classifer\n","Define semantic integrity classifer. Note that you don't need to run this function at the end."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zWLJxWTCUnjG"},"outputs":[],"source":["def softmax(vector):\n","  e = np.exp(vector)\n","  return (e / sum(e))\n","\n","# semantic integrity classifer predicts the value of one categorical feature with respect to continuous features of the data\n","# In this case, we had one categorical feature\n","# The categorical feature is one-hot encoded\n","def semantic_integrity_classifier(X_syn, X_train, X_validation):\n","\n","  X_train_total = np.concatenate((X_train, X_validation))\n","\n","  # indices for the continous columns for this dataset\n","  indices = np.array([list(np.arange(0,10))+list(np.arange(14,15))]).reshape(-1)\n","  print(indices)\n","\n","\n","  #finding the label for each data instance, the label is the value of te categorical column\n","  # if we have multiple categorical columns we need to do this multiple times\n","  # in this case the one-hot encoding of the categorical feauture is in columns 10-11-12-13.\n","  label = np.zeros((X_train_total.shape[0]))\n","  for i in range(X_train_total.shape[0]):\n","    wilderness_areas_sample = X_train_total [i, 10:14]\n","    wilderness_areas_sample = softmax(wilderness_areas_sample)\n","    index = np.argmax(wilderness_areas_sample)\n","    label [i] = index\n","\n","  # label train shows the value of the categorical feature in the training data\n","  label_train = np.zeros((X_train.shape[0]))\n","  for i in range(X_train.shape[0]):\n","    wilderness_areas_sample = X_train [i, 10:14]\n","    wilderness_areas_sample = softmax(wilderness_areas_sample)\n","    index = np.argmax(wilderness_areas_sample)\n","    # print (\"index: \", index)\n","    label_train[i] = index\n","\n","  # label validation shows the value of the categorical feature in the validation data\n","  label_validation = np.zeros((X_validation.shape[0]))\n","  for i in range(X_validation.shape[0]):\n","    wilderness_areas_sample = X_validation[i, 10:14]\n","    wilderness_areas_sample = softmax(wilderness_areas_sample)\n","    index = np.argmax(wilderness_areas_sample)\n","    # print (\"index: \", index)\n","    label_validation [i] = index\n","\n","  # Semantic integrity classifer\n","\n","  clf = RandomForestClassifier()\n","\n","  #computing the train and validation accuracy of the semantic integrity classifier\n","  #\n","  clf.fit(X_train[:, indices], label_train)\n","  y_pred_train = clf.predict(X_train[:, indices])  # Plug in values here!!\n","  train_acc = accuracy_score(label_train, y_pred_train)\n","  print('Train accuracy: {})'.format(train_acc))\n","\n","  # Predict the response for validation set\n","  y_pred_validation = clf.predict(X_validation[:, indices])  # Plug in values here!!!\n","  val_acc = accuracy_score(label_validation, y_pred_validation)\n","  print('Val accuracy: {})'.format(val_acc))\n","\n","  # Using all the data to train the final classifier\n","  clf.fit(X_train_total[:, indices], label)\n","  X_syn_final = np.zeros((1, X_syn.shape[1]))\n","  print(\"Shape of X_syn: \", X_syn.shape)\n","  print(\"Shape of X_syn_final: \", X_syn_final.shape)\n","\n","\n","  # we predict the value of categorical feature of the synthetic data with respect to continous features of the synthetic data\n","  # if not match, we disregard that synthetic data record\n","  for i in range(X_syn.shape[0]):\n","    predict = clf.predict(X_syn[i, indices].reshape(1, -1))\n","\n","    index = np.argmax(softmax(X_syn[i, 10:14]))\n","\n","    # if the value of the categorical value is semantically correct, we consider that synthetic data point\n","    if predict == index:\n","      for j in range(10, 14):\n","        if j == index+10:\n","          X_syn[i, j] = 1\n","        else:\n","          X_syn[i, j] = 0\n","\n","      X_syn_final = np.concatenate((X_syn_final, X_syn[i, :].reshape(1,-1)), axis= 0)\n","\n","  np.delete(X_syn_final, 0, 0)\n","  print (\"Shape of final X_syn: \", X_syn_final.shape)\n","\n","  return X_syn_final"]},{"cell_type":"markdown","metadata":{"id":"hxCYIlw8zaJo"},"source":["#### Use above functions to generate synthetic data (1 point)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YmgKE1EWWFja"},"outputs":[],"source":["# generating the synthetic data using KDE. This may take several minutes, please be patient\n","# YOUR CODE HERE"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HojpwWtrWJUw"},"outputs":[],"source":["# use semantic integrity classifer to verify the semantic integrity of the categorical feature values,\n","# you don't need to run this cell, because it takes too long. We will provide the synthetic data for later use\n","# YOUR CODE HERE"]},{"cell_type":"markdown","metadata":{"id":"NlufoU_aNsA9"},"source":["### Part 2.3: Synthetic data labeling (4 points)\n","- Implement Random Forest for synthetic data labeling\n","- Note that We will load the synthetic data from ECE477 datasets since the final part of synthetic data generation semantic_integrity_classifier() takes too long to run on Colab"]},{"cell_type":"markdown","metadata":{"id":"a2xQ8PUKpR4V"},"source":["#### Search for best max_depth (2 points)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OzGpfBudXivq"},"outputs":[],"source":["# load synthetic data\n","X_syn = np.load('/content/drive/Shared drives/ECE477 datasets/Assignment3/X_syn_original_1.npy')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QcvdFqwHOuDI"},"outputs":[],"source":["# Using the Random forest model to label the synthetic data\n","validation_accs = []\n","train_accs = []\n","\n","# go over different max_depth of Random forest\n","for i in range (1, 25):\n","  print(f\"training Random forest max_depth={i}\")\n","  clf = RandomForestClassifier(n_estimators=350, max_depth=i, criterion='gini', random_state=random_state)\n","\n","  # fit the model, and save training accuracy in train_accs, validation accuracy in validation_accs\n","  # YOUR CODE HERE"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hOHhpWTsnUvx"},"outputs":[],"source":["plt.title('Training and validation accuracy vs. Tree depth')\n","plt.plot(train_accs, label='Training accuracy')\n","plt.plot(validation_accs, label='Validation accuracy')\n","plt.legend()\n","plt.show()\n","\n","best_tree_id = # YOUR CODE HERE\n","best_tree_depth = best_tree_id + 1\n","print(f\"Baseline Random Forest depth={best_tree_depth}\")\n","print(f\"train acc={train_accs[best_tree_id]}\")\n","print(f\"validation acc={validation_accs[best_tree_id]}\")"]},{"cell_type":"markdown","metadata":{"id":"_8GklEf9n4cx"},"source":["#### Which max_depth is the best? Explain the reason. (1 point)\n","Your answer here"]},{"cell_type":"markdown","metadata":{"id":"oia5D1Uuoz3A"},"source":["#### fit a Random Forset with the best max_depth, and use it to label synthetic data (1 point)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SmNt6IqSnsT0"},"outputs":[],"source":["# after finding the best depth, we will retrain a random forset with both training and validation data\n","\n","X_train_total = np.concatenate((X_train, X_validation), axis=0)\n","y_train_total = np.concatenate((y_train, y_validation), axis=0)\n","\n","clf = RandomForestClassifier(n_estimators=350, max_depth=best_tree_depth, criterion='gini', random_state=random_state)\n","# fit the model with total data, and report training accuracy\n","# YOUR CODE HERE\n","print(f\"train acc={train_acc}\")\n","\n","# label the synthetic data with current model\n","y_syn = # YOUR CODE HERE\n","print(f\"y_syn.shape: {y_syn.shape}\")\n","\n"]},{"cell_type":"markdown","metadata":{"id":"dryuYx0gX6es"},"source":["## Part 3: Model Training (9 points)\n","Objective:\n","- Build and train neural network with Schema A\n","\n","Note that training may take 3-10 minutes to finish. You could train for fewer epochs when debugging"]},{"cell_type":"markdown","metadata":{"id":"xb-isABWZr-o"},"source":["### Part 3.1: Neural Network Architecture (2 points)\n","Complete the DynamicNet class definition"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"isrSCdEAjflJ"},"outputs":[],"source":["import torch.nn as nn\n","from torch.autograd import Variable\n","import torch.nn.functional as F\n","\n","# DNN model defination\n","class DynamicNet(torch.nn.Module):\n","  def __init__(self, D_in, H_1=200, H_2=100, D_out=7):\n","    super(DynamicNet, self).__init__()\n","    self.input_linear = nn.Linear(D_in, H_1)\n","    self.middle_linear = nn.Linear(H_1, H_2)\n","    self.output_linear = nn.Linear(H_2, D_out)\n","\n","  def forward(self, x):\n","    # Add F.leaky_relu as the activation function after self.input_linear and self.middle_linear.\n","    # input --> input_linear, activation, middle_linear, activation, output_linear --> output\n","    # YOUR CODE HERE\n"]},{"cell_type":"markdown","metadata":{"id":"RwNJK22Yam5J"},"source":["### Part 3.2: Baseline model training (3 points)\n","Train a DNN with reduced original data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HO3JgcpKscEW"},"outputs":[],"source":["# Set a fixed random seed for reproducibility\n","import random\n","random.seed(random_state)\n","np.random.seed(random_state)\n","torch.manual_seed(random_state)\n","if device == 'cuda':\n","  torch.cuda.manual_seed(seed)\n","  torch.cuda.manual_seed_all(seed)\n","  torch.backends.cudnn.deterministic = True\n","  torch.backends.cudnn.benchmark = False\n","\n","# training the baseline DNN model with reduced original data\n","# this may take several minutes\n","print (\"Baseline NN\")\n","\n","x = Variable(torch.from_numpy(X_train).type(torch.FloatTensor))\n","y = Variable(torch.from_numpy(y_train.reshape(-1)).type(torch.LongTensor))\n","\n","# put x and y to device\n","# YOUR CODE HERE\n","\n","# initialize model only with the input dimension (match the data)\n","# put model to device\n","# YOUR CODE HERE\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-2 )\n","\n","for t in range(1500):\n","  # train for 1500 epoch\n","  # YOUR CODE HERE\n","\n","\n","correct = 0\n","total = X_test.shape[0]\n","with torch.no_grad():\n","  for i in range (X_test.shape[0]):\n","    x_t = torch.from_numpy(X_test[i,:]).type(torch.FloatTensor)\n","    x_t = Variable(x_t)\n","    x_t = x_t.to(device)\n","\n","    y_t = torch.from_numpy(np.asarray(y_test[i])).type(torch.LongTensor)\n","    y_t = Variable(y_t)\n","    y_t = y_t.to(device)\n","\n","    output = model.forward(x_t)\n","    np_output = (output.cpu()).numpy()\n","    y_pred = np.argmax(np_output)\n","    label = int(y_test[i])\n","    if (y_pred == label):\n","      correct += 1\n","print ('Test Accuracy:', 100* correct/total)"]},{"cell_type":"markdown","metadata":{"id":"d4wuJiSXZm1Q"},"source":["### Part 3.3: Learning Scheme A (3 points)\n","Implement the two-phase training process: Scheme A uses the synthetic data to pretrain the network architecture. Pretraining is followed by use of the real training dataset to fine-tune."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N5TT25GFsure"},"outputs":[],"source":["# Set a fixed random seed for reproducibility\n","random.seed(random_state)\n","np.random.seed(random_state)\n","torch.manual_seed(random_state)\n","if device == 'cuda':\n","  torch.cuda.manual_seed(seed)\n","  torch.cuda.manual_seed_all(seed)\n","  torch.backends.cudnn.deterministic = True\n","  torch.backends.cudnn.benchmark = False\n","\n","# This method uses the synthetic data for DNN pretraining, and uses the real data for final training\n","print (\"Schema A\")\n","\n","# Loading the synthetic data\n","x= Variable(torch.from_numpy(X_syn).type(torch.FloatTensor))\n","y = Variable(torch.from_numpy(y_syn.reshape(-1)).type(torch.LongTensor))\n","\n","# put data to device\n","# YOUR CODE HERE\n","\n","# initialize model only with the input dimension (match the data)\n","# put model to device\n","# YOUR CODE HERE\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-2 )\n","\n","# stage 1: pretraining with the synthetic data\n","for t in range(500):\n","  # train for 500 epoch\n","  # YOUR CODE HERE\n","\n","\n","X_all = np.concatenate((X_train, X_validation))\n","y_all = np.concatenate((y_train, y_validation))\n","\n","x= Variable(torch.from_numpy(X_all).type(torch.FloatTensor))\n","y = Variable(torch.from_numpy(y_all.reshape(-1)).type(torch.LongTensor))\n","\n","# put data to device\n","# YOUR CODE HERE\n","\n","# stage 2: final training with real data\n","for t in range(1500):\n","  # train for 1500 epoch\n","  # YOUR CODE HERE\n","\n","\n","correct = 0\n","total = X_test.shape[0]\n","with torch.no_grad():\n","  for i in range (X_test.shape[0]):\n","    x_t = torch.from_numpy(X_test[i,:]).type(torch.FloatTensor)\n","    x_t = Variable(x_t)\n","    x_t = x_t.to(device)\n","\n","    y_t = torch.from_numpy(np.asarray(y_test[i])).type(torch.LongTensor)\n","    y_t = Variable(y_t)\n","    y_t = y_t.to(device)\n","\n","    output = model.forward(x_t)\n","    np_output = (output.cpu()).numpy()\n","    y_pred = np.argmax(np_output)\n","    label = int(y_test[i])\n","    if (y_pred == label):\n","      correct += 1\n","print ('Test Accuracy:', 100* correct/total)\n"]},{"cell_type":"markdown","metadata":{"id":"KC4VXrSKfW4R"},"source":["## Compare Sechema A with baseline, comment on the performance differnce. (1 point)\n","Your answer here"]}],"metadata":{"colab":{"provenance":[{"file_id":"14rMCW97sBCwXlEcSvlBUmK08h4qOu17r","timestamp":1739384759470}],"authorship_tag":"ABX9TyMxYFmLiKMDR7Pl2KY+zNb9"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}