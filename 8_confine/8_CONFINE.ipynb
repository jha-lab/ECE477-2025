{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3ivv9pWqOKZ"
      },
      "source": [
        "#Put your Google Colab link here:\n",
        "*your link here*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCS3Ib5GhmFP"
      },
      "source": [
        "## Important notice: any use of generative AI for completing the assignment is strictly prohibited."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yKoHry41pcF"
      },
      "source": [
        "## Get access to a GPU:\n",
        "To gain access to the GPUs on Colab, navigate to the `Runtime` tab above and select `Change runtime type`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STrgYrSs1r61",
        "outputId": "ae73bd31-bce7-499c-d2c5-abfeb8501169"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "# use if working in colab\n",
        "import torch\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "884ruHA4DqfR"
      },
      "source": [
        "### Warning: to ensure the reproducibility of your results and to achieve the full grade, do not change or remove RANDOM_STATE variables and setting random seed statements. If you remove or change them, you may not get the full grade."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DtfXLAKk-mdY"
      },
      "outputs": [],
      "source": [
        "random_state = 42"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Data Loading and Preprocessing (1 pt)"
      ],
      "metadata": {
        "id": "e63w_An__vel"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FYvXhXHZNKME"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-0Bd7WINMqP"
      },
      "outputs": [],
      "source": [
        "!unzip \"/content/drive/Shared drives/ECE477 datasets/Assignment8/coviddeep.zip\" -d \".\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tL8P2UXLNQrt"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "path_to_real_data = 'three-way_classification_real_data'\n",
        "\n",
        "# Loading the data\n",
        "X_train, y_train, X_validation, y_validation, X_test, y_test = \"\"\"TO DO\"\"\"\n",
        "print(X_train.shape, X_validation.shape, X_test.shape)\n",
        "print(np.unique(y_train))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Training (5 pts)"
      ],
      "metadata": {
        "id": "SVLb-MzV_4d9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 2.1: Neural Network Architecture (1 pt)\n",
        "finish DNN class defination\n"
      ],
      "metadata": {
        "id": "qzRrwh61ARPF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97SnWcsaQnGm"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Network and function definitions\n",
        "class DNN(torch.nn.Module):\n",
        "    def __init__(self, D_in, H_1, H_2, H_3, D_out):\n",
        "        super(DNN, self).__init__()\n",
        "        # define 4 linear layers\n",
        "        \"\"\"TO DO\"\"\"\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.leaky_relu(self.fc1(x))\n",
        "        x = F.leaky_relu(self.fc2(x))\n",
        "        x = F.leaky_relu(self.fc3(x))\n",
        "        y_pred = (self.fc4(x))\n",
        "        return y_pred\n",
        "\n",
        "    def get_embeddings(self, x):\n",
        "        # get the embedding right before the last linear layer\n",
        "        \"\"\"TO DO\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 2.2: Train DNN (3 pts)\n"
      ],
      "metadata": {
        "id": "5z5qpLHwBmcs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_KlZNNMOQmAv"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "# Random states\n",
        "random.seed(random_state)\n",
        "np.random.seed(random_state)\n",
        "torch.manual_seed(random_state)\n",
        "if device == 'cuda':\n",
        "  torch.cuda.manual_seed(random_state)\n",
        "  torch.cuda.manual_seed_all(random_state)\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "# Convert training and validation data into torch tensors, move to device\n",
        "# Remember to set correct data type\n",
        "\"\"\"TO DO\"\"\"\n",
        "\n",
        "# define D_in and D_out\n",
        "D_in = \"\"\"TO DO\"\"\"\n",
        "D_out = \"\"\"TO DO\"\"\"\n",
        "\n",
        "# define DNN parameters\n",
        "H_1 = 256\n",
        "H_2 = 128\n",
        "H_3 = 128\n",
        "\n",
        "# create DNN model, move to device\n",
        "\"\"\"TO DO\"\"\"\n",
        "\n",
        "# loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# training loop\n",
        "num_epochs = 2000\n",
        "train_accs = []\n",
        "val_accs = []\n",
        "\n",
        "best_val_acc = 0.0  # Track highest validation accuracy\n",
        "best_model_path = \"\"\"TO DO\"\"\"\n",
        "\n",
        "print(\"Training loop...\")\n",
        "for t in range(num_epochs):\n",
        "    # Training\n",
        "    \"\"\"TO DO\"\"\"\n",
        "\n",
        "    # Compute training accuracy\n",
        "    \"\"\"TO DO\"\"\"\n",
        "    train_accuracy = \"\"\"TO DO\"\"\"\n",
        "    train_accs.append(train_accuracy)\n",
        "\n",
        "    # Print loss, accuracies every 100 epochs\n",
        "    if (t + 1) % 100 == 0:\n",
        "        model.eval()  # Set model to evaluation mode\n",
        "        with torch.no_grad():\n",
        "            # Compute validation accuracy\n",
        "            \"\"\"TO DO\"\"\"\n",
        "            val_accuracy = \"\"\"TO DO\"\"\"\n",
        "            val_accs.append(val_accuracy)\n",
        "\n",
        "        print(f\"Epoch {t+1}/{num_epochs}, Loss: {loss.item():.4f}, Train Acc: {train_accuracy * 100:.2f}%, Val Acc: {val_accuracy * 100:.2f}%\")\n",
        "\n",
        "        # Save the model to best_model_path if validation accuracy is the highest, update best accuracy\n",
        "        if val_accuracy > best_val_acc:\n",
        "            \"\"\"TO DO\"\"\"\n",
        "            print(f\"New best model saved at epoch {t+1} with Val Acc: {best_val_acc * 100:.2f}%\")\n",
        "\n",
        "        model.train()  # Switch back to training mode\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 2.3: Evaluate model (1 pt)\n"
      ],
      "metadata": {
        "id": "6S7YyQ5hB1W7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wSP9buDQ4rjR"
      },
      "outputs": [],
      "source": [
        "# Load the best model, move to device, set to evaluation mode\n",
        "\"\"\"TO DO\"\"\"\n",
        "\n",
        "print(f\"Loaded best model from {best_model_path} for evaluation.\")\n",
        "\n",
        "# Convert test data into torch tensors, correct data type, move to device\n",
        "\"\"\"TO DO\"\"\"\n",
        "\n",
        "# Evaluate on test set\n",
        "with torch.no_grad():\n",
        "    \"\"\"TO DO\"\"\"\n",
        "\n",
        "print(f\"Test Accuracy of the Best Model: {test_accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: CONFINE implementation (12 pts)\n",
        "Please read CONFINE paper for details.\n",
        "- Read Algorithm 1 and section 3.2 to understand CONFINE, CONFINE-classwise, and calibration set.\n",
        "- Read section 4.2 to understand Evaluation Metrics\n",
        "\n",
        "Note: There's one thing different from the paper --- We don't append a softmax layer in this assignment."
      ],
      "metadata": {
        "id": "wxjEapz4CDO7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 3.1: prepare data (2 pts)\n",
        "\n",
        "- In order to improve model accuracy, we will remove training samples that are incorrectly classified by the pre-trained network\n",
        "- Feature embeddings for the processed training set, the calibration set, and the test set are extracted using the outputs from a certain layer of the pre-trained neural network. Here we use the embedding right before the final fully-connected layer.\n",
        "- Use validation set as calibration set.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JG9ThHBCCR5e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "isudALVW5SFM"
      },
      "outputs": [],
      "source": [
        "# Convert training data to torch tensors, ...\n",
        "\"\"\"TO DO\"\"\"\n",
        "\n",
        "# Make predictions on the training set\n",
        "with torch.no_grad():\n",
        "    # get predictions\n",
        "    # Find the indices of correctly predicted samples\n",
        "    \"\"\"TO DO\"\"\"\n",
        "    correct_indices = \"\"\"TO DO\"\"\"\n",
        "\n",
        "# Create new dataset with only correctly predicted points\n",
        "X_train_correct = \"\"\"TO DO\"\"\"\n",
        "y_train_correct = \"\"\"TO DO\"\"\"\n",
        "\n",
        "# Convert X_train_correct, X_validation, X_test into torch tensors, ...\n",
        "\"\"\"TO DO\"\"\"\n",
        "\n",
        "# Get embeddings using the model's get_embeddings() function\n",
        "# move to cpu, convert to numpy\n",
        "with torch.no_grad():\n",
        "    embeddings_train = \"\"\"TO DO\"\"\"\n",
        "    embeddings_cal = \"\"\"TO DO\"\"\"\n",
        "    embeddings_test = \"\"\"TO DO\"\"\"\n",
        "\n",
        "print(embeddings_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 3.2: conformal prediction method (8 pts)\n",
        "Define functions for conformal prediction."
      ],
      "metadata": {
        "id": "gRZ1RqDBCqx4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GcFCr6oEGe0Q"
      },
      "outputs": [],
      "source": [
        "def A_k(Ut, Yt, u, y, k=5):\n",
        "    \"\"\"\n",
        "    Computes the CONFINE nonconformity score using cosine distance.\n",
        "    - Ut: embeddings_train, the reference.\n",
        "    - Yt: y_train, the reference.\n",
        "    - u: test embedding.\n",
        "    - y: class label under consideration.\n",
        "    - k: number of nearest neighbors.\n",
        "\n",
        "    Returns:\n",
        "    - A_k score = (average cosine distance of k nearest neighbors with same label) /\n",
        "                  (average cosine distance of k nearest neighbors with different label)\n",
        "    \"\"\"\n",
        "\n",
        "    # Compute cosine distances to all training embeddings\n",
        "    cos_distances = np.zeros(len(Ut))  # Initialize the array\n",
        "    for i in range(len(Ut)):\n",
        "        # compute cosine distance between Ut[i] and u\n",
        "        cos_distances[i] = \"\"\"TO DO\"\"\"\n",
        "\n",
        "    # Separate distances based on class labels (whether Yt is the same with y)\n",
        "    same_class_distances = \"\"\"TO DO\"\"\"\n",
        "    diff_class_distances = \"\"\"TO DO\"\"\"\n",
        "\n",
        "    # Get the mean distance of k smallest distances in each group\n",
        "    avg_same_class_dist = np.mean(\"\"\"TO DO\"\"\")\n",
        "    avg_diff_class_dist = np.mean(\"\"\"TO DO\"\"\")\n",
        "\n",
        "    return avg_same_class_dist / avg_diff_class_dist\n",
        "\n",
        "def compute_nonconformity_cal(embeddings_train, y_train, embeddings_cal, y_cal, k=5):\n",
        "    \"\"\"\n",
        "    Computes nonconformity scores for the calibration set.\n",
        "    - embeddings_train: Ut, the reference.\n",
        "    - y_train: Yt, the reference.\n",
        "    - embeddings_cal: Uc, embeddings from calibration set.\n",
        "    - y_cal: Yc, labels from calibration set.\n",
        "    - k: Number of nearest neighbors.\n",
        "    \"\"\"\n",
        "    alpha_c = np.zeros(len(y_cal))\n",
        "    for i in range(len(y_cal)):\n",
        "        # compute Ak((Ut, Yt), ui, yi), where (ui, yi)âˆˆ(Uc, Yc)\n",
        "        alpha_c[i] = \"\"\"TO DO\"\"\"\n",
        "    return alpha_c\n",
        "\n",
        "def compute_p_value(embeddings_train, y_train, y_cal, test_embedding, alpha_c, k=5, classwise=False):\n",
        "    \"\"\"\n",
        "    Implements the CONFINE Algorithm for a given test embedding.\n",
        "    - embeddings_train: Ut, embeddings from training set.\n",
        "    - y_train: Yt, labels from training set.\n",
        "    - y_cal: Yc, labels from calibration set.\n",
        "    - test_embedding: u, embedding of test sample.\n",
        "    - k: Number of nearest neighbors.\n",
        "    - classwise: Whether to use CONFINE-classwise computation.\n",
        "\n",
        "    Return:\n",
        "    - p_value: Dictionary where keys are class labels, and values are the p-values for each class.\n",
        "    \"\"\"\n",
        "    unique_classes = np.unique(y_train)\n",
        "\n",
        "    p_value = {}\n",
        "\n",
        "    # Compute nonconformity scores and p-values for each class, based on the test embedding\n",
        "    for y_j in unique_classes:\n",
        "        # compute Ak((Ut, Yt), u, y_j)\n",
        "        alpha_test = \"\"\"TO DO\"\"\"\n",
        "\n",
        "        if classwise:\n",
        "            # Use only calibration samples from class y_j\n",
        "            mask = \"\"\"TO DO\"\"\"\n",
        "        else:\n",
        "            # Use all calibration samples\n",
        "            mask = \"\"\"TO DO\"\"\"\n",
        "        # p_value for y_j: p(y_j)\n",
        "        p_value[y_j] = \"\"\"TO DO\"\"\"\n",
        "\n",
        "\n",
        "    return p_value\n",
        "\n",
        "\n",
        "def compute_predictions(p_value, epsilon=0.05):\n",
        "    \"\"\"\n",
        "    Computes the prediction set, prediction, credibility, and confidence based on the p-values.\n",
        "\n",
        "    - p_value: Dictionary where keys are class labels, and values are the p-values for each class.\n",
        "    - epsilon: Significance level threshold for determining which classes belong to the prediction set.\n",
        "\n",
        "    Returns:\n",
        "    - prediction_set: List of class labels that meet the threshold for inclusion in the prediction set (p-value > epsilon).\n",
        "    - prediction: The class label with the highest p-value.\n",
        "    - credibility: The highest p-value corresponding to the final prediction.\n",
        "    - confidence: 1 - second-highest p-value\n",
        "    \"\"\"\n",
        "    prediction_set = \"\"\"TO DO\"\"\"\n",
        "\n",
        "    # Sort p_value by the p-value of each class\n",
        "    sorted_p = \"\"\"TO DO\"\"\"\n",
        "    prediction = \"\"\"TO DO\"\"\"  # Class with highest p-value\n",
        "    credibility = \"\"\"TO DO\"\"\" # Highest p-value\n",
        "\n",
        "\n",
        "    confidence = \"\"\"TO DO\"\"\" # 1 - second highest p-value\n",
        "    return prediction_set, prediction, credibility, confidence\n",
        "\n",
        "\n",
        "def evaluate(p_values, y_test, epsilon=0.05):\n",
        "    \"\"\"\n",
        "    Evaluates the performance of the model based on p-values for each test sample.\n",
        "\n",
        "    - p_values: A list of dictionaries containing p-values for each class, one for each test sample.\n",
        "    - y_test: Ground truth labels for the test dataset.\n",
        "    - epsilon: Significance level threshold used to form the prediction set.\n",
        "\n",
        "    The function computes and prints three evaluation metrics:\n",
        "    - Accuracy: Proportion of correct predictions.\n",
        "    - Correct Efficiency: Proportion of predictions where the true class is the only class in the prediction set.\n",
        "    - Coverage: Proportion of predictions where the true class is included in the prediction set.\n",
        "    \"\"\"\n",
        "    accuracy = 0\n",
        "    correct_efficiency = 0\n",
        "    coverage = 0\n",
        "    for y, p_value in zip(y_test, p_values):\n",
        "        prediction_set, prediction, credibility, confidence = \"\"\"TO DO\"\"\"\n",
        "        # update accuracy, correct_efficiency, coverage\n",
        "        \"\"\"TO DO\"\"\"\n",
        "\n",
        "    print(f\"Accuracy: {accuracy / len(y_test):.4f}\")\n",
        "    print(f\"Correct Efficiency: {correct_efficiency / len(y_test):.4f}\")\n",
        "    print(f\"Coverage: {coverage / len(y_test):.4f}\")\n",
        "    return\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 3.3: Run CONFINE on test dataset (2 pts)\n",
        "Use previous defined functions. Use k=5, epsilon=0.1."
      ],
      "metadata": {
        "id": "29mkx9HBC52I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute alpha_c for calibration set, use k=5\n",
        "# Please check the dimentions of input data before you use the functions\n",
        "# this may take 5 mins\n",
        "\"\"\"TO DO\"\"\"\n",
        "alpha_c = \"\"\"TO DO\"\"\"\n"
      ],
      "metadata": {
        "id": "UYF8zjsHmxp6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute p_values of test set, using CONFINE\n",
        "# this may take 15 mins\n",
        "p_values = []\n",
        "for test_embedding in embeddings_test:\n",
        "    p_value = \"\"\"TO DO\"\"\"\n",
        "    p_values.append(p_value)\n",
        "\n",
        "# evaluate the result using evaluate(), set epsilon=0.1\n",
        "\"\"\"TO DO\"\"\""
      ],
      "metadata": {
        "id": "v-vwUtPKpiAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute p_values of test set, using CONFINE-classwise\n",
        "# this may take 15 mins\n",
        "p_values_class = []\n",
        "for test_embedding in embeddings_test:\n",
        "    p_value = \"\"\"TO DO\"\"\"\n",
        "    p_values_class.append(p_value)\n",
        "\n",
        "# evaluate the result using evaluate(), set epsilon=0.1\n",
        "\"\"\"TO DO\"\"\""
      ],
      "metadata": {
        "id": "KEtJAYHL-TlR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 4: Questions (2 pts)"
      ],
      "metadata": {
        "id": "V3ReU6v_EYQI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Compare the performance of DNN, CONFINE, and CONFINE-classwise methods. Discuss any perfomance difference observed.\n",
        "\n",
        "your answer here"
      ],
      "metadata": {
        "id": "LmP_iYy7GYRy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. How do Accuracy, Correct Efficiency, and Coverage change as epsilon increases from 0 to 1? Explain the reason.\n",
        "\n",
        "your answer here"
      ],
      "metadata": {
        "id": "rhqsPfa-GvkI"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}