{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make cells wider if working locally in jupyter\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important notice: any use of generative AI for completing the assignment is strictly prohibited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use that if working in colab\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you should be added as viewer to shared Google drive \"ECE477 datasets\"\n",
    "#  at https://drive.google.com/drive/u/0/folders/0ABIZHKB-QPnRUk9PVA\n",
    "\n",
    "!unzip \"/content/drive/Shared drives/ECE477 datasets/Assignment2/data.zip\" -d \".\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warning: to ensure the reproducibility of your results and to achieve the full grade, do not change or remove RANDOM_STATE variables and setting random seed statements. If you remove or change them, you may not get the full grade. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "RANDOM_STATE = 0\n",
    "np.random.seed(RANDOM_STATE)\n",
    "random.seed(RANDOM_STATE)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some util functions\n",
    "\n",
    "def neuronMask(mask, add_index, func='in', sparse=False, ratio=0.2):\n",
    "    if sparse:\n",
    "        num = int(mask.size(0)*ratio)\n",
    "        if func == 'in':\n",
    "            for i in add_index:\n",
    "                indices = list(range(mask.size(0)))\n",
    "                np.random.shuffle(indices)\n",
    "                for j in indices[:num]:\n",
    "                    mask[j, i] = 1\n",
    "        elif func == 'out':\n",
    "            for i in add_index:\n",
    "                indices = list(range(mask.size(1)))\n",
    "                np.random.shuffle(indices)\n",
    "                for j in indices[:num]:\n",
    "                    mask[i, j] = 1\n",
    "    else:\n",
    "        if func == 'in':\n",
    "            for i in add_index:\n",
    "\t            mask[:, i] = 1\n",
    "        elif func == 'out':\n",
    "            for i in add_index:\n",
    "                mask[i, :] = 1\n",
    "    return mask\n",
    "\n",
    "\n",
    "def load_data_train(data_path='./data/sonar/', mode='all', fold=None):\n",
    "    if fold != None:\n",
    "        X_train = np.load(data_path + \"X_train\" + str(fold) + '.npy')\n",
    "        y_train = np.load(data_path + \"y_train\" + str(fold) + '.npy')\n",
    "        X_test = np.load(data_path + \"X_validation\"+ str(fold) + '.npy')\n",
    "        y_test = np.load(data_path + \"y_validation\"+ str(fold) + '.npy')\n",
    "         \n",
    "    else:\n",
    "        if mode == 'all':\n",
    "            X_train = np.load(data_path + \"X_train.npy\")\n",
    "            y_train = np.load(data_path + \"y_train.npy\")\n",
    "        elif mode == 'easy':\n",
    "            X_train = np.load(data_path + \"X_easy.npy\")\n",
    "            y_train = np.load(data_path + \"y_easy.npy\")\n",
    "        elif mode == 'hard':\n",
    "            X_train = np.load(data_path + \"X_hard.npy\")\n",
    "            y_train = np.load(data_path + \"y_hard.npy\")\n",
    "        \n",
    "        X_test = np.load(data_path + \"X_validation.npy\")\n",
    "        y_test = np.load(data_path + \"y_validation.npy\")\n",
    "    \n",
    "    X_train = X_train.astype(float)\n",
    "    X_test = X_test.astype(float)\n",
    "    y_train = y_train.astype(int)\n",
    "    y_test = y_test.astype(int)\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "def load_data_test(data_path='./data/sonar/', fold=None):\n",
    "    if fold != None:\n",
    "        X_train = np.load(data_path + \"X_train\" + str(fold) + '.npy')\n",
    "        y_train = np.load(data_path + \"y_train\" + str(fold) + '.npy')\n",
    "        X_test = np.load(data_path + \"X_test\"+ str(fold) + '.npy')\n",
    "        y_test = np.load(data_path + \"y_test\"+ str(fold) + '.npy')\n",
    "        \n",
    "         \n",
    "    else:\n",
    "        X_train = np.load(data_path + \"X_train_total.npy\")\n",
    "        X_test = np.load(data_path + \"X_test.npy\")\n",
    "        y_train = np.load(data_path + \"y_train_total.npy\")\n",
    "        y_test = np.load(data_path + \"y_test.npy\")\n",
    "\n",
    "    X_train = X_train.astype(float)\n",
    "    X_test = X_test.astype(float)\n",
    "    y_train = y_train.astype(int)\n",
    "    y_test = y_test.astype(int)\n",
    "    \n",
    "\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Deep Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SDNN(object):\n",
    "    def __init__(self, in_num, out_num, init_size, max_size, dataset, batch_size, **kwargs):\n",
    "        self.dataset = dataset # the name of the dataset\n",
    "        self.in_num = in_num # The number of input features\n",
    "        self.out_num = out_num # The number of output classes\n",
    "        self.init_size = init_size # the inital number of hidden neurons \n",
    "        self.max_size = max_size  # max number of allowed hidden neurons in the architecture\n",
    "        self.cur_size = init_size - 1  # current size of hidden neurons\n",
    "        self.batch_size = batch_size # Batch size\n",
    "        self.name = dataset \n",
    "        self.flag = 0\n",
    "        \n",
    "        self.epoch = 0\n",
    "        self.best_acc = 0\n",
    "        self.best_acc_prune = 0\n",
    "        self.now_acc = 0\n",
    "        self.connection_count = 0\n",
    " \n",
    "\n",
    "    # Forward pass  \n",
    "    # we first compute the hidden activations by utilizing the input to hidden weights (w1) , and hidden to hidden weights,\n",
    "    # (w2) and their corresponding mask matrices (m1, m2), as well as the hidden biases (b1)\n",
    "    # After computing the hidden activations, we use it alongside hidden to out weight matrix (w3) and corresponding \n",
    "    # mask (m3), and input to output connections (w4) and its corresponding mask (m4), and output biases (b2)\n",
    "    # at the end we return the computed output\n",
    "    def forward(self, x):\n",
    "        self.hidden = torch.zeros(x.size(0), self.max_size)\n",
    "\n",
    "        for i,j in enumerate(self.active_index):\n",
    "            self.hidden[:, j] = F.relu((torch.mm(self.hidden.clone(), torch.mul(self.w2[:, j], self.m2[:, j]).view(-1, 1)) \n",
    "                                       + torch.mm(x, torch.mul(self.w1[:, j], self.m1[:, j]).view(-1,1))\n",
    "                                       + self.b1[:, j])).squeeze(1)\n",
    "\n",
    "        out = torch.mm(self.hidden, torch.mul(self.w3, self.m3)) \\\n",
    "                  + torch.mm(x, torch.mul(self.w4, self.m4)) \\\n",
    "                  + self.b2\n",
    "        out.retain_grad()\n",
    "        return out\n",
    "    \n",
    "    \n",
    "    # used in connection growth \n",
    "    def forwardMask(self, display=True):\n",
    "        for i,j in enumerate(self.active_index):\n",
    "            mask_idx = list(set(range(self.max_size)) - set(self.active_index[:i]))\n",
    "            self.m2.data[:, j][mask_idx] = 0\n",
    "        if display:    \n",
    "            print('Forward mask, m2: %d' %np.count_nonzero(self.m2.data))\n",
    "            \n",
    "    \n",
    "    def backwardGrad(self, outgrad):\n",
    "        self.hidden.grad = torch.mm(outgrad, torch.t(self.w3))\n",
    "        rev_idx = np.flip(self.active_index, axis=0)\n",
    "        for i,j in enumerate(rev_idx):\n",
    "            for k in range(i):\n",
    "                self.hidden.grad.data[:, j] = self.hidden.grad.data[:, j] + self.hidden.grad.data[:, k] \\\n",
    "                                                  *self.w2.data[j, k]  \n",
    "                \n",
    "\n",
    "    def displayConnection(self, display=True):\n",
    "        \"\"\"it shows the number of active weights in m1, m2, m3, and m4 masks\"\"\"\n",
    "        m1 = 0\n",
    "        m2 = 0\n",
    "        m3 = 0\n",
    "        m4 = np.count_nonzero(self.m4.data)\n",
    "        for i,j in enumerate(self.active_index):\n",
    "            m1 += np.count_nonzero(self.m1.data[:, j])\n",
    "            m3 += np.count_nonzero(self.m3.data[j, :])\n",
    "            for k in range(i):\n",
    "                m2 += np.count_nonzero(self.m2.data[self.active_index[k]][j])\n",
    "        \n",
    "        if display:\n",
    "            print('Connection Info: ')\n",
    "            print('m1: %d, m2: %d, m3: %d, m4: %d' %(m1,m2,m3,m4))\n",
    "            print('Total: %d' % (m1+m2+m3+m4))\n",
    "        return m1, m2, m3, m4, m1+m2+m3+m4\n",
    "\n",
    "\n",
    "    def save_checkpoint(self, state, is_best, folder_to_save, filename = '_checkpoint.pth.tar'):\n",
    "        name_to_save = os.path.join(folder_to_save, self.name + filename)\n",
    "        torch.save(state, name_to_save)\n",
    "        if is_best:\n",
    "            shutil.copyfile(name_to_save, os.path.join(folder_to_save, self.name + '_model_best.pth.tar'))\n",
    "            print(f\"also saved as the best checkpoint to {os.path.join(folder_to_save, self.name + '_model_best.pth.tar')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define SDNN class memebers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: handle validation data (1 point)\n",
    "\n",
    "fill in the missing code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the data\n",
    "# mode:train -> loading train and validation data, we use this mode in training the model and learning the \n",
    "# architecture\n",
    "# mode: test - > loading train_total (train+validation set) and test data\n",
    "# we use the test mode after learning the architecture to evaluate the model on the test set\n",
    "\n",
    "def loadData(self, folder_to_load='data', mode='train', fold=None):\n",
    "    path_to_load = os.path.join(folder_to_load, self.dataset) + '/'\n",
    "    if mode == 'train':\n",
    "        self.X_train, self.y_train, self.X_validation, self.y_validation = load_data_train(path_to_load)\n",
    "    elif mode == 'test':\n",
    "        self.X_train, self.y_train, self.X_validation, self.y_validation = load_data_test(path_to_load)\n",
    "        \n",
    "    # converting the numpy arrays to torch tensors\n",
    "    self.X_train = torch.tensor(self.X_train, dtype=torch.float32)\n",
    "    self.y_train = torch.tensor(self.y_train.reshape(-1), dtype=torch.long)\n",
    "    # create validation splits from numpy arrays\n",
    "    # your code is here\n",
    "    self.X_validation = ...\n",
    "    self.y_validation = ...\n",
    "\n",
    "    # creating the data loaders for train\n",
    "    self.traindata = torch.utils.data.TensorDataset(self.X_train, self.y_train)\n",
    "    self.trainloader = torch.utils.data.DataLoader(self.traindata, batch_size=self.batch_size, shuffle=True)\n",
    "    \n",
    "    # creating the data loaders for validation\n",
    "    # don't shuffle validation data!\n",
    "    # your code is here\n",
    "    self.validationdata = ...\n",
    "    self.validationloader = ...\n",
    "    \n",
    "    \n",
    "SDNN.loadData = loadData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: define optimizer (2 points)\n",
    "\n",
    "Define SGD optimizer on all network weights and biases. \n",
    "\n",
    "Parameters: lr=0.001, momentum=0.9, weight_decay=1e-4, nesterov=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing the structure\n",
    "# four main components and their corresponding masks\n",
    "# We define weight matrices, and masks, for connection between input neurons to hidden neurons,\n",
    "# hidden neurons to hidden neurons, hidden neurons to output neurons, and input neurons to output neurons\n",
    "# We also define bias terms for hidden neurons and output neurons\n",
    "\n",
    "def structureInit(self, load=False, sparse=True, ratio=0.2, file=None):\n",
    "    # input to hidden\n",
    "    self.w1 = torch.randn(self.in_num, self.max_size) * 0.1\n",
    "    self.m1 = torch.zeros(self.in_num, self.max_size)\n",
    "    # hidden to hidden\n",
    "    self.w2 = torch.randn(self.max_size, self.max_size) * 0.1\n",
    "    self.m2 = torch.ones(self.max_size, self.max_size)\n",
    "    # hidden to output\n",
    "    self.w3 = torch.randn(self.max_size, self.out_num) * 0.1\n",
    "    self.m3 = torch.zeros(self.max_size, self.out_num)\n",
    "    # input to output\n",
    "    self.w4 = torch.randn(self.in_num, self.out_num) * 0.1\n",
    "    self.m4 = torch.ones(self.in_num, self.out_num)\n",
    "\n",
    "    self.b1 = torch.zeros(1, self.max_size)\n",
    "    self.b2 = torch.zeros(1, self.out_num)\n",
    "\n",
    "    self.w1.requires_grad = True\n",
    "    self.w2.requires_grad = True\n",
    "    self.w3.requires_grad = True\n",
    "    self.w4.requires_grad = True\n",
    "    self.b1.requires_grad = True\n",
    "    self.b2.requires_grad = True\n",
    "\n",
    "    self.params = {'w1': self.w1, 'w2': self.w2, 'w3': self.w3, 'w4': self.w4,\n",
    "                   'm1': self.m1, 'm2': self.m2, 'm3': self.m3, 'm4': self.m4,\n",
    "                  }\n",
    "\n",
    "    self.criterion = nn.CrossEntropyLoss()\n",
    "    # your code is here\n",
    "    # clarification: predefined self.params are intended for different purpose\n",
    "    # don't pass it into the optimizer\n",
    "    self.optimizer = ...\n",
    "\n",
    "    # starting from scratch\n",
    "    # at first, we have init_size active hidden neurons\n",
    "    # neuronMask is a helper function defined in utils.py\n",
    "    # This function sets the appropriate number of mask values equal to 1\n",
    "    # note that in the initialization step, we only activate connections between input to hidden (m1)\n",
    "    # and hidden to output (m3), and biases for hidden neurons (b1)\n",
    "    if load == False:\n",
    "        self.active_index = list(range(self.init_size))\n",
    "        if sparse:\n",
    "            self.m1.data = neuronMask(self.m1.data, self.active_index, sparse=True, ratio=ratio)\n",
    "        else:\n",
    "            self.m1.data = neuronMask(self.m1.data, self.active_index)\n",
    "        self.m3.data = neuronMask(self.m3.data, self.active_index, 'out')\n",
    "        self.b1.data = neuronMask(self.b1.data, self.active_index)\n",
    "\n",
    "    # loading from a pretrained model\n",
    "    # we have to load all the parameters of the model, including the index of active neurons\n",
    "    # and all the learned weight, bias, and mask matrices\n",
    "    else:\n",
    "        checkpoint = torch.load(file, weights_only=True)\n",
    "        self.active_index = checkpoint['active_index']\n",
    "        self.w1.data = checkpoint['state_dict']['w1']\n",
    "        self.m1.data = checkpoint['state_dict']['m1']\n",
    "        self.w2.data = checkpoint['state_dict']['w2']\n",
    "        self.m2.data = checkpoint['state_dict']['m2']\n",
    "        self.w3.data = checkpoint['state_dict']['w3']\n",
    "        self.m3.data = checkpoint['state_dict']['m3']\n",
    "        self.w4.data = checkpoint['state_dict']['w4']\n",
    "        self.m4.data = checkpoint['state_dict']['m4']\n",
    "        self.b1.data = checkpoint['state_dict']['b1']\n",
    "        self.b2.data = checkpoint['state_dict']['b2']\n",
    "        self.epoch = checkpoint['epoch']\n",
    "        self.best_acc = checkpoint['best_acc']\n",
    "        self.now_acc = checkpoint['now_acc']\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        \n",
    "        \n",
    "SDNN.structureInit = structureInit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(self, duration=10, folder_to_save='tmp'):\n",
    "    for epoch in range(self.epoch, self.epoch+duration): #loop over the dataset multiple times based on #epochs\n",
    "        running_loss = 0.0\n",
    "        # reading the data using the data loaders defined earlier\n",
    "        for i, data in enumerate(self.trainloader, 0):\n",
    "            # get the inputs\n",
    "            inputs, labels = data\n",
    "            inputs.requires_grad_(True)\n",
    "            # zero the parameter gradients\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = self.forward(inputs)\n",
    "            loss = self.criterion(outputs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            # computing the running loss\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # updating the weight matrices\n",
    "            self.w1.data = self.w1.data * self.m1.data\n",
    "            self.w2.data = self.w2.data * self.m2.data\n",
    "            self.w3.data = self.w3.data * self.m3.data\n",
    "            self.w4.data = self.w4.data * self.m4.data\n",
    "\n",
    "        # computing the train accuracy\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for i, data in enumerate(self.trainloader, 0):\n",
    "            inputs, labels = data\n",
    "            #inputs = inputs.view(inputs.size(0), -1)\n",
    "            outputs = self.forward(inputs) \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct += (predicted == labels.data).sum()\n",
    "            total += labels.size(0)\n",
    "        train_acc = correct * 1. / total\n",
    "\n",
    "        # computing the validation accuracy\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for i, data in enumerate(self.validationloader, 0):\n",
    "            inputs, labels = data\n",
    "            outputs = self.forward(inputs) \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct += (predicted == labels.data).sum()\n",
    "            total += labels.size(0)\n",
    "        validation_acc = correct * 1. /total\n",
    "        self.now_acc = validation_acc\n",
    "\n",
    "        # saving the model if the validation accuracy is better than the current best validation accuracy \n",
    "        # of the pruned model\n",
    "        # we use the save_checkpoint function defined later to save the model and all the important parameters\n",
    "        # you can change the filename as you wish\n",
    "        if (validation_acc > self.best_acc_prune) and (self.flag == 1):\n",
    "            self.best_acc_prune = validation_acc\n",
    "            self.save_checkpoint({\n",
    "                'epoch': epoch,\n",
    "                'best_acc': self.best_acc,\n",
    "                'now_acc': self.now_acc,\n",
    "                'state_dict': {\n",
    "                    'w1': self.w1.data,\n",
    "                    'm1': self.m1.data,\n",
    "                    'w2': self.w2.data,\n",
    "                    'm2': self.m2.data,\n",
    "                    'w3': self.w3.data,\n",
    "                    'm3': self.m3.data,\n",
    "                    'w4': self.w4.data,\n",
    "                    'm4': self.m4.data,\n",
    "                    'b1': self.b1.data,\n",
    "                    'b2': self.b2.data,                    \n",
    "                },\n",
    "                'active_index': self.active_index,\n",
    "                'optimizer': self.optimizer.state_dict(),\n",
    "            }, False, folder_to_save, filename='_prune.pth.tar')\n",
    "\n",
    "        if (validation_acc > self.best_acc):\n",
    "            self.best_acc = validation_acc\n",
    "            self.save_checkpoint({\n",
    "                'epoch': epoch + 1,\n",
    "                'best_acc': self.best_acc,\n",
    "                'now_acc': self.now_acc,\n",
    "                'state_dict': {\n",
    "                    'w1': self.w1.data,\n",
    "                    'm1': self.m1.data,\n",
    "                    'w2': self.w2.data,\n",
    "                    'm2': self.m2.data,\n",
    "                    'w3': self.w3.data,\n",
    "                    'm3': self.m3.data,\n",
    "                    'w4': self.w4.data,\n",
    "                    'm4': self.m4.data,\n",
    "                    'b1': self.b1.data,\n",
    "                    'b2': self.b2.data,                    \n",
    "                },\n",
    "                'active_index': self.active_index,\n",
    "                'optimizer': self.optimizer.state_dict(),\n",
    "            }, True, folder_to_save)\n",
    "        else:\n",
    "            self.save_checkpoint({\n",
    "                'epoch': epoch,\n",
    "                'best_acc': self.best_acc,\n",
    "                'now_acc': self.now_acc,\n",
    "                'state_dict': {\n",
    "                    'w1': self.w1.data,\n",
    "                    'm1': self.m1.data,\n",
    "                    'w2': self.w2.data,\n",
    "                    'm2': self.m2.data,\n",
    "                    'w3': self.w3.data,\n",
    "                    'm3': self.m3.data,\n",
    "                    'w4': self.w4.data,\n",
    "                    'm4': self.m4.data,\n",
    "                    'b1': self.b1.data,\n",
    "                    'b2': self.b2.data,                    \n",
    "                },\n",
    "                'active_index': self.active_index,\n",
    "                'optimizer': self.optimizer.state_dict(),\n",
    "            }, False, folder_to_save)\n",
    "        print('Epoch: %d, Training accuracy: %f, Validation accuracy: %f' \n",
    "              % (epoch, train_acc, validation_acc))\n",
    "\n",
    "        m1,m2,m3,m4,m_all = self.displayConnection(display=False)\n",
    "\n",
    "    self.epoch += duration\n",
    "    \n",
    "    \n",
    "SDNN.train = train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell division function, we have the options between activation based, gradient-based, and random cell-division\n",
    "# We normally use activation-based (duplicating the cell with the highest activation value) or\n",
    "# random cell division (randomly selecting a hidden cell to be duplicated)\n",
    "# We can make this decisions either by looking at the full data, or a batch of data\n",
    "# Other than mode, the other inputs are num (shows number of neurons to be duplicated)\n",
    "# full_data that shows whether or not to use the full data for neuron selection\n",
    "# and if full data is flase, size shows how many batches to use to compute the neuron actications\n",
    "\n",
    "def cellDivision(self, mode='acti', num=1, full_data=False, size=1):\n",
    "    '''\n",
    "    Function: add neurons.\n",
    "    Arguments:\n",
    "        mode: 'acti' activation-based,'grad' gradient-based, 'rand' random\n",
    "        num: number of neurons added each time\n",
    "        full_data: whether to use full data to decide which neuron to split\n",
    "        size: if full_data=False, number of batches used to decide which neuron to split\n",
    "    '''\n",
    "\n",
    "    # computing the hidden activation values, either by using the whole data or several batches of data\n",
    "    # we sum up the hidden activations for several batches\n",
    "    if mode == 'acti':\n",
    "        activation = np.zeros(self.max_size)\n",
    "        if full_data:\n",
    "            for i, data in enumerate(self.trainloader, 0):\n",
    "                inputs,_ = data\n",
    "                self.forward(inputs) \n",
    "                activation += torch.sum(torch.abs(self.hidden.data), 0)\n",
    "        else:\n",
    "            loader = iter(self.trainloader)\n",
    "            for i in range(size):\n",
    "                inputs,_ = next(loader)\n",
    "                self.forward(inputs) \n",
    "                activation += torch.sum(torch.abs(self.hidden.data), 0).cpu().numpy()\n",
    "\n",
    "        # selecting the neurons with the highest activations to be duplicated\n",
    "        # we select 'num' neurons to be duplicated\n",
    "        max_index_arr = np.flip(np.argsort(activation)[-num:], axis=0)\n",
    "    elif mode == 'grad':\n",
    "\n",
    "        # selecting the neurons to be activated based on the hidden gradients\n",
    "        # we did not use this method in the final experiments of the paper\n",
    "        # however, it is worth exploring\n",
    "        # we use the function badwardGrad defined later to compute gradients\n",
    "        activation = np.zeros(self.max_size)\n",
    "        if full_data:\n",
    "            for i, data in enumerate(self.trainloader, 0):\n",
    "                inputs, labels = data\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.forward(inputs)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                self.backwardGrad(outputs)\n",
    "                activation += torch.sum(self.hidden.grad.data, 0)\n",
    "        else:\n",
    "            loader = iter(self.trainloader)\n",
    "            for i in range(size):\n",
    "                inputs, labels = loader.next()\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.forward(inputs)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "\n",
    "                self.backwardGrad(outputs)\n",
    "                activation += torch.sum(self.hidden.grad.data, 0)\n",
    "        max_index_arr = np.flip(np.argsort(activation)[-num:], axis=0)\n",
    "    elif mode == 'rand':\n",
    "        # selection 'num' neurons random from active neurons\n",
    "        max_index_arr = np.random.choice(self.active_index, size=num, replace=False)\n",
    "\n",
    "    # after selecting the neuron to be duplicated, we duplicate that neuron and its connections, \n",
    "    # and add noise to weights of the new added neuron\n",
    "\n",
    "    for max_index in max_index_arr:\n",
    "        # we add the index at the end of active_index list of active neurons\n",
    "        add_index = len(self.active_index)\n",
    "        # current size\n",
    "        self.cur_size = add_index\n",
    "        print('Max index: %d' %max_index)\n",
    "\n",
    "        # we only add a new neurons if the number of neurons will be less than the maximum number of neurons\n",
    "        # set at the beginning\n",
    "        if add_index < self.max_size:\n",
    "            print('Adding neuron: %d' %add_index)\n",
    "            python_max_index = int(max_index)  # Convert to Python int\n",
    "            if python_max_index in self.active_index:\n",
    "                self.active_index.insert(self.active_index.index(python_max_index), add_index)\n",
    "                # duplicating the masks\n",
    "                self.m1.data[:, add_index] = self.m1.data[:, python_max_index]\n",
    "                self.m2.data[:, add_index] = self.m2.data[:, python_max_index]\n",
    "                self.m3.data[add_index, :] = self.m3.data[python_max_index, :]\n",
    "\n",
    "                # duplicating the weight matrices and adding noise\n",
    "                self.w1.data[:, python_max_index] = self.w1.data[:, python_max_index]\n",
    "                self.w1.data[:, add_index] = self.w1.data[:, python_max_index] + torch.randn(self.in_num) * 0.01\n",
    "                self.w2.data[:, add_index] = self.w2.data[:, python_max_index] + torch.randn(self.max_size) * 0.01\n",
    "                self.w3.data[add_index, :] = self.w3.data[python_max_index, :] + torch.randn(self.out_num) * 0.01\n",
    "                self.b1.data[:, add_index] = self.b1.data[:, python_max_index]\n",
    "            else:\n",
    "                print(f\"Value {python_max_index} not found in active_index. Skipping insertion due to error.\")\n",
    "    \n",
    "    # updating the weight matrices\n",
    "    self.w1.data = self.w1.data * self.m1.data\n",
    "    self.w2.data = self.w2.data * self.m2.data\n",
    "    self.w3.data = self.w3.data * self.m3.data\n",
    "    self.w4.data = self.w4.data * self.m4.data\n",
    "    self.displayConnection()\n",
    "    \n",
    "    \n",
    "SDNN.cellDivision = cellDivision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: update correlation matrix and masking weights (6 points)\n",
    "\n",
    "Fill in the missing code where prompted (2 spots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addConnection(self, mode='grad', percentile={'m2':90, }, size=1, full_data=False):\n",
    "    '''\n",
    "    Function: add connections.\n",
    "    Arguments:\n",
    "        mode: 'corr' correlation-based, 'grad' gradient-based, 'rand' random\n",
    "        percentile: top-k percentile of connections are added\n",
    "    '''\n",
    "    print('\\nAdding connection...')\n",
    "    self.flag = 0\n",
    "\n",
    "    cov_mat = {\n",
    "        'm1': np.zeros([self.in_num, self.max_size]),\n",
    "        'm2': np.zeros([self.max_size, self.max_size]),\n",
    "        'm3': np.zeros([self.max_size, self.out_num]),\n",
    "        'm4': np.zeros([self.in_num, self.out_num]),\n",
    "    }\n",
    "\n",
    "    # gradient-based growth \n",
    "    # we use the backwardGrad function to compute gradients\n",
    "    if mode == 'grad':\n",
    "        loader = iter(self.trainloader)\n",
    "        for i in range(size):\n",
    "            inputs, lables = next(loader)\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.forward(inputs)\n",
    "\n",
    "            loss = self.criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            self.backwardGrad(outputs)\n",
    "\n",
    "            cov_mat_m1 = torch.mm(inputs.T, self.hidden.grad)\n",
    "            cov_mat_m2 = torch.mm(self.hidden.T, self.hidden.grad)\n",
    "            cov_mat_m3 = torch.mm(self.hidden.T, outputs.grad)\n",
    "            cov_mat_m4 = torch.mm(inputs.T, outputs.grad)\n",
    "\n",
    "            # add to covariance matrix values cov_mat_m1, cov_mat_m2, cov_mat_m3, cov_mat_m4\n",
    "            # your code is here\n",
    "            cov_mat['m1'] += \n",
    "            cov_mat['m2'] += \n",
    "            cov_mat['m3'] += \n",
    "            cov_mat['m4'] += \n",
    "\n",
    "    elif mode == 'rand':\n",
    "        cov_mat['m1'][:, :self.cur_size] = np.random.rand(self.in_num, self.cur_size)\n",
    "        cov_mat['m2'][:self.cur_size, :self.cur_size] = np.random.rand(self.cur_size, self.cur_size)\n",
    "        cov_mat['m3'][:self.cur_size, :] = np.random.rand(self.cur_size, self.out_num)\n",
    "        cov_mat['m4'] = np.random.rand(self.in_num, self.out_num)\n",
    "\n",
    "\n",
    "    for i in percentile:\n",
    "        if len(np.nonzero(cov_mat[i])[0]) == 0:\n",
    "            threshold = 0\n",
    "        else:\n",
    "            threshold = np.percentile(cov_mat[i][np.nonzero(cov_mat[i])], percentile[i])\n",
    "        self.params[i].data[torch.Tensor(cov_mat[i])>threshold] = 1\n",
    "\n",
    "    self.forwardMask()\n",
    "    self.displayConnection()\n",
    "\n",
    "    # self.m1, self.m2, self.m3, self.m4 are masks for correspondings weights.\n",
    "    # they are float tensors containing 1. and 0. values\n",
    "    # update weights masking out corresponding values.\n",
    "    # Impprtant: For weights and masks tensors in calculation, call their .data() property\n",
    "    # to prevent tracking gradients on these tensors by torch autograd system\n",
    "\n",
    "    # your code is here\n",
    "    self.w1 = \n",
    "    self.w2 =\n",
    "    self.w3 =\n",
    "    self.w4 =\n",
    "    \n",
    "    \n",
    "    \n",
    "SDNN.addConnection = addConnection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: fill in validation accuracy calculation (2 points)\n",
    "Fill in the missing code to calculate the validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def displayAcc(self):\n",
    "    \"\"\"computing and displaying the train and test accuracy\"\"\"\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for i, data in enumerate(self.trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        outputs = self.forward(inputs) \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct += (predicted == labels.data).sum()\n",
    "        total += labels.size(0)\n",
    "    print('Train: %d/%d' %(correct, total))\n",
    "    train_acc = correct * 1. / total\n",
    "\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    \n",
    "    # your code is here\n",
    "    # hint: use validationloader\n",
    "    \n",
    "    \n",
    "\n",
    "    print('Train accuracy: %f, Test accuracy: %f'\n",
    "          % (train_acc, validation_acc))\n",
    "    return validation_acc\n",
    "\n",
    "\n",
    "SDNN.displayAcc = displayAcc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name of the dataset (folder) in the data directory\n",
    "dataset = 'arrhythmia0'\n",
    "\n",
    "expr = '_1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: define the number of features and classes for the loaded dataset (1 points)\n",
    "\n",
    "These are required to correctly define number of input and output neurons in the network.\n",
    "\n",
    "Display both in_num and out_num numbers explicitly to get the full credit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test dataset load to define number of features and classes only\n",
    "# you don't need this further, since SDNN class loads its own dataset instance\n",
    "\n",
    "test_dataset_load = load_data_train(f'./data/{dataset}/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code is here\n",
    "\n",
    "# number of input features\n",
    "in_num = \n",
    "# number of output classes\n",
    "out_num = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperaparameters\n",
    "\n",
    "params = {\n",
    "    'init_num': 5,   # initial number of hidden neurons\n",
    "    'max_num': 100,  # max number of hidden neurons\n",
    "    'sparse': False, \n",
    "    'sparse_ratio': 0.1,   \n",
    "    'loop_num': 10,   # number of iterations in growth and pruning applications\n",
    "    'full_data': False,  # for small data use full_data : True to use all the data\n",
    "    'remove': False,  \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructive approach\n",
    "\n",
    "This is a constructive approach. We start by having a small number of hidden neurons in the\n",
    "architecture (‘init_size’). The starting number of neurons in the architecture is a hyperparameter\n",
    "that can be chosen based on a task at hand and by looking at the validation performance. In an\n",
    "iterative process, we apply connection growth and neuron growth to grow the network size. We\n",
    "have a max number of neurons that we cannot surpass (one of the parameters).\n",
    "\n",
    "\n",
    "Each of these architecture changing functions have their own respective hyperapameters that\n",
    "can be optimized by monitoring validation accuracy (these are explained in comments in the\n",
    "code). The number of epochs to train the architecture, the number of iterations to change the\n",
    "architecture, the number of neurons to add at each neuron growth operation, the o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create folder to store checkpoints\n",
    "os.makedirs('record_full', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: run training (5 points)\n",
    "\n",
    "**if training finishes successfully, it outputs \"Training is done\" at the end of iterations. You will be able to see that both training and validation accuracies changes with adding new connections.**\n",
    "\n",
    "Create a new model from scratch and train it.\n",
    "\n",
    "Make sure to store the training results in checkpoint folder created right above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============\n",
    "# Random states:\n",
    "np.random.seed(RANDOM_STATE)\n",
    "random.seed(RANDOM_STATE)\n",
    "\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "torch.cuda.manual_seed(RANDOM_STATE)\n",
    "# ==============\n",
    "\n",
    "\n",
    "\n",
    "# create the SDNN object using params dict. Set batch_size to 256.\n",
    "# your code is here\n",
    "sdnet = ...\n",
    "\n",
    "\n",
    "#starting the model from scratch\n",
    "sdnet.structureInit(load=False, sparse=params['sparse'], ratio=params['sparse_ratio'])\n",
    "name = dataset + expr + '_1'\n",
    "sdnet.name = name\n",
    "\n",
    "\n",
    "# loading the data in train mode\n",
    "sdnet.loadData(mode='train')\n",
    "\n",
    "# train the architecture for 10 epochs first, before applying the growth and pruning.\n",
    "# save data to the checkpoint folder\n",
    "# your code is here\n",
    "...\n",
    "\n",
    "# iterative GP in the loop\n",
    "for i in range(params['loop_num']):\n",
    "    # adding connections, mode, and percentile can be explored\n",
    "    sdnet.addConnection(mode='grad', percentile={'m2':70, 'm1':70, 'm3':70, 'm4':70}, full_data=False)\n",
    "    \n",
    "    # after adding connections, train the architecture for 10 epochs, save data to the checkpoint folder\n",
    "    # your code is here\n",
    "    ...\n",
    "    \n",
    "    # neuron growth operation\n",
    "    sdnet.cellDivision(full_data=params['full_data'])\n",
    "    # after architecture changing operation is applied, again\n",
    "    # train the architecture for 10 epochs, save data to the checkpoint folder\n",
    "    # your code is here\n",
    "    ...\n",
    "    sdnet.train(params['interval'], 'record_full')\n",
    "\n",
    "print(\"Training is done\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the best model learned in the iterative process\n",
    "\n",
    "During training, we are seeking to maximize the validation accuracy.\n",
    "\n",
    "Here you will see the best model that you got during iterative growing neurons and connections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: report the best validation accuracy (3 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code is here\n",
    "\n",
    "# load the SDNN object from the best checkpoint\n",
    "# hint: search for SDNN member function that loads from checkpoint \n",
    "# your code is here\n",
    "\n",
    "# display the active connections and train and validation accuracies\n",
    "# for the best checkpoint\n",
    "# hint: use a predefined functions for that\n",
    "# your code is here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scann",
   "language": "python",
   "name": "scann"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
