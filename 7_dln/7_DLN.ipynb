{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3ivv9pWqOKZ"
      },
      "source": [
        "#Put your Google Colab link here:\n",
        "*your link here*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### This notebook demonstrates the process of training and evaluating a Differentiable Logic Network (DLN) for classification using the Heart Disease Kaggle dataset. It covers the steps of data preparation, including preprocessing, scaling, and feature reordering. It then shows how to train a DLN model, evaluate its performance, and visualize the learned network."
      ],
      "metadata": {
        "id": "cNCtbMV4I25x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 0. Prepare Python Environment"
      ],
      "metadata": {
        "id": "OLVgRYf-yM2R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install/Download packages\n",
        "\n",
        "# Clone the DLN repo\n",
        "!git clone https://github.com/chang-yue/dln.git\n",
        "\n",
        "# cd to DLN folder\n",
        "%cd dln/quickstart"
      ],
      "metadata": {
        "id": "OVKvimLsxQsN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1. Prepare Dataset (14 points)\n",
        "\n",
        "#### The processed datasets and related information will be saved in the data/datasets/NAME/seed_{SEED}/data directory:\n",
        "- `train.csv` and `test.csv` (store features and the target class).\n",
        "- `data_info.json` (stores dataset information such as feature data types and scaling).\n",
        "\n",
        "#### The columns of the datasets should follow these standards:\n",
        "- Features should be ordered as categorical features, then continuous features, then the target.\n",
        "- Features should be scaled between 0 and 1.\n",
        "- The target column should be named “Target” and labeled from 0 up to (num_classes – 1).\n",
        "- Try to avoid using characters other than letters or underscores in feature names."
      ],
      "metadata": {
        "id": "4rytWWuXsBvt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import os\n",
        "import sys\n",
        "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath('__file__'))))\n",
        "from data.data_utils import *\n",
        "pd.options.mode.chained_assignment = None\n",
        "pd.set_option('display.max_columns', None)"
      ],
      "metadata": {
        "id": "PO1j_YOflDkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.1 Download dataset (4 points)\n",
        "* Use Linux command to complete this step. In Google Colab, prefix the command with ! so it runs as a shell command instead of Python code. For example: !pip install numpy\n",
        "* We will use the Heart Disease Kaggle dataset"
      ],
      "metadata": {
        "id": "qnIvd4Z5teWe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the dataset ZIP to example/data_raw/Heart/\n",
        "# Create the directory example/data_raw/Heart\n",
        "\"\"\"TO DO\"\"\"\n",
        "\n",
        "# download the dataset from https://www.kaggle.com/api/v1/datasets/download/cherngs/heart-disease-cleveland-uci\n",
        "# save it as example/data_raw/Heart/heart-disease-cleveland-uci.zip\n",
        "\"\"\"TO DO\"\"\"\n",
        "\n",
        "# Unzip the downloaded file into example/data_raw/Heart/, then delete the ZIP file.\n",
        "\"\"\"TO DO\"\"\"\n",
        "\n",
        "# Read the .csv file into a pandas DataFrame\n",
        "df = \"\"\"TO DO\"\"\"\n",
        "\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "1bhPOYi9yk7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.2 Check missing values, analyze class distribution, and split data (6 points)"
      ],
      "metadata": {
        "id": "nQ-_C_Mqtlpv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values\n",
        "\n",
        "print(df.isnull().sum().sum())"
      ],
      "metadata": {
        "id": "i1CgKSHqyk-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing columns\n",
        "# Make categorical features one-hot\n",
        "\n",
        "oh_list = [\"cp\", \"restecg\", \"slope\", \"thal\", \"ca\"]\n",
        "# check features in oh_list\n",
        "for _f in oh_list:\n",
        "  print(f\"{_f}: {np.unique(df[_f], return_counts=True)}\")\n",
        "\n",
        "# drop 'restecg' == 1 since it contains only 4 samples\n",
        "df.drop(df[df[\"restecg\"]==1].index, inplace=True)\n",
        "\n",
        "# change columns in oh_list to object type\n",
        "\"\"\"TO DO\"\"\"\n",
        "\n",
        "# create one-hot encoding for columns in oh_list\n",
        "# use the functuion pd.get_dummies(); get k-1 dummies out of k categorical levels by removing the first level.\n",
        "\"\"\"TO DO\"\"\"\n",
        "\n",
        "# drop the original columns in oh_list\n",
        "\"\"\"TO DO\"\"\"\n",
        "\n",
        "# join the one-hot encoded columns\n",
        "\"\"\"TO DO\"\"\"\n",
        "\n",
        "\n",
        "# reset index\n",
        "df.reset_index(inplace=True, drop=True)\n",
        "\n",
        "# Assign the column name of the target feature as \"Target\"\n",
        "df.rename(columns={\"condition\":\"Target\"}, inplace=True)\n",
        "\n",
        "print('\\ndata shape: ', df.shape, sep='')\n",
        "print('\\nclass distribution:\\n', df.Target.value_counts(), sep='')\n",
        "# print('\\ncolumn types:\\n', df.dtypes, sep='')\n",
        "\n",
        "# visualize the data\n",
        "# df should contain multiple columns for each categorical feature, and the original columns should be removed\n",
        "# for example, the column \"cp\" should be removed and replaced by \"cp_1\", \"cp_2\", \"cp_3\"\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "diZXE_WxylEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort features into the [categorical, continuous, target] order\n",
        "\n",
        "continuous_features = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']\n",
        "\n",
        "# other features are categorical\n",
        "categorical_features = \"\"\"TO DO\"\"\"\n",
        "\n",
        "print('continuous_features:', continuous_features)\n",
        "print('\\ncategorical_features:', categorical_features)\n",
        "\n",
        "# Reindex columns to [cat, con, label]\n",
        "df = \"\"\"TO DO\"\"\"\n",
        "print(df.head())\n",
        "\n",
        "dtype_dict = df.dtypes.to_dict()"
      ],
      "metadata": {
        "id": "0Yjjd_D8fbQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Shuffle and split data into train/(val)/test\n",
        "seed = 0\n",
        "\n",
        "train_fraction = 0.75 ###\n",
        "df_train, df_test = shuffle_split_data(df, train_fraction, seed=seed)\n",
        "\n",
        "print('train:', df_train.shape)\n",
        "print(np.unique(df_train.Target, return_counts=True))\n",
        "print('\\ntest:', df_test.shape)\n",
        "print(np.unique(df_test.Target, return_counts=True))"
      ],
      "metadata": {
        "id": "PfVMocL3mcHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.3 Visualize training data"
      ],
      "metadata": {
        "id": "2WO1lh1iucUj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot histograms of the training data\n",
        "\n",
        "ncol, nrow = 2, int(np.ceil(len(df_train.columns)/2))\n",
        "figsize = (16,3*nrow)\n",
        "\n",
        "plot_hist(df_train, figsize, nrow, ncol)"
      ],
      "metadata": {
        "id": "orHQesr-hD0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.4 Clip outliers, scale features, and then save the processed data and info (4 points)"
      ],
      "metadata": {
        "id": "US5zVkrdumzU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature outlier clipping and [0, 1] scaling\n",
        "\n",
        "for feature in continuous_features:\n",
        "    # clip outliers to 0.5th and 99.5th percentiles\n",
        "    # get 0.5th percentile and 99.5th percentile of current feature\n",
        "    \"\"\"TO DO\"\"\"\n",
        "\n",
        "    # set values below 0.5th percentile to the 0.5th percentile, and set values above 99.5th percentile to the 99.5th percentile\n",
        "    # do it for both training and testing data\n",
        "    \"\"\"TO DO\"\"\"\n",
        "\n",
        "\n",
        "scaler_list = [MinMaxScaler(clip=True), MinMaxScaler(clip=True)]\n",
        "feature_list = [continuous_features, categorical_features]\n",
        "df_train_scaled, df_test_scaled, scaler_params = scale_features(df_train, df_test, feature_list, scaler_list)"
      ],
      "metadata": {
        "id": "LpPeHltWhtSe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the scaled training data\n",
        "\n",
        "plot_hist(df_train_scaled, figsize, nrow, ncol)"
      ],
      "metadata": {
        "id": "-DP5IAhKhtUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the processed data and feature information\n",
        "\n",
        "# Save data into the data/datasets/Heart/seed_0/data directory\n",
        "# scaler_params and dtype_dict are used for network visualization\n",
        "parent_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
        "folderpath = f'{parent_dir}/data/datasets/Heart/seed_{seed}/data'\n",
        "save_data(folderpath, continuous_features, categorical_features, scaler_params, dtype_dict, df_train_scaled, df_test_scaled)"
      ],
      "metadata": {
        "id": "vpYh4Fb_xW2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2. Training and Evaluation (3 points)\n",
        "\n",
        "#### Let's use the dataset we just prepared. We present a general use case here. For more advanced functions such as pruning, freezing, and the unified phase, see the descriptions in `experiments/main.py`.\n",
        "\n",
        "#### For training, use ```--train_model``` flag. For evaluation, use ```--evaluate_model``` flag, which loads the model and evaluates its balanced-class accuracy. It then attempts to simplify the model using SymPy before evaluating the model’s high-level OPs, basic gate-level OPs, number of parameters, and disk space usage. If simplification is successful, the simplified model is used for these evaluations.\n",
        "\n",
        "#### Please check how to use command in [DLN repo](https://github.com/chang-yue/dln). Ensure you use the same parameters as specified in the DLN repo readme."
      ],
      "metadata": {
        "id": "uWemoLHzJKMX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cd to the dln directory\n",
        "import os\n",
        "\"\"\"TO DO\"\"\""
      ],
      "metadata": {
        "id": "gzmfsERqEq04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"TO DO\"\"\"\n",
        "\n",
        "# Training:\n",
        "# last_hidden_layer_size = first_hidden_layer_size x last_hl_size_wrt_first\n",
        "# The middle hidden layers will have sizes in a geometric progression from the first to the last layer\n",
        "# Will save the model with the best mean train + val balanced-class accuracy"
      ],
      "metadata": {
        "id": "KCqM61WmEjjy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the eval results\n",
        "\n",
        "import json\n",
        "from experiments.utils import *\n",
        "\n",
        "results_path = get_results_path(dataset='Heart', seed=0)\n",
        "with open(f\"{results_path}/eval_results.json\", 'r') as f:\n",
        "    data = json.load(f)\n",
        "print(json.dumps(data, indent=4))"
      ],
      "metadata": {
        "id": "fXYW74xyEjoa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3. Visualization (3 points)\n",
        "\n",
        "#### We use Graphviz to render DLNs generated from SymPy code."
      ],
      "metadata": {
        "id": "VbCrVnnDJn5M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python experiments/DLN_viz.py \\\n",
        "results/Heart/seed_0/sympy_code.py \\\n",
        "quickstart/example/viz\n",
        "\n",
        "# A file named viz.png will be created"
      ],
      "metadata": {
        "id": "2KkXYJuTEx8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "Image(filename='quickstart/example/viz.png')"
      ],
      "metadata": {
        "id": "3q3RdHVfE2i5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### How many continuous features and categorical features are there in the dataset? how many of them does the DLN use? (3 points)\n",
        "\n",
        "*your answer here*"
      ],
      "metadata": {
        "id": "m0HzQgWtJxqW"
      }
    }
  ]
}