{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "zn1gxz5lpHd5"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3ivv9pWqOKZ"
      },
      "source": [
        "#Put your Google Colab link here:\n",
        "*your link here*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Introduction\n",
        "This assignment provides hands-on experience querying different Large Language Models (LLMs) through a cloud API provider (Nebius AI). You will focus on a simplified medical Question Answering task, using sample questions from MedMCQA, a large-scale, Multiple-Choice Question Answering (MCQA) dataset designed to address real-world medical entrance exam questions.\n",
        "\n",
        "The goal of this assignment is to develop practical skills in interacting with LLM APIs, comparing the capabilities of different open-source models (including those potentially fine-tuned for biomedical domains), exploring the impact of generation parameters, and evaluating their potential and limitations in the healthcare context.\n"
      ],
      "metadata": {
        "id": "cNCtbMV4I25x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### API platform\n",
        "We will use Nebius AI Studio (https://studio.nebius.com/).\n",
        "\n",
        "Nebius AI offers new users $1 in free credits, which should be sufficient to complete this assignment. You can monitor your usage in the Nebius AI dashboard.\n"
      ],
      "metadata": {
        "id": "4wTkvYA1NHZP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Setup"
      ],
      "metadata": {
        "id": "OLVgRYf-yM2R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Install Libraries:"
      ],
      "metadata": {
        "id": "KxROr7EzPkS6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this cell to install the OpenAI library\n",
        "!pip install openai -q"
      ],
      "metadata": {
        "id": "OVKvimLsxQsN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Obtain Nebius AI API Key:\n",
        "\n",
        "- Sign up or log in to Nebius AI Studio: https://studio.nebius.com/\n",
        "\n",
        "- Navigate to API keys to generate an API key.\n",
        "- Important: Treat your API key like a password. Do not share it publicly. Please **delete the key** before you submit the assignmnet.\n",
        "\n"
      ],
      "metadata": {
        "id": "zn1gxz5lpHd5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3 Configure API Client\n",
        "Initialize the OpenAI client to use the Nebius AI API endpoint and your key."
      ],
      "metadata": {
        "id": "6xpM8RPlpr-q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        "# TODO: Paste your Nebius AI API Key here\n",
        "# Important: delete your API key before submission\n",
        "NEBIUS_API_KEY = \"\"\"TO DO\"\"\"\n",
        "\n",
        "# Initialize the client to connect to Nebius AI\n",
        "client = OpenAI(\n",
        "    base_url=\"https://api.studio.nebius.com/v1/\", # Nebius AI endpoint\n",
        "    api_key=NEBIUS_API_KEY)\n"
      ],
      "metadata": {
        "id": "Y3xSOGNHpfKM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Load Dataset (3 points)\n",
        "Install Hugging Face datasets Library: We need this library to load the MedMCQA dataset."
      ],
      "metadata": {
        "id": "AQsl3MKhQBM-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this cell to install the datasets library\n",
        "# It's OK if you see some dependency conflicts\n",
        "!pip install datasets"
      ],
      "metadata": {
        "id": "2mcqaI4nqEKn",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load and Prepare [MedMCQA](https://huggingface.co/datasets/openlifescienceai/medmcqa) Dataset: We will load the validation split of the openlifescienceai/medmcqa dataset from Hugging Face, shuffle it, and select the first 100 examples for our assignment to manage API costs. We will construct the prompt based on the dataset."
      ],
      "metadata": {
        "id": "Uo-0pvyoR3K4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "# Define a seed for reproducibility\n",
        "SEED = 42\n",
        "\n",
        "# Load the validation split from openlifescienceai/medmcqa\n",
        "print(\"Loading MedMCQA dataset (validation split)...\")\n",
        "\"\"\"TO DO\"\"\"\n",
        "\n",
        "# Shuffle the dataset using SEED\n",
        "print(\"Shuffling dataset...\")\n",
        "\"\"\"TO DO\"\"\"\n",
        "\n",
        "# Select the first 100 examples\n",
        "print(\"Selecting first 100 examples...\")\n",
        "medmcqa_sample = \"\"\"TO DO\"\"\"\n",
        "\n",
        "# Process the data into queries and true answers\n",
        "queries = []\n",
        "true_answers = []\n",
        "\n",
        "for example in medmcqa_sample:\n",
        "    query = f\"Question:\\n{\"\"\"TO DO\"\"\"}\\n\\nOptions:\\nA. {\"\"\"TO DO\"\"\"}\\nB. {\"\"\"TO DO\"\"\"}\\nC. {\"\"\"TO DO\"\"\"}\\nD. {\"\"\"TO DO\"\"\"}\"\n",
        "    answer = \"\"\"TO DO\"\"\" # keep it as it is (0, 1, 2, 3)\n",
        "    # append into list\n",
        "    \"\"\"TO DO\"\"\"\n",
        "\n",
        "# check an example\n",
        "print('query 1:')\n",
        "print(queries[1])\n",
        "print('answer 1:')\n",
        "print(true_answers[1])"
      ],
      "metadata": {
        "id": "QWaqS_i7SB1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: Query Different Models (13 points)\n",
        "\n",
        "We will use 3 different models available on Nebius AI: Llama 3.1 8B, Llama 3.3 70B, and OpenBioLLM 70B.\n",
        "\n",
        "Llama models are developed by Meta for general tasks, and [OpenBioLLM](https://huggingface.co/aaditya/Llama3-OpenBioLLM-70B) is finetuned from Llama for medical/biological contexts."
      ],
      "metadata": {
        "id": "niIAR9JuqEKd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Define the function to get LLM response from API. (3 points)\n",
        "You can check the example code in Nebius ai website --> Playground --> view code"
      ],
      "metadata": {
        "id": "kZRglAqHNlmN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "def query_llm(model_name, system_prompt, prompt, temperature=0.6, max_tokens=1000, top_p=1):\n",
        "    \"\"\"\n",
        "    Sends a prompt to a specified OpenAI chat model and returns the generated response.\n",
        "\n",
        "    Parameters:\n",
        "        model_name (str): The name of the model.\n",
        "        system_prompt (str): A message that sets the behavior or tone of the assistant.\n",
        "        prompt (str): The user's input or question for the model to respond to.\n",
        "        temperature (float, optional): set temperature for generation.\n",
        "        max_tokens (int, optional): The maximum number of tokens in the response.\n",
        "        top_p (float, optional): set top_p for generation.\n",
        "\n",
        "    Returns:\n",
        "        str: The text content of the model's response.\n",
        "    \"\"\"\n",
        "    completion = \"\"\"TO DO\"\"\"\n",
        "    response_text = \"\"\"TO DO\"\"\"\n",
        "    return response_text\n"
      ],
      "metadata": {
        "id": "t3YZ7wQtTxSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Query Models (3 points)\n",
        "Loop through the sampled quries. For each query, use defined function to get answers from selected models.\n",
        "\n",
        "Please use temperature 0.0.\n",
        "\n",
        "We will prompt the model to respond in 2 different format, and evaluate the performance. Please don't change anything in the prompt."
      ],
      "metadata": {
        "id": "ZiVeFKDIqOxq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "models = [\"meta-llama/Meta-Llama-3.1-8B-Instruct\", \"meta-llama/Llama-3.3-70B-Instruct-fast\", \"aaditya/Llama3-OpenBioLLM-70B\"]\n",
        "\n",
        "system_prompt = r\"\"\"You are a medical expert specializing in answering multiple-choice questions. \\\n",
        "For each question provided, carefully analyze the options (A, B, C, D) and select the most accurate answer based on your knowledge.\"\"\"\n",
        "\n",
        "# format 1: note (The correct answer is:)\n",
        "format_prompt_1 = r\"\\nAt the end of each response, present your final answer after 'The correct answer is:'. \\\n",
        "For example: The correct answer is: A\"\n",
        "\n",
        "# save the responses in the following dict\n",
        "responses_1 = {models[0]: [], models[1]: [], models[2]: []}\n",
        "\n",
        "# This may take 15-20 minutes\n",
        "for query in tqdm(queries, desc=\"runing queries...\"):\n",
        "    # get responses for each model\n",
        "    # please use system_prompt+format_prompt_1 as system prompt, and use query+format_prompt_1 as user prompt\n",
        "    \"\"\"TO DO\"\"\"\n",
        "\n",
        "import json\n",
        "# Optional: save the responses\n",
        "with open('responses_1.txt', 'w') as file:\n",
        "    json.dump(responses_1, file)"
      ],
      "metadata": {
        "id": "KWiVD_gbZk9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# format 2: boxed (\\boxed{})\n",
        "format_prompt_2 = r\"\\nAt the end of each response, present your final answer in \\boxed{}. \\\n",
        "For example: \\boxed{A}\"\n",
        "\n",
        "responses_2 = {models[0]: [], models[1]: [], models[2]: []}\n",
        "\n",
        "# This may take 15-20 minutes\n",
        "for query in tqdm(queries, desc=\"runing queries...\"):\n",
        "    # get responses for each model\n",
        "    # please use system_prompt+format_prompt_2 as system prompt, and use query+format_prompt_2 as user prompt\n",
        "    \"\"\"TO DO\"\"\"\n",
        "\n",
        "# Optional: save the responses\n",
        "with open('responses_2.txt', 'w') as file:\n",
        "    json.dump(responses_2, file)"
      ],
      "metadata": {
        "id": "ZBv7fjlcgsub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3 Define Processing Functions (5 points)\n",
        "Define functions to process the responses, convert the letter (A-D) to an index (0-3), and compute accuracies."
      ],
      "metadata": {
        "id": "QZPcouNq4k7_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def process_answer(answer):\n",
        "    \"\"\"\n",
        "    Converts a letter-based multiple choice answer to a numerical index.\n",
        "\n",
        "    Parameters:\n",
        "        answer (str): The answer, expected to start with one of A, B, C, or D. We only care about the first letter.\n",
        "\n",
        "    Returns:\n",
        "        int: The index corresponding to the answer (A -> 0, B -> 1, C -> 2, D -> 3).\n",
        "           Returns 4 if the answer is invalid.\n",
        "    \"\"\"\n",
        "    # Check if the answer starts with A, B, C, or D (only accept capital letters). If so, return the corresponding index; else return 4\n",
        "    \"\"\"TO DO\"\"\"\n",
        "\n",
        "\n",
        "def get_answers(responses, format):\n",
        "    \"\"\"\n",
        "    Extracts answers from a list of LLM-generated responses and converts them to numeric indices.\n",
        "\n",
        "    Parameters:\n",
        "        responses (list of str): List of responses containing answers in the format.\n",
        "        format (str): The format of the answer, 'note' or 'boxed'\n",
        "\n",
        "    Returns:\n",
        "        list of int: A list of numeric indices corresponding to the extracted answers.\n",
        "                If no answer is found, 4 is used to indicate an invalid/missing answer.\n",
        "    \"\"\"\n",
        "    # define searching pattern\n",
        "    if format == \"note\":\n",
        "        pattern = re.compile(r'The correct answer is:\\s*(.*)')\n",
        "\n",
        "    elif format == \"boxed\":\n",
        "        pattern = re.compile(r'\\\\boxed\\{([^}]+)\\}')\n",
        "\n",
        "    answers = []\n",
        "\n",
        "    # Process each response\n",
        "    for i, response in enumerate(responses):\n",
        "        pattern_match = \"\"\"TO DO\"\"\"\n",
        "        if pattern_match:\n",
        "            \"\"\"TO DO\"\"\"\n",
        "        else:\n",
        "          \"\"\"TO DO\"\"\"\n",
        "          print(f\"No answer found in format in response {i}\")\n",
        "\n",
        "    return answers\n",
        "\n",
        "def compute_accuracy(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Computes accuracy by comparing true labels with predicted labels, and identifies indices of incorrect predictions.\n",
        "\n",
        "    Parameters:\n",
        "        y_true (list of int): Ground truth answer.\n",
        "        y_pred (list of int): Predicted answer.\n",
        "\n",
        "    Returns:\n",
        "        accuracy (float): accuracy.\n",
        "        wrong_indices (list of int): Indices where predictions are incorrect.\n",
        "    \"\"\"\n",
        "    \"\"\"TO DO\"\"\"\n",
        "    return accuracy, wrong_indices"
      ],
      "metadata": {
        "id": "PJU9naIAqcKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.4 Process responses (2 points)\n",
        "Process the LLM responses using the finction defined, print the accuracy and wrong indices for each model."
      ],
      "metadata": {
        "id": "uhBhNeY_V1vq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# results for format 1: note\n",
        "for model, responses in responses_1.items():\n",
        "    print(f\"Processing {model}...\")\n",
        "    llm_answers = \"\"\"TO DO\"\"\"\n",
        "    accuracy, wrong_id = \"\"\"TO DO\"\"\"\n",
        "    print(f\"Accuracy: {accuracy}\")\n",
        "    print(f\"Wrong IDs: {wrong_id}\\n\")\n"
      ],
      "metadata": {
        "id": "uiHuJPMrz4Du"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# results for format 2: boxed\n",
        "for model, responses in responses_2.items():\n",
        "    print(f\"Processing {model}...\")\n",
        "    llm_answers = \"\"\"TO DO\"\"\"\n",
        "    accuracy, wrong_id = \"\"\"TO DO\"\"\"\n",
        "    print(f\"Accuracy: {accuracy}\")\n",
        "    print(f\"Wrong IDs: {wrong_id}\\n\")"
      ],
      "metadata": {
        "id": "AMc0JOZXm1WZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 4: Questions (4 points)\n"
      ],
      "metadata": {
        "id": "gJ9T8Ulaqccg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Compare and comment on the performance of different models. (1 point)\n",
        "\n",
        "your answer here"
      ],
      "metadata": {
        "id": "dOaaOQ7fWfBL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Explain the concept of the temperature parameter in LLM inference. How does adjusting the temperature affect generation behavior? In your opinion, what temperature setting is most appropriate when evaluating LLMs on multiple-choice questions, and why? (3 points)\n",
        "\n",
        "your answer here"
      ],
      "metadata": {
        "id": "Su2LxghGWgsy"
      }
    }
  ]
}