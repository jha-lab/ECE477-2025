{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3ivv9pWqOKZ"
      },
      "source": [
        "#Put your Google Colab link here:\n",
        "*your link here*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# COMFORT: A Continual Fine-Tuning Framework for Foundation Models Targeted at Consumer Healthcare\n",
        "\n",
        "In this exercise, we will use PyTorch to recreate some of the experiments done in the [COMFORT](https://arxiv.org/abs/2409.09549) paper. We will first build a health foundation model with large pre-training. Then, we will use parameter-efficient fine-tuning algorithms to continually fine-tune the health foundation model to learn downstream disease-detection tasks.\n",
        "\n",
        "*   Navigate to the tabs above. Click `Runtime` -> `Change runtime type` -> select `GPU` as the hardware accelerator to enable GPU, which will allow you to train faster."
      ],
      "metadata": {
        "id": "C9w9zHgTl0-K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import useful libraries:"
      ],
      "metadata": {
        "id": "hI-lvxCatRCZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "iOMZMu42DzcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add the current assignment folder to the system path\n",
        "# to enable importing functions from utils.py located in this directory\n",
        "import sys\n",
        "sys.path.append('/content/drive/Shareddrives/ECE477 datasets/Assignment11')"
      ],
      "metadata": {
        "id": "RzZ0L-0ZD1DG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cK8xvOiXlps_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import math\n",
        "import torch\n",
        "import argparse\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from utils import *\n",
        "from torch import Tensor\n",
        "from pathlib import Path\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.decomposition import PCA\n",
        "from typing import Union, Dict, Optional\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.metrics import confusion_matrix, classification_report"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Get access to a GPU:\n",
        "To gain access to the GPUs on Colab, navigate to the `Runtime` tab above and select `Change runtime type`."
      ],
      "metadata": {
        "id": "-yKoHry41pcF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "metadata": {
        "id": "STrgYrSs1r61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1: Prepare the experimental datasets (1 pt)\n",
        "\n",
        "### Experimental datasets:\n",
        "\n",
        "For this exercise, we use the DiabDeep dataset collected for the [DiabDeep](https://ieeexplore.ieee.org/abstract/document/8935429) project and the MHDeep dataset collected for the [MHDeep](https://dl.acm.org/doi/full/10.1145/3527170) project.\n",
        "\n",
        "*  **The DiabDeep Dataset:**\n",
        "The DiabDeep dataset contains physiological signals and environmental information collected from 25 non-diabetic individuals, 14 Type-I diabetic patients, and 13 Type-II diabetic patients with a smartwatch and a smartphone.\n",
        "\n",
        "*  **The MHDeep Dataset:**\n",
        "The MHDeep dataset contains physiological signals and environmental information collected from 23 healthy participants, 23 participants with bipolar disorder, 10 participants with major depressive disorder, and 16 participants with schizoaffective disorder with a smartwatch and a smartphone.\n",
        "\n",
        "The datasets used here are their streamlined versions. The DiabDeep dataset contains 20,957 data instances with 4,485 features. The MHDeep dataset includes 27,082 data instances with 4,485 features. Refer to Table 1 in [COMFORT](https://arxiv.org/abs/2409.09549) for the data features included in these datasets.\n",
        "\n",
        "### Data preprocessing:\n",
        "\n",
        "It is crucial and a good practice to [preprocess](https://neptune.ai/blog/data-preprocessing-guide) your dataset before you jump into model training. Data preprocessing includes handling missing values, data nomalization, feature selection, dimensionality reduction, etc. This part has been done for you. See Section 4.2 in [COMFORT](https://arxiv.org/abs/2409.09549) for the details about the dataset preprocessing that had been done.\n",
        "\n",
        "### Prepare data for pre-training:\n",
        "\n",
        "COMFORT deems to use a large number of physiological data collected from healthy individuals with wearable medical sensors (WMSs) to pre-train the health foundation model. However, in light of the difficulty of accessing such large datasets for this exercise, we will just use the data from the healthy participants in both DiabDeep and MHDeep datasets to pre-train the foundation model. These data are already packed separately in `diabdeep_data_4pretrain.zip` and `mhdeep_data_4pretrain.zip` for you."
      ],
      "metadata": {
        "id": "54vFnhPvIr_I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Create a custom dataset class: (0.5 pt)\n",
        "To prepare our datasets, we create a `CustomDataset` class that inherits PyTorch's [Dataset](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) class.\n"
      ],
      "metadata": {
        "id": "b39RDi_62n0J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, x, y, device):\n",
        "        # cast x and y to `device`\n",
        "        \"\"\"TO DO\"\"\"\n",
        "        self.x =\n",
        "        self.y =\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return the len of the dataset\n",
        "        \"\"\"TO DO\"\"\"\n",
        "        return\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.x[idx], self.y[idx]"
      ],
      "metadata": {
        "id": "7_rOx9El3ZFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Load the data for pre-training: (0.5 pt)\n",
        "\n",
        "Use Numpy to load the data from both datasets and transform them to torch tensors. Then, concatenate them along the corrent dimension."
      ],
      "metadata": {
        "id": "DZufgx0c3ibS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# unzip the pre-training datasets\n",
        "!unzip '/content/drive/Shareddrives/ECE477 datasets/Assignment11/diabdeep_data_4pretrain.zip' -d diabdeep_data_4pretrain\n",
        "!unzip '/content/drive/Shareddrives/ECE477 datasets/Assignment11/mhdeep_data_4pretrain.zip' -d mhdeep_data_4pretrain\n",
        "\n",
        "# check out the data\n",
        "print('\\n', os.listdir('diabdeep_data_4pretrain'))\n",
        "print('\\n', os.listdir('mhdeep_data_4pretrain'))\n",
        "\n",
        "# load the data and transform them to torch tensors\n",
        "\"\"\"TO DO\"\"\"\n",
        "diab_x_4pretrain =\n",
        "mh_x_4pretrain =\n",
        "# concatenate the above data along the correct dimension\n",
        "\"\"\"TO DO\"\"\"\n",
        "x_pretrain =\n",
        "print(x_pretrain.shape)"
      ],
      "metadata": {
        "id": "bZS_dJtQGRaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2: Create the building blocks for a Transformer (11 pts)\n",
        "\n",
        "COMFORT targets at classification applications in the WMS data domain, where input data and output data are both numerical. Therefore, we do not need a decoder for our Transformer model. In addition, we want our health foundation model to understand the bidirectional relationship between WMS data, so we employ the BERT_TINY [[1](https://arxiv.org/abs/1810.04805), [2](https://arxiv.org/abs/2110.01518), [3](https://arxiv.org/abs/1908.08962)] architecture and pre-training objectives to construct the health foundation model.\n",
        "\n",
        "In the following cells, we will create the building blocks of an encoder-only Transformer model. Each cell is provided with a blog link for extra readings."
      ],
      "metadata": {
        "id": "nCE_ggY9Fe75"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Positional encoding: (2 pts)\n",
        "\n",
        "[Positional encoding](https://medium.com/@hunterphillips419/positional-encoding-7a93db4109e6) is used to provide a relative position for each token in a sequence.\n",
        "\n",
        "**Hint: There is an efficient and PyTorch-centric approach to implement this.**"
      ],
      "metadata": {
        "id": "sN9jcDNC5VH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model: int, dropout: float = 0.1, max_length: int = 1000):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          d_model:          dimension of embeddings\n",
        "          dropout:          probability of dropout occurring\n",
        "          max_length:       max sequence length\n",
        "        \"\"\"\n",
        "        # inherit from Module\n",
        "        super().__init__()\n",
        "\n",
        "        # initialize a dropout layer\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        # create an empty tensor of 0s\n",
        "        \"\"\"TO DO\"\"\"\n",
        "        pe =     # pe.shape = (max_length, d_model)\n",
        "\n",
        "        # create a column matrix for all possible positions\n",
        "        \"\"\"TO DO\"\"\"\n",
        "        k =     # k.shape = (max_length, 1)\n",
        "\n",
        "        # calculate the divisor for positional encoding\n",
        "        # use n = 10000.0\n",
        "        \"\"\"TO DO\"\"\"\n",
        "        div_term =\n",
        "\n",
        "        # calcualte sine on even indices\n",
        "        \"\"\"TO DO\"\"\"\n",
        "        pe[:, 0::2] =\n",
        "\n",
        "        # calculate cosine on odd indices\n",
        "        \"\"\"TO DO\"\"\"\n",
        "        pe[:, 1::2] =\n",
        "\n",
        "        # add one dimension\n",
        "        pe = pe.unsqueeze(0)\n",
        "\n",
        "        # register the positional encoding pe to buffers\n",
        "        # buffers are saved in state_dict but not trained by the optimizer\n",
        "        self.register_buffer(\"pe\", pe)\n",
        "\n",
        "    def forward(self, x: Tensor):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          x:        input embeddings\n",
        "        Returns:\n",
        "          x:        input embeddings + positional encodings\n",
        "        \"\"\"\n",
        "        # add positional encoding to the embeddings\n",
        "        x = x + self.pe[:, : x.size(1)].requires_grad_(False)\n",
        "\n",
        "        # perform dropout\n",
        "        x = self.dropout(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "fCII80q6654O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-head attention layers: (3 pts)\n",
        "\n",
        "[Multi-head attention](https://medium.com/@hunter-j-phillips/multi-head-attention-7924371d477a) layers are the heart of a Transformer model."
      ],
      "metadata": {
        "id": "D3NXAQsm696O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model: int = 512, n_heads: int = 8, dropout: float = 0.1):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          d_model:      dimension of embeddings\n",
        "          n_heads:      number of self attention heads\n",
        "          dropout:      probability of dropout occurring\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        assert d_model % n_heads == 0            # Ensure an even num of heads\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.d_key = d_model // n_heads          # Assume d_value equals d_key\n",
        "\n",
        "        # Use nn.Linear() to initialize the query layer weights\n",
        "        \"\"\"TO DO\"\"\"\n",
        "        self.Wq =\n",
        "        # Use nn.Linear() to initialize the key layer weights\n",
        "        \"\"\"TO DO\"\"\"\n",
        "        self.Wk =\n",
        "        # Use nn.Linear() to initialize the value layer weights\n",
        "        \"\"\"TO DO\"\"\"\n",
        "        self.Wv =\n",
        "        # Use nn.Linear() to initialize the output layer weights\n",
        "        \"\"\"TO DO\"\"\"\n",
        "        self.Wo =\n",
        "\n",
        "        # Initialize a dropout layer\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, query: Tensor, key: Tensor, value: Tensor, mask: Tensor = None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          query:         query vector       (batch_size, q_length, d_model)\n",
        "          key:           key vector         (batch_size, k_length, d_model)\n",
        "          value:         value vector       (batch_size, s_length, d_model)\n",
        "          mask:          mask for decoder\n",
        "        Returns:\n",
        "          output:        attention values   (batch_size, q_length, d_model)\n",
        "        \"\"\"\n",
        "        batch_size = key.size(0)\n",
        "\n",
        "        # calculate query, key, and value tensors\n",
        "        \"\"\"TO DO\"\"\"\n",
        "        Q =\n",
        "        K =\n",
        "        V =\n",
        "\n",
        "        # split each tensor into n-heads to compute attention\n",
        "        # query tensor\n",
        "        Q = Q.view(\"\"\"TO DO\"\"\").permute(0, 2, 1, 3)\n",
        "\n",
        "        # key tensor\n",
        "        K = K.view(\"\"\"TO DO\"\"\").permute(0, 2, 1, 3)\n",
        "\n",
        "        # value tensor\n",
        "        V = V.view(\"\"\"TO DO\"\"\").permute(0, 2, 1, 3)\n",
        "\n",
        "        # computes attention\n",
        "        # scaled dot product -> QK^{T}\n",
        "        scaled_dot_prod = torch.matmul(Q, K.permute(0, 1, 3, 2)) / math.sqrt(self.d_key)\n",
        "\n",
        "        # fill those positions of product as (-1e10) where mask positions are 0\n",
        "        if mask is not None:\n",
        "          scaled_dot_prod = scaled_dot_prod.masked_fill(mask == 0, -1e10)\n",
        "\n",
        "        # apply softmax\n",
        "        attn_probs = torch.softmax(scaled_dot_prod, dim=-1)\n",
        "\n",
        "        # multiply by values to get attention\n",
        "        A = torch.matmul(self.dropout(attn_probs), V)\n",
        "\n",
        "        # reshape attention back\n",
        "        A = A.permute(0, 2, 1, 3).contiguous()\n",
        "        A = A.view(batch_size, -1, self.n_heads*self.d_key)\n",
        "\n",
        "        # push through the final weight layer\n",
        "        \"\"\"TO DO\"\"\"\n",
        "        output =\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "sBS7c1E68UGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Position-wise feed-forward network: (1 pts)\n",
        "\n",
        "[Position-wise feed-forward network](https://medium.com/@hunter-j-phillips/position-wise-feed-forward-network-ffn-d4cc9e997b4c) is an expand-and-contract network that transforms each sequence using the same dense layers."
      ],
      "metadata": {
        "id": "znpzWM8FY2eU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionwiseFeedForward(nn.Module):\n",
        "    def __init__(self, d_model: int, d_ffn: int, dropout: float = 0.1):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          d_model:      dimension of embeddings\n",
        "          d_ffn:        dimension of feed-forward network\n",
        "          dropout:      probability of dropout occurring\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # Use nn.Linear() to initialize the w_1 layer weights\n",
        "        \"\"\"TO DO\"\"\"\n",
        "        self.w_1 =\n",
        "\n",
        "        # Use nn.Linear() to initialize the w_2 layer weights\n",
        "        \"\"\"TO DO\"\"\"\n",
        "        self.w_2 =\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          x:        Output from attention layer             (batch_size, seq_length, d_model)\n",
        "        Returns:\n",
        "          x:        Expanded-and-contracted representation  (batch_size, seq_length, d_model)\n",
        "        \"\"\"\n",
        "        # pass x through self.w_1 and a relu() layer\n",
        "        \"\"\"TO DO\"\"\"\n",
        "        x =\n",
        "        x = self.dropout(x)\n",
        "        # pass x through self.w_2\n",
        "        x =\n",
        "        return x"
      ],
      "metadata": {
        "id": "257kgEYOf0Rn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder layer and encoder stack:\n",
        "\n",
        "The [encoder layer](https://medium.com/@hunter-j-phillips/the-encoder-f698b2c7afc0) is a wrapper for the sublayers created in the previous cells. The encoder stack is a wrapper to combine the encoder layers."
      ],
      "metadata": {
        "id": "PgV5jTBsf2kF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model: int, n_heads: int, d_ffn: int, dropout: float):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          d_model:      dimension of embeddings\n",
        "          n_heads:      number of self-attention heads\n",
        "          d_ffn:        dimension of feed-forward network\n",
        "          dropout:      probability of dropout occurring\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # multi-head attention sublayer\n",
        "        self.attention = MultiHeadAttention(d_model, n_heads, dropout)\n",
        "        # layer norm for multi-head attention\n",
        "        self.attn_layer_norm = nn.LayerNorm(d_model)\n",
        "\n",
        "        # position-wise feed-forward network\n",
        "        self.positionwise_ffn = PositionwiseFeedForward(d_model, d_ffn, dropout)\n",
        "        # layer norm for position-wise ffn\n",
        "        self.ffn_layer_norm = nn.LayerNorm(d_model)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src: Tensor, src_mask: Tensor):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          src:          positionally embedded sequences     (batch_size, seq_length, d_model)\n",
        "          src_mask:     mask for the sequences              (batch_size, 1, 1, seq_length)\n",
        "        Returns:\n",
        "          src:          sequences after self-attention and feed-forward layers    (batch_size, seq_length, d_model)\n",
        "        \"\"\"\n",
        "        # pass embeddings through multi-head attention\n",
        "        _src = self.attention(src, src, src, src_mask)\n",
        "\n",
        "        # residual add and norm\n",
        "        src = self.attn_layer_norm(src + self.dropout(_src))\n",
        "\n",
        "        # position-wise feed-forward network\n",
        "        _src = self.positionwise_ffn(src)\n",
        "\n",
        "        # residual add and norm\n",
        "        src = self.ffn_layer_norm(src + self.dropout(_src))\n",
        "\n",
        "        return src\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, d_model: int, n_layers: int,\n",
        "                 n_heads: int, d_ffn: int, dropout: float = 0.1):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          d_model:      dimension of embeddings\n",
        "          n_layers:     number of encoder layers\n",
        "          n_heads:      number of self-attention heads\n",
        "          d_ffn:        dimension of feed-forward network\n",
        "          dropout:      probability of dropout occurring\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # create n_layers encoders\n",
        "        self.layers = nn.ModuleList([EncoderLayer(d_model, n_heads, d_ffn, dropout)\n",
        "                                     for layer in range(n_layers)])\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src: Tensor, src_mask: Tensor):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          src:          embedded sequences              (batch_size, seq_length, d_model)\n",
        "          src_mask:     mask for the sequences          (batch_size, 1, 1, seq_length)\n",
        "\n",
        "        Returns:\n",
        "        src:          sequences after encoder layers    (batch_size, seq_length, d_model)\n",
        "        \"\"\"\n",
        "        # pass the sequences through each encoder\n",
        "        for layer in self.layers:\n",
        "            src = layer(src, src_mask)\n",
        "        return src"
      ],
      "metadata": {
        "id": "UU9_tbfw4tN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the Transformer class for the foundation model:\n",
        "\n",
        "Here, we define a Transformer class as the wrapper to combine the building blocks defined above."
      ],
      "metadata": {
        "id": "U-XcQW8OjeN4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, encoder: Encoder, pos_enc: PositionalEncoding, src_pad_idx: int, device):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          encoder:      encoder stack\n",
        "          pos_enc:      positional encodings\n",
        "          src_pad_idx:  padding index\n",
        "          device:       device used for training\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.encoder     = encoder\n",
        "        self.pos_enc     = pos_enc\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "        self.device      = device\n",
        "\n",
        "    def make_src_mask(self, src: Tensor):\n",
        "        \"\"\"\n",
        "        Create a mask for source sequences.\n",
        "\n",
        "        Args:\n",
        "          src:          raw sequences with padding\n",
        "        Returns:\n",
        "          src_mask:     mask for each sequence\n",
        "        \"\"\"\n",
        "        # assign 1 to tokens that need attended to and 0 to padding tokens, then add 2 dimensions\n",
        "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "        return src_mask\n",
        "\n",
        "    def forward(self, src: Tensor):\n",
        "        \"\"\"\n",
        "        Pass the source sequences through the encoder.\n",
        "\n",
        "        Args:\n",
        "          src:          raw source sequences\n",
        "        Returns:\n",
        "          src:          sequences after encoder stack\n",
        "        \"\"\"\n",
        "        # create source masks\n",
        "        src_4_mask = src[:, :, 0]\n",
        "        src_mask = self.make_src_mask(src_4_mask)\n",
        "\n",
        "        # pass the src through the pos_enc and encoder layers\n",
        "        src = self.pos_enc(src)\n",
        "        src = self.encoder(src, src_mask)\n",
        "        return src"
      ],
      "metadata": {
        "id": "0Khb-MKui-7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make the foundation model:\n",
        "\n",
        "Here, we define a function `make_foundation_model()` to instantiate a Transformer model as our foundation model."
      ],
      "metadata": {
        "id": "Sb9dWxNK9QeJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_foundation_model(device, n_layers: int = 3, in_features: int = 4485, d_model: int = 512,\n",
        "               d_ffn: int = 2048, n_heads: int = 8, dropout: float = 0.1,\n",
        "               max_length: int = 1000, pad_idx: int = 100):\n",
        "        \"\"\"\n",
        "        Construct a model when provided parameters.\n",
        "\n",
        "        Args:\n",
        "          device:       device used for training\n",
        "          n_layers:     number of Transformer layers\n",
        "          in_features:  dimension of input features\n",
        "          d_model:      dimension of embeddings\n",
        "          d_ffn:        dimension of feed-forward network\n",
        "          n_heads:      number of heads\n",
        "          dropout:      probability of dropout occurring\n",
        "          max_length:   maximum sequence length for positional encodings\n",
        "          pad_idx:      padding index\n",
        "        Returns:\n",
        "          model:        a Transformer model\n",
        "        \"\"\"\n",
        "        # create the encoder\n",
        "        encoder = Encoder(d_model, n_layers, n_heads, d_ffn, dropout)\n",
        "\n",
        "        # create a positional encoding matrix\n",
        "        pos_enc = PositionalEncoding(d_model, dropout, max_length)\n",
        "\n",
        "        # create the Transformer model\n",
        "        model = nn.Sequential(Transformer(encoder, pos_enc, pad_idx, device))\n",
        "\n",
        "        # initialize parameters with Xavier/Glorot\n",
        "        for p in model.parameters():\n",
        "            if p.dim() > 1:\n",
        "                nn.init.xavier_uniform_(p)\n",
        "        return model"
      ],
      "metadata": {
        "id": "MmMzA7Yf2RD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the pre-training function: (3 pts)\n",
        "\n",
        "Here, we define the pre-training function `pretrain_model()` to pre-train our foundation model."
      ],
      "metadata": {
        "id": "l03azNpM90VI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pretrain_model(model, device, iterator, optimizer, criterion, clip):\n",
        "    \"\"\"\n",
        "    Train the model on the given data.\n",
        "\n",
        "    Args:\n",
        "        model:        the Transformer model to be trained\n",
        "        device:       device used for training\n",
        "        iterator:     data to be trained on\n",
        "        optimizer:    optimizer for updating parameters\n",
        "        criterion:    loss function for updating parameters\n",
        "        clip:         value to help prevent exploding gradients\n",
        "    Returns:\n",
        "        avg_loss:     average loss for the epoch\n",
        "    \"\"\"\n",
        "    # set the model to training mode\n",
        "    model.train()\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    # loop through each batch in the iterator\n",
        "    for i, batch in enumerate(iterator):\n",
        "\n",
        "        # set the source (data features) and target (target values) batches\n",
        "        src, trg = batch\n",
        "        for i in range(len(src)):\n",
        "            # in src, randomly select 5 row indices out of the 15 rows (tokens) without replacement for masking\n",
        "            \"\"\"TO DO\"\"\"\n",
        "            row_mask_idx =\n",
        "            for j in range(len(row_mask_idx)):\n",
        "                # in src, for each row_mask_idx, randomly select 15% of the data features without replacement for masking\n",
        "                \"\"\"TO DO\"\"\"\n",
        "                column_mask_idx =\n",
        "                # in each src, replace data at locations selected above with random numbers from the standard normal distribution\n",
        "                \"\"\"TO DO\"\"\"\n",
        "                src[i, row_mask_idx[j], column_mask_idx] =\n",
        "\n",
        "        # zero out the optimizer gradients\n",
        "        \"\"\"TO DO\"\"\"\n",
        "\n",
        "\n",
        "        # get output logits for src\n",
        "        \"\"\"TO DO\"\"\"\n",
        "        logits =\n",
        "        logits = logits.contiguous().view(-1, logits.shape[-1])\n",
        "\n",
        "        # expected output (traget values)\n",
        "        expected_output = trg.contiguous().view(-1, trg.shape[-1])\n",
        "\n",
        "        # calculate the loss\n",
        "        loss = criterion(logits, expected_output)\n",
        "\n",
        "        # backpropagate loss\n",
        "        \"\"\"TO DO\"\"\"\n",
        "\n",
        "        # clip the weights\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "\n",
        "        # update the weights\n",
        "        \"\"\"TO DO\"\"\"\n",
        "\n",
        "        # update the loss\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    # return the average loss for the epoch\n",
        "    avg_loss = epoch_loss / len(iterator)\n",
        "    return avg_loss"
      ],
      "metadata": {
        "id": "a-GbpYHci6n7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize model parameters:\n",
        "\n",
        "We initialize some hyperparameters here.\n",
        "\n",
        "Typically, constructing a foundation model requires a significant amount of pre-training data and training epochs. However, due to the resourse limitation, we will just briefly pre-train our health foundation model for **5 steps** and **20 epochs** each."
      ],
      "metadata": {
        "id": "7JvaFd8a-vwy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "layers     = 2            # number of Transformer layers\n",
        "features   = 128          # dimension of input features\n",
        "d_model    = 128          # dimensions of embeddings\n",
        "d_ffn      = d_model*4    # dimension of feed-forward network\n",
        "heads      = 2            # number of heads\n",
        "max_length = 15           # max sequence length\n",
        "PAD_NUM    = 100          # filling number for padding\n",
        "\n",
        "epochs     = 20           # briefly pre-train the foundation model for 20 epochs per step\n",
        "steps      = 5            # briefly pre-train the foundation model for 5 step\n",
        "lr         = 5e-4         # initial learning rate\n",
        "batch_size = 128          # batch size"
      ],
      "metadata": {
        "id": "pQ52rRyUicRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pre-train the foundation model: (2 pts)"
      ],
      "metadata": {
        "id": "aTKrLZYB--Lg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "SEED = 0\n",
        "\n",
        "# Set a fixed random seed for reproducibility\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if device == 'cuda':\n",
        "  torch.cuda.manual_seed(SEED)\n",
        "  torch.cuda.manual_seed_all(SEED)\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# use make_foundation_model() to instantiate a Transformer model as the foundation model\n",
        "\"\"\"TO DO\"\"\"\n",
        "model =\n",
        "# cast the model to `device`\n",
        "\"\"\"TO DO\"\"\"\n",
        "\n",
        "\n",
        "# initialize the Adam optimizer\n",
        "\"\"\"TO DO\"\"\"\n",
        "optimizer =\n",
        "# set the loss criterion as the MSELoss()\n",
        "\"\"\"TO DO\"\"\"\n",
        "criterion =\n",
        "# set the CLIP value to prevent exploding gradient\n",
        "CLIP = 1\n",
        "\n",
        "# training may take 20 min\n",
        "for step in range(steps):\n",
        "    # shuffle the pre-training data to inject randomness\n",
        "    \"\"\"TO DO\"\"\"\n",
        "    x_pretrain =\n",
        "    # create a copy of the pre-training data as their target values\n",
        "    \"\"\"TO DO\"\"\"\n",
        "    y_pretrain =\n",
        "\n",
        "    # apply positional encoding to the target values\n",
        "    ##################################################################################\n",
        "    pe = torch.zeros(max_length, d_model)\n",
        "    k = torch.arange(0, max_length).unsqueeze(1)\n",
        "    div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "    pe[:, 0::2] = torch.sin(k * div_term)\n",
        "    pe[:, 1::2] = torch.cos(k * div_term)\n",
        "    pe = pe.unsqueeze(0)\n",
        "    y_pretrain = y_pretrain + pe[:, : y_pretrain.size(1)]\n",
        "    ##################################################################################\n",
        "\n",
        "    # use CustomDataset() to create the pre-training dataset pretrain_set\n",
        "    \"\"\"TO DO\"\"\"\n",
        "    pretrain_set =\n",
        "    # use PyTorch's DataLoader() to create the dataloader pretrain_loader\n",
        "    \"\"\"TO DO\"\"\"\n",
        "    pretrain_loader =\n",
        "\n",
        "    # loop through each epoch\n",
        "    for epoch in range(epochs):\n",
        "        # use pretrain_model() to calculate the train loss and update the parameters\n",
        "        \"\"\"TO DO\"\"\"\n",
        "        train_loss =\n",
        "        # save the model parameters\n",
        "        torch.save(model.state_dict(), f'pretrain_comfort_mlm_exercise.pt')\n",
        "\n",
        "        print(f'Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f}')"
      ],
      "metadata": {
        "id": "OM_ekdAih340"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3: Continual parameter-efficient fine-tuning for downstream disease-detection tasks (8 pts)\n",
        "\n",
        "Now that we finished pre-training our health foundation model, we will learn downstream disease-detection tasks using parameter-efficient fine-tuning (PEFT) algorithms. We will use [Low-Rank Adaptation (LoRA)](https://arxiv.org/abs/2106.09685) to learn the DiabDeep detection task."
      ],
      "metadata": {
        "id": "XWuQXuel4CAz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PEFT with LoRA:\n",
        "\n",
        "Here, we will define the building blocks for implementing LoRA."
      ],
      "metadata": {
        "id": "bSR3qA7HR7Sx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Position-wise feed-forward network with LoRA: (3 pts)\n",
        "\n",
        "We will insert low-rank matrices to the position-wise feed-forward networks for PEFT."
      ],
      "metadata": {
        "id": "nTDFh6JiTtQJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LoRAPositionwiseFeedForward(PositionwiseFeedForward):\n",
        "    \"\"\"\n",
        "    Extends PositionwiseFeedForward module with Low-Rank Adaptation (LoRA).\n",
        "    LoRA adds two matrices to the layer, allowing for efficient training of large models.\n",
        "    \"\"\"\n",
        "    def __init__(self, rank: int, d_model:int, d_ffn: int, dropout: float = 0.1):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          rank:         rank of LoRA metrices\n",
        "          d_model:      dimension of embeddings\n",
        "          d_ffn:        dimension of feed-forward network\n",
        "          dropout:      probability of dropout occurring\n",
        "        \"\"\"\n",
        "        super().__init__(d_model, d_ffn, dropout)\n",
        "\n",
        "        # Initialize LoRA matrices\n",
        "        self.alpha = 8\n",
        "        self.rank = rank\n",
        "        # use nn.Parameter() to initialize the low-rank matrix B for w_1 in the position-wise feed-forward netwrok\n",
        "        # hint: zero matrix\n",
        "        \"\"\"TO DO\"\"\"\n",
        "        self.lora_w_1_B =\n",
        "        # use nn.Parameter() to initialize the low-rank matrix A for w_1 in the position-wise feed-forward netwrok\n",
        "        # hint: random values from the standard normal distribution\n",
        "        \"\"\"TO DO\"\"\"\n",
        "        self.lora_w_1_A =\n",
        "        # use nn.Parameter() to initialize the low-rank matrix B for w_2 in the position-wise feed-forward netwrok\n",
        "        # hint: zero matrix\n",
        "        \"\"\"TO DO\"\"\"\n",
        "        self.lora_w_2_B =\n",
        "        # use nn.Parameter() to initialize the low-rank matrix A for w_2 in the position-wise feed-forward netwrok\n",
        "        # hint: random values from the standard normal distribution\n",
        "        \"\"\"TO DO\"\"\"\n",
        "        self.lora_w_2_A =\n",
        "\n",
        "        # Freeze the original weight matrix\n",
        "        self.w_1.requires_grad = False\n",
        "        self.w_2.requires_grad = False\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        # Compute LoRA weight matrices (delta W)\n",
        "        # hint: BA\n",
        "        \"\"\"TO DO\"\"\"\n",
        "        lora_w_1_weights =\n",
        "        lora_w_2_weights =\n",
        "        # scale LoRA weight matrices\n",
        "        lora_w_1_weights *= (self.alpha / self.rank)\n",
        "        lora_w_2_weights *= (self.alpha / self.rank)\n",
        "        # Apply the original and LoRA-adjusted linear transformations\n",
        "        # pass x through w_1 layer with LoRA and a relu() activation\n",
        "        # hint: w_1 * x + BA * x\n",
        "        \"\"\"TO DO\"\"\"\n",
        "        x =\n",
        "        x = self.dropout(x)\n",
        "        # pass x through w_2 layer with LoRA\n",
        "        # hint: w_2 * x + BA * x\n",
        "        \"\"\"TO DO\"\"\"\n",
        "        x =\n",
        "        return x"
      ],
      "metadata": {
        "id": "QmakAT43T0WN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-head attention layers with LoRA: (3 pts)\n",
        "\n",
        "We will insert low-rank matrices to the multi-head attention layers for PEFT. As the original implementation in [LoRA](https://arxiv.org/abs/2106.09685), We will apply LoRA to the query and value layers only."
      ],
      "metadata": {
        "id": "_uH1lobwT1XU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LoRAMultiHeadAttention(MultiHeadAttention):\n",
        "    \"\"\"\n",
        "    A module inherits from the standard MultiHeadAttention.\n",
        "    This initializes and adds the LoRA (LoRA: Low-Rank Adaptation of Large Language Models) matricies to the original MultiHeadAttention.\n",
        "    In __init__, initialize the new matricies needed for LoRA.\n",
        "    The parameters should all start with \"lora_\" as we will check for this string later in the full model wrapper to identify the lora matricies.\n",
        "    The LoRA paper concluded that only updating the query and value matricies is the most efficient.\n",
        "    We overwrite the forward method with the new LoRA logic.\n",
        "    \"\"\"\n",
        "    def __init__(self, rank: int, **kwargs):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          rank:         rank of LoRA matrices\n",
        "          d_model:      dimension of embeddings\n",
        "          n_heads:      number of attention heads\n",
        "          dropout:      dropout probability, randomly zeroes-out some of the input\n",
        "        \"\"\"\n",
        "        super().__init__(**kwargs)\n",
        "        self.alpha = 8\n",
        "        self.rank = rank\n",
        "        self.d_model = kwargs['d_model']\n",
        "        self.n_heads = kwargs['n_heads']\n",
        "        self.dropout = nn.Dropout(p=kwargs['dropout'])\n",
        "\n",
        "        # Initialize trainable matrices for query and value vectors\n",
        "        # B should be initialized as zeros, A as random gaussian, such that their product and\n",
        "        # thus the weight delta is zero in the beginning\n",
        "\n",
        "        # use nn.Parameter() to initialize the low-rank matrix B for query\n",
        "        \"\"\"TO DO\"\"\"\n",
        "        self.lora_query_matrix_B =\n",
        "        # use nn.Parameter() to initialize the low-rank matrix A for query\n",
        "        \"\"\"TO DO\"\"\"\n",
        "        self.lora_query_matrix_A =\n",
        "        # use nn.Parameter() to initialize the low-rank matrix B for value\n",
        "        \"\"\"TO DO\"\"\"\n",
        "        self.lora_value_matrix_B =\n",
        "        # use nn.Parameter() to initialize the low-rank matrix A for value\n",
        "        \"\"\"TO DO\"\"\"\n",
        "        self.lora_value_matrix_A =\n",
        "\n",
        "    def lora_query(self, x):\n",
        "        \"\"\"\n",
        "        LoRA query logic. To fully work with only training the LoRA parameters the regular linear\n",
        "        layer has to be frozen before initializing the optimizer.\n",
        "\n",
        "        Args:\n",
        "          x:        inputs\n",
        "\n",
        "        Returns:\n",
        "          output:   Wq(x) + delta_Wq(x)\n",
        "        \"\"\"\n",
        "        # Compute LoRA weight matrices (delta W)\n",
        "        # hint: BA\n",
        "        \"\"\"TO DO\"\"\"\n",
        "        lora_full_query_weights =\n",
        "        # scale LoRA weight matrix\n",
        "        lora_full_query_weights *= (self.alpha / self.rank)\n",
        "        # Apply the original and LoRA-adjusted linear transformations\n",
        "        # pass x through Wq layer with LoRA\n",
        "        # hint: Wq * x + BA * x\n",
        "        \"\"\"TO DO\"\"\"\n",
        "        output =\n",
        "        return output\n",
        "\n",
        "    def lora_value(self, x):\n",
        "        \"\"\"\n",
        "        LoRA value logic. To fully work with only training the LoRA parameters the regular linear\n",
        "        layer has to be frozen before initializing the optimizer.\n",
        "\n",
        "        Args:\n",
        "          x:        inputs\n",
        "\n",
        "        Returns:\n",
        "          output:   Wv(x) + delta_Wv(x)\n",
        "        \"\"\"\n",
        "        # Compute LoRA weight matrices (delta W)\n",
        "        # hint: BA\n",
        "        \"\"\"TO DO\"\"\"\n",
        "        lora_full_value_weights =\n",
        "        # scale LoRA weight matrix\n",
        "        lora_full_value_weights *= (self.alpha / self.rank)\n",
        "        # Apply the original and LoRA-adjusted linear transformations\n",
        "        # pass x through Wv layer with LoRA\n",
        "        # hint: Wv * x + BA * x\n",
        "        \"\"\"TO DO\"\"\"\n",
        "        output =\n",
        "        return output\n",
        "\n",
        "    def forward(self, query: Tensor, key: Tensor, value: Tensor, mask: Tensor = None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "           query:         query vector         (batch_size, q_length, d_model)\n",
        "           key:           key vector           (batch_size, k_length, d_model)\n",
        "           value:         value vector         (batch_size, s_length, d_model)\n",
        "           mask:          mask for decoder\n",
        "\n",
        "        Returns:\n",
        "           output:        attention values     (batch_size, q_length, d_model)\n",
        "           attn_probs:    softmax scores       (batch_size, n_heads, q_length, k_length)\n",
        "        \"\"\"\n",
        "        batch_size = key.size(0)\n",
        "\n",
        "        # calculate query, key, and value tensors\n",
        "        # keep the same key logic\n",
        "        K = self.Wk(key)\n",
        "        # replace with LoRA query logic\n",
        "        \"\"\"TO DO\"\"\"\n",
        "        Q =\n",
        "        # replace with LoRA value logic\n",
        "        \"\"\"TO DO\"\"\"\n",
        "        V =\n",
        "\n",
        "        # split each tensor into n-heads to compute attention\n",
        "        # query tensor\n",
        "        Q = Q.view(\"\"\"TO DO\"\"\").permute(0, 2, 1, 3)\n",
        "\n",
        "        # key tensor\n",
        "        K = K.view(\"\"\"TO DO\"\"\")).permute(0, 2, 1, 3)\n",
        "\n",
        "        # value tensor\n",
        "        V = V.view(\"\"\"TO DO\"\"\")).permute(0, 2, 1, 3)\n",
        "\n",
        "        # computes attention\n",
        "        # scaled dot product -> QK^{T}\n",
        "        scaled_dot_prod = torch.matmul(Q, K.permute(0, 1, 3, 2)) / math.sqrt(self.d_key)\n",
        "\n",
        "        # fill those positions of product as (-1e10) where mask positions are 0\n",
        "        if mask is not None:\n",
        "          scaled_dot_prod = scaled_dot_prod.masked_fill(mask == 0, -1e10)\n",
        "\n",
        "        # apply softmax\n",
        "        attn_probs = torch.softmax(scaled_dot_prod, dim=-1)\n",
        "\n",
        "        # multiply by values to get attention\n",
        "        A = torch.matmul(self.dropout(attn_probs), V)\n",
        "\n",
        "        # reshape attention back\n",
        "        A = A.permute(0, 2, 1, 3).contiguous()\n",
        "        A = A.view(batch_size, -1, self.n_heads*self.d_key)\n",
        "\n",
        "        # push through the final weight layer\n",
        "        output = self.Wo(A)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "ou6cDBDBSIkS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LoRA Wrapper:\n",
        "\n",
        "Here, we define a wrapper to create a Transformer model with LoRA."
      ],
      "metadata": {
        "id": "zndgbi1RUD7o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LoRAWrapper(nn.Module):\n",
        "    def __init__(self, task: str, encoder: Encoder,pos_enc: PositionalEncoding,\n",
        "                 d_model: int, n_layers: int, n_heads: int, pad_idx: int,\n",
        "                 device, in_features: int = 4485, d_ffn: int = 512,\n",
        "                 dropout_rate: float = 0.1, lora_rank: int = 8,\n",
        "                 max_length: int = 20, num_classes: int = None,\n",
        "                 train_biases: bool = True, train_layer_norms: bool = True):\n",
        "        \"\"\"\n",
        "        Initializes a LoRAWrapper instance, which is a wrapper around the Transformer incorporating\n",
        "        Low-Rank Adaptation (LoRA) to efficiently retrain the model for different tasks.\n",
        "        LoRA allows for effective adaptation of large pre-trained models with minimal updates.\n",
        "\n",
        "        Args:\n",
        "          task:                 type of task to configure the model for: {'diabdeep', 'mhdeep'}\n",
        "                                for 'diabdeep', the number of classes is 3, and 4 for 'mhdeep'\n",
        "          encoder:              encoder module for the Transformer\n",
        "          pos_enc:              positionalEncoding module for input src\n",
        "          d_model:              dimension of embeddings\n",
        "          n_layers:             number of layers in the Transformer\n",
        "          n_heads:              number of attention heads\n",
        "          pad_idx:              padding index\n",
        "          device:               device used for training\n",
        "          in_features:          input feature dimensionality\n",
        "          d_ffn:                dimensionality for PositionwiseFeedForward module\n",
        "          dropout_rate:         dropout probability, randomly zeroes-out some of the input\n",
        "          lora_rank:            rank of the LoRA matrices\n",
        "          max_length:           max input sequence length\n",
        "          num_classes:          number of classes for the classification head\n",
        "          train_biases:         flag indicating whether to update bias parameters during training\n",
        "          train_layer_norms:    flag indicating whether to update the layer norms during training, usually this is a good idea.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        supported_tasks = ['diabdeep', 'mhdeep']\n",
        "        assert isinstance(task, str) and task.lower() in supported_tasks, f\"task has to be one of {supported_tasks}\"\n",
        "\n",
        "        if task == \"diabdeep\":\n",
        "            num_classes = 3\n",
        "        elif task == \"mhdeep\":\n",
        "            num_classes = 4\n",
        "\n",
        "        # 1. Initialize the base model with parameters\n",
        "        self.model = nn.Sequential(Transformer(encoder, pos_enc, pad_idx, device))\n",
        "        # load the pretrained weights\n",
        "        print(f\"Loading pre-trained weights from pre-training...\")\n",
        "        self.model.load_state_dict(torch.load(f'pretrain_comfort_mlm_exercise.pt', weights_only=True), strict=False)\n",
        "\n",
        "        self.base_model_param_count = count_parameters(self.model)\n",
        "\n",
        "        self.lora_rank = lora_rank\n",
        "        self.train_biases = train_biases\n",
        "        self.train_layer_norms = train_layer_norms\n",
        "\n",
        "        # 2. Save parameters and add the classifier head for the fine-tuning tasks\n",
        "        self.task = task.lower()\n",
        "        self.d_model = d_model\n",
        "        self.n_layers = n_layers\n",
        "        self.n_heads = n_heads\n",
        "        self.pad_idx = pad_idx\n",
        "        self.device = device\n",
        "        self.in_features = in_features\n",
        "        self.d_ffn = d_ffn\n",
        "        self.dropout = dropout_rate\n",
        "        self.max_length = max_length\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # 3. Define the additional classifier head\n",
        "        self.finetune_head_fc1 = nn.Linear(d_model, 512)\n",
        "        self.finetune_head_fc2 = nn.Linear(512, 128)\n",
        "        self.finetune_head_fc3 = nn.Linear(128, num_classes)\n",
        "\n",
        "        # 4. Set up the lora model for fine-tuning\n",
        "        self.replace_multihead_attention()\n",
        "        self.freeze_parameters_except_lora_and_bias()\n",
        "\n",
        "    def replace_multihead_attention(self, verbose = True):\n",
        "        \"\"\"\n",
        "        Replaces MultiHeadAttention with LoRAMultiHeadAttention in the model, which contains the LoRA logic and parameters.\n",
        "        \"\"\"\n",
        "        self.multiheadattention_replaced_modules = 0\n",
        "        self.positionwisefeedforward_replaced_modules = 0\n",
        "        self.replace_multihead_attention_recursion(self.model)\n",
        "        if verbose:\n",
        "            print(f\"Replaced {self.multiheadattention_replaced_modules} modules of MultiHeadAttention with LoRAMultiHeadAttention\")\n",
        "            print(f\"Replaced {self.positionwisefeedforward_replaced_modules} modules of PositionwiseFeedForward with LoRAPositionwiseFeedForward\")\n",
        "\n",
        "    def replace_multihead_attention_recursion(self, model):\n",
        "        \"\"\"\n",
        "        Recursively replaces MultiHeadAttention with LoRAMultiHeadAttention in the given model/module.\n",
        "        If some components are wrapped in another class this function can recursively apply the replacement to\n",
        "        find all instances of the Attention.\n",
        "        \"\"\"\n",
        "        # Model can also be a module if it contains sub-components\n",
        "        for name, module in model.named_children():\n",
        "            if isinstance(module, MultiHeadAttention): # or isinstance(module, PositionwiseFeedForward):\n",
        "\n",
        "                if isinstance(module, MultiHeadAttention):\n",
        "                    # Create a new LoRAMultiheadAttention layer\n",
        "                    new_layer = LoRAMultiHeadAttention(rank=self.lora_rank, d_model=self.d_model, n_heads=self.n_heads, dropout=self.dropout)\n",
        "                    self.multiheadattention_replaced_modules += 1\n",
        "\n",
        "                elif isinstance(module, PositionwiseFeedForward):\n",
        "                    # Create a new LoRAPositionwiseFeedForward layer\n",
        "                    new_layer = LoRAPositionwiseFeedForward(rank=self.lora_rank, d_model=self.d_model, d_ffn=self.d_ffn, dropout=self.dropout)\n",
        "                    self.positionwisefeedforward_replaced_modules += 1\n",
        "\n",
        "                # Get the state of the original layer\n",
        "                state_dict_old = module.state_dict()\n",
        "\n",
        "                # Load the state dict to the new layer\n",
        "                new_layer.load_state_dict(state_dict_old, strict=False)\n",
        "\n",
        "                # Get the state of the new layer\n",
        "                state_dict_new = new_layer.state_dict()\n",
        "\n",
        "                # Compare keys of both state dicts\n",
        "                keys_old = set(k for k in state_dict_old.keys() if not k.startswith(\"lora_\"))\n",
        "                keys_new = set(k for k in state_dict_new.keys() if not k.startswith(\"lora_\"))\n",
        "                assert keys_old == keys_new, f\"Keys of the state dictionaries don't match (ignoring lora parameters):\\n\\tExpected Parameters: {keys_old}\\n\\tNew Parameters (w.o. LoRA): {keys_new}\"\n",
        "\n",
        "                # Replace the original layer with the new layer\n",
        "                setattr(model, name, new_layer)\n",
        "            else:\n",
        "                # Recurse on the child modules\n",
        "                self.replace_multihead_attention_recursion(module)\n",
        "\n",
        "    def freeze_parameters_except_lora_and_bias(self):\n",
        "        \"\"\"\n",
        "        Freezes all parameters in the model, except those in LoRA layers and bias parameters, if specified.\n",
        "        All LoRA parameters are identified by having a name that starts with *lora_*.\n",
        "        \"\"\"\n",
        "        for name, param in self.model.named_parameters():\n",
        "            if (\"lora_\" in name) or (\"finetune_head_\" in name) or (self.train_biases and \"bias\" in name) \\\n",
        "                or (self.train_layer_norms and \"LayerNorm\" in name):\n",
        "                param.requires_grad = True\n",
        "            else:\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def make_src_mask(self, src: Tensor):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            src:          raw sequences with padding        (batch_size, seq_length)\n",
        "\n",
        "        Returns:\n",
        "            src_mask:     mask for each sequence            (batch_size, 1, 1, seq_length)\n",
        "        \"\"\"\n",
        "        # assign 1 to tokens that need attended to and 0 to padding tokens, then add 2 dimensions\n",
        "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "        return src_mask\n",
        "\n",
        "    def forward(self, src: Tensor):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            src:          raw src sequences                 (batch_size, src_seq_length)\n",
        "\n",
        "        Returns:\n",
        "            src:          sequences after the model         (batch_size, trg_seq_length, output_dim)\n",
        "        \"\"\"\n",
        "        # push the src through the model\n",
        "        src = self.model(src)       # (batch_size, src_seq_length, d_model)\n",
        "        src = torch.mean(src, 1)    # (batch_size, d_model)\n",
        "        src = self.finetune_head_fc1(src).relu()\n",
        "        src = self.finetune_head_fc2(src).relu()\n",
        "        src = self.finetune_head_fc3(src)\n",
        "        return src\n",
        "\n",
        "    def save_lora_state_dict(self, lora_filepath: Optional[Union[str, Path]] = None) -> Optional[Dict]:\n",
        "        \"\"\"\n",
        "        Save the trainable parameters of the model into a state dict.\n",
        "        If a file path is provided, it saves the state dict to that file.\n",
        "        If no file path is provided, it simply returns the state dict.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        lora_filepath : Union[str, Path], optional\n",
        "            The file path where to save the state dict. Can be a string or a pathlib.Path. If not provided, the function\n",
        "            simply returns the state dict without saving it to a file.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        Optional[Dict]\n",
        "            If no file path was provided, it returns the state dict. If a file path was provided, it returns None after saving\n",
        "            the state dict to the file.\n",
        "        \"\"\"\n",
        "        # Create a state dict of the trainable parameters\n",
        "        state_dict = {name: param for name, param in self.named_parameters() if param.requires_grad}\n",
        "\n",
        "        # add addional parameters to state dict\n",
        "        state_dict['task'] = self.task\n",
        "        state_dict['d_model'] = self.d_model\n",
        "        state_dict['n_layers'] = self.n_layers\n",
        "        state_dict['n_heads'] = self.n_heads\n",
        "        state_dict['pad_idx'] = self.pad_idx\n",
        "        state_dict['device'] = self.device\n",
        "        state_dict['in_features'] = self.in_features\n",
        "        state_dict['d_ffn'] = self.d_ffn\n",
        "        state_dict['lora_rank'] = self.lora_rank\n",
        "        state_dict['dropout_rate'] = self.dropout\n",
        "        state_dict['max_length'] = self.max_length\n",
        "        state_dict['num_classes'] = self.num_classes\n",
        "\n",
        "        if lora_filepath is not None:\n",
        "            # Convert string to pathlib.Path if necessary\n",
        "            if isinstance(lora_filepath, str):\n",
        "                lora_filepath = Path(lora_filepath)\n",
        "\n",
        "            # Save the state dict to the specified file\n",
        "            torch.save(state_dict, lora_filepath)\n",
        "        else:\n",
        "            # Return the state dict if no file path was provided\n",
        "            return state_dict\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def load_lora_state_dict(lora_parameters: Union[str, Path, Dict] = None):\n",
        "        \"\"\"\n",
        "        Load a state dict into the model from a specified file path or a state dict directly.\n",
        "        This is a staticmethod to be used from the base clase, returning a fully initialized and LoRA loaded model.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        lora_parameters : Union[str, Path, Dict]\n",
        "            Either the file path to the state dict (can be a string or pathlib.Path) or the state dict itself. If a file path\n",
        "            is provided, the function will load the state dict from the file. If a state dict is provided directly, the function\n",
        "            will use it as is.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        LoRAWrapper object, initialized and with the LoRA weights loaded.\n",
        "        \"\"\"\n",
        "        # Check if a filepath or state dict was provided\n",
        "        if lora_parameters is not None:\n",
        "            # Convert string to pathlib.Path if necessary\n",
        "            if isinstance(lora_parameters, str):\n",
        "                lora_parameters = Path(lora_parameters)\n",
        "\n",
        "            # If the provided object is a Path, load the state dict from file\n",
        "            if isinstance(lora_parameters, Path):\n",
        "                state_dict = torch.load(lora_parameters, weights_only=True)\n",
        "            else:\n",
        "                # If it's not a Path, assume it's a state dict\n",
        "                state_dict = lora_parameters\n",
        "        else:\n",
        "            raise ValueError(\"No filepath or state dict provided\")\n",
        "\n",
        "        encoder = Encoder(state_dict['d_model'], state_dict['n_layers'], state_dict['n_heads'], state_dict['d_ffn'], state_dict['dropout_rate'])\n",
        "        pos_enc = PositionalEncoding(state_dict['d_model'], state_dict['dropout_rate'], state_dict['max_length'])\n",
        "\n",
        "        print(\"\\nLoad trained LoRA weights...\")\n",
        "        instance = LoRAWrapper(task=state_dict['task'], encoder=encoder,\n",
        "                               pos_enc=pos_enc, d_model=state_dict['d_model'],\n",
        "                               n_layers=state_dict['n_layers'], n_heads=state_dict['n_heads'],\n",
        "                               pad_idx=state_dict['pad_idx'], device=state_dict['device'],\n",
        "                               in_features=state_dict['in_features'], d_ffn=state_dict['d_ffn'],\n",
        "                               dropout_rate=state_dict['dropout_rate'], lora_rank = state_dict['lora_rank'],\n",
        "                               max_length=state_dict['max_length'], num_classes=state_dict['num_classes'])\n",
        "\n",
        "        # Load the state dict into the model\n",
        "        print(f\"Loading LoRA state dict from {lora_parameters}...\")\n",
        "        instance.load_state_dict(state_dict, strict=False)\n",
        "\n",
        "        return instance"
      ],
      "metadata": {
        "id": "oiOm4Sj__cxP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make the LoRA model:\n",
        "\n",
        "Here, we define a function `make_lora_model()` to instantiate a Transformer model with LoRA based on our health foundation model."
      ],
      "metadata": {
        "id": "za8hj4O9YdZn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_lora_model(task: str, device, d_model: int = 512,\n",
        "                    n_layers: int = 3, n_heads: int = 8, d_ffn: int = 2048,\n",
        "                    dropout: float = 0.1, in_features: int = 4485,\n",
        "                    pad_idx: int = 100, lora_rank: int = 8, max_length: int = 20):\n",
        "        \"\"\"\n",
        "        Construct a LoRA model when provided parameters.\n",
        "\n",
        "        Args:\n",
        "          task:         type of task to configure the model for: {'diabdeep', 'mhdeep'}\n",
        "          device:       device used for training\n",
        "          d_model:      dimension of embeddings\n",
        "          n_layers:     number of Encoder and Decoders\n",
        "          n_heads:      number of attention heads\n",
        "          d_ffn:        dimension of feed-forward network\n",
        "          dropout:      probability of dropout occurring\n",
        "          in_features:  input feature dimensionality\n",
        "          pad_idx:      padding index\n",
        "          lora_rank:    rank of the LoRA matrices\n",
        "          max_length:   max input sequence length\n",
        "\n",
        "        Returns:\n",
        "          model:        the LoRA model\n",
        "        \"\"\"\n",
        "        # create the encoder\n",
        "        encoder = Encoder(d_model, n_layers, n_heads, d_ffn, dropout)\n",
        "\n",
        "        # create a positional encoding matrix\n",
        "        pos_enc = PositionalEncoding(d_model, dropout, max_length)\n",
        "\n",
        "        # create the LoRA model\n",
        "        model = LoRAWrapper(task, encoder, pos_enc, d_model, n_layers, n_heads,\n",
        "                            pad_idx, device, in_features=in_features, d_ffn=d_ffn,\n",
        "                            lora_rank=lora_rank, max_length=max_length)\n",
        "        return model"
      ],
      "metadata": {
        "id": "OQknwoTMYYuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load data from the DiabDeep dataset and create the datasets and dataloaders:\n",
        "\n",
        "The dataset has been processed into the training, validation, and test sets for you. You just need to load the corresponding experimental data.\n",
        "\n",
        "First, we use Numpy to load the experimental data and transform them into torch tensors. Then, we create the training, validation, and test sets using the `CustomDataset` class. Finally, we instantiate dataloaders with PyTorch's [DataLoader](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)."
      ],
      "metadata": {
        "id": "fFZPBjYumw2W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# unzip the DiabDeep datasets\n",
        "!unzip '/content/drive/Shareddrives/ECE477 datasets/Assignment11/diabdeep_data.zip' -d diabdeep_data\n",
        "\n",
        "# check out the data\n",
        "print('\\n', os.listdir('diabdeep_data'))\n",
        "\n",
        "# load the data and transform them to torch tensor\n",
        "\"\"\"TO DO\"\"\"\n",
        "x_train =\n",
        "y_train =\n",
        "x_valid =\n",
        "y_valid =\n",
        "x_test  =\n",
        "y_test  =\n",
        "\n",
        "print('x_train shape:', x_train.shape, 'y_train shape:', y_train.shape)\n",
        "print('x_valid shape:', x_valid.shape, 'y_valid shape:', y_valid.shape)\n",
        "print('x_test shape:', x_test.shape, 'y_test shape:', y_test.shape)\n",
        "\n",
        "train_batch_size = 128    # batch size for the training set\n",
        "test_batch_size = 128     # batch size for the validation and test sets\n",
        "\n",
        "# instantiate the datasets with CustomDataset\n",
        "\"\"\"TO DO\"\"\"\n",
        "train_set =\n",
        "valid_set =\n",
        "test_set =\n",
        "# instantiate the dataloaders, set shuffle=True\n",
        "\"\"\"TO DO\"\"\"\n",
        "train_loader =\n",
        "valid_loader =\n",
        "test_loader ="
      ],
      "metadata": {
        "id": "6vKdHxJXmtg3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Continual fine-tuning the health foundation model with LoRA: (2 pt)\n",
        "\n",
        "Here, we fine-tune our health foundation model for the DiabDeep detection task with LoRA. The functions used here, train(), evaluate(), test(), and report(), are defined in `utils.py` in the shared folder."
      ],
      "metadata": {
        "id": "adrBvFVoQqTW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set a fixed random seed for reproducibility\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if device == 'cuda':\n",
        "  torch.cuda.manual_seed(SEED)\n",
        "  torch.cuda.manual_seed_all(SEED)\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  torch.backends.cudnn.benchmark = False\n",
        "\n",
        "task = 'diabdeep'\n",
        "lora_rank = 8\n",
        "num_classes = 3\n",
        "epochs = 30\n",
        "\n",
        "# use make_lora_model() to instantiate a Transformer model to be fine-tuned with LoRA\n",
        "\"\"\"TO DO\"\"\"\n",
        "model =\n",
        "# cast the model to `device`\n",
        "\"\"\"TO DO\"\"\"\n",
        "\n",
        "\n",
        "# initialize the Adam optimizer\n",
        "\"\"\"TO DO\"\"\"\n",
        "optimizer =\n",
        "# set the loss criterion as the CrossEntropyLoss()\n",
        "\"\"\"TO DO\"\"\"\n",
        "criterion =\n",
        "\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_acc = float('-inf')\n",
        "\n",
        "# loop through each epoch\n",
        "for epoch in range(epochs):\n",
        "    # use train() to calculate the train loss and update the parameters\n",
        "    train_loss = train(model, train_loader, optimizer, criterion, CLIP)\n",
        "    # use evaluate() to calculate the train accuracy\n",
        "    _, train_acc = evaluate(model, train_loader, criterion, PAD_NUM)\n",
        "\n",
        "    # use evaluate() to calculate the loss and accuracy on the validation set\n",
        "    valid_loss, valid_acc = evaluate(model, valid_loader, criterion, PAD_NUM)\n",
        "\n",
        "    # save the model when it performs better than the previous run\n",
        "    if valid_acc >= best_valid_acc:\n",
        "        best_valid_acc = valid_acc\n",
        "        model.save_lora_state_dict(f\"comfort_{task}.pt\")\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02}')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc:.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc:.3f}')\n",
        "\n",
        "# load the weights\n",
        "model = LoRAWrapper.load_lora_state_dict(f'comfort_{task}.pt').to(device)\n",
        "\n",
        "# use evaluate() to calculate the loss on the test set\n",
        "test_loss, _ = evaluate(model, test_loader, criterion, PAD_NUM)\n",
        "# use test() to calculate the test accuracy\n",
        "test_acc = test(model, test_loader, PAD_NUM)\n",
        "\n",
        "print(f'\\nTask: {task}')\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc:.3f}\\n')\n",
        "report(model, task, device, x_test, y_test)"
      ],
      "metadata": {
        "id": "SeBpCT9anH9P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}