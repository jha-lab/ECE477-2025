{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a1036c8-5b1f-41f6-8cfd-1263f3298300",
   "metadata": {},
   "source": [
    "# Important notice: any use of generative AI for completing the assignment is strictly prohibited."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ed16ec-ccc9-4446-b6d1-d7317d10fd9d",
   "metadata": {},
   "source": [
    "### Note: if working in Colab, don't forget to select runtime type in Colab: GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c534ea5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.mixture import GaussianMixture as GMM\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "import math as ma\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import statistics\n",
    "from six import StringIO\n",
    "\n",
    "from itertools import chain\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e372c29-539f-496c-8ab3-198203e731dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use that if working in colab\n",
    "# permit Colab access your google drive, and select Princeton account\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0fde24-2edd-42c8-93f4-c3d2b9fcdeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you should be added as viewer to shared Google drive \"ECE477 datasets\"\n",
    "#  at https://drive.google.com/drive/u/0/folders/0ABIZHKB-QPnRUk9PVA\n",
    "\n",
    "!unzip \"/content/drive/Shared drives/ECE477 datasets/Assignment4/three-way_classification_real_data.zip\" -d \".\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36d5357-6f72-4340-a443-fa5b9ed725f7",
   "metadata": {},
   "source": [
    "## Warning: to ensure the reproducibility of your results and to achieve the full grade, do not change or remove RANDOM_STATE variables and setting random seed statements. If you remove or change them, you may not get the full grade. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83026d28",
   "metadata": {},
   "source": [
    "# Part 1: synthetic data generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b24cf34",
   "metadata": {},
   "source": [
    "## Define generators.\n",
    "\n",
    "We will use Kernel Density Estimation, Gaussian Mixture Models and Multivariate Normal Distribution methods to generate synthetic data from the read data distribtion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed513344",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "RANDOM_STATE = 0\n",
    "np.random.seed(RANDOM_STATE)\n",
    "random.seed(RANDOM_STATE)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9514b296-ee5b-4125-ab6b-2791f02d9531",
   "metadata": {},
   "source": [
    "### Task: fit Gaussian Mixture Model (GMM) (2 points)\n",
    "\n",
    "To get full points, you also should report correct best number of components later (when prompted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b0623a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating synthetic data based on GMM model - The number of Gaussians is selected based on maximizing\n",
    "# the sum of the log-probabilities of instances in the validation set\n",
    "def GMM_sample_generation(X_train, X_validation):\n",
    "    max_components = 5\n",
    "    n_components = np.arange(1, max_components, 1)\n",
    "    sum_array = np.zeros((max_components - 1))\n",
    "    for i in n_components:\n",
    "        # define and fit GMM model with covariance_type full, RANDOM_STATE\n",
    "        # your code is here\n",
    "        model = ...\n",
    "        ...\n",
    "        \n",
    "        # calc Log-likelihood over validation dataset for i components \n",
    "        # and save it to sum_array.\n",
    "        # hint: call .score_samples()\n",
    "        # your code is here\n",
    "        log_likelihoods = ...\n",
    "        sum_array[i - 1] = ...\n",
    "        print(f\"Accumulated log likelihood for {i} components:\", sum_array[i - 1])\n",
    "        \n",
    "    n_components = np.argmax(sum_array)\n",
    "    n_components = n_components + 1\n",
    "    print(\"Number of components: {}\".format(n_components))\n",
    "\n",
    "    # define and fit the model with best number of components\n",
    "    # your code is here\n",
    "    gmm = ...\n",
    "    ...\n",
    "    print(gmm.converged_)\n",
    "\n",
    "    out = gmm.score(X_validation)\n",
    "    print(out)\n",
    " \n",
    "    X_syn = gmm.sample(100000)\n",
    "    print(X_syn[0].shape)\n",
    "    return X_syn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568e6b3b",
   "metadata": {},
   "source": [
    "### Task 1a: fit Kernel Density Estimation Model (KDE) (3 points)\n",
    "\n",
    "To get full points, you also should report correct best bandwidth later (when prompted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581c2e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating synthetic data based on KDE model - The bandwidth is selected based on maximizing\n",
    "# the sum of the log-probabilities of instances in the validation set\n",
    "def KDE_sample_generation(X_train, X_validation):\n",
    "    bw_list = [0.01, 0.05, 0.1, 0.2, 0.3]\n",
    "    log_like = np.zeros((len(bw_list)))\n",
    "    \n",
    "    # fit KDE with gaussian kernel and all bandwidth, score them on validation dataset\n",
    "    # and store their score at log_like array\n",
    "    for bw in bw_list:\n",
    "        # your code is here\n",
    "        \n",
    "    j = np.argmax(log_like)\n",
    "    # fit the model with the highest score \n",
    "    # your code is here\n",
    "    kde = ...\n",
    "    ...\n",
    "    out = kde.score(X_validation)\n",
    "    print(out)\n",
    "    X_syn = kde.sample(100000)\n",
    "    print(X_syn.shape)\n",
    "\n",
    "    return X_syn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1308a433",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_generation (X_train, X_validation, method):\n",
    "    \"\"\"supported methods: GMM, KDE\"\"\"\n",
    "    count = 100000\n",
    "    if method == \"GMM\":\n",
    "        X_syn_out = GMM_sample_generation(X_train, X_validation)\n",
    "        X_syn = X_syn_out[0]\n",
    "    elif method == \"KDE\":\n",
    "        X_syn = KDE_sample_generation(X_train, X_validation)\n",
    "    else:\n",
    "        print (\"method not supported\")\n",
    "    return X_syn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264a63f8-54f8-49af-a561-718c818c2c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_real_data = 'three-way_classification_real_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391b3fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "X_train, y_train, X_validation, y_validation = (\n",
    "        np.load(os.path.join(path_to_real_data, 'X_train.npy')),\n",
    "        np.load(os.path.join(path_to_real_data, 'y_train.npy')),\n",
    "        np.load(os.path.join(path_to_real_data, 'X_validation.npy')),\n",
    "        np.load(os.path.join(path_to_real_data, 'y_validation.npy')),\n",
    ")\n",
    "\n",
    "X_train_original = X_train\n",
    "X_validation_original = X_validation\n",
    "\n",
    "X_train_original.shape, X_validation_original.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6b29cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the feature dictionary for various sensors for real data\n",
    "featdict = {\n",
    "    \"GSR\": list(np.arange(3900,3960)),\n",
    "    \"Temp\": list(np.arange(3960,4020)),\n",
    "    \"IBI\" : list(np.arange(4080,4140)),\n",
    "    \"Ox\" : list(np.arange(4146,4147)),\n",
    "    \"BP\" : list(np.arange(4160,4162)),\n",
    "    \"Q\": list(np.arange(4147,4158))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13ae0bc",
   "metadata": {},
   "source": [
    "## Generating the synthetic data based on 6 set of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadaddac",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = np.array(featdict[\"GSR\"] + featdict[\"Temp\"] + featdict[\"IBI\"] + featdict[\"Ox\"] + featdict [\"BP\"] + featdict[\"Q\"]).reshape(-1)\n",
    "X_train  = X_train_original[:, indexes]\n",
    "X_validation = X_validation_original[:, indexes]\n",
    "\n",
    "X_train.shape, X_validation.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0694be",
   "metadata": {},
   "source": [
    "### Task 1b: fitting Random Forest Classifier (2 points)\n",
    "\n",
    "#### We fit RF on real data, and then label synthetically generated data with this classifier\n",
    "\n",
    "To get full points, you also should report correct best tree depth later (when prompted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847c3496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the most accurate model on the validation set, will be used later to label the synthetic data\n",
    "validation_acc = []\n",
    "train_acc = []\n",
    "\n",
    "RANDOM_STATE_RF = 11\n",
    "\n",
    "print(\"Fitting Random Forest Classifier\")\n",
    "for i in range (1, 10):\n",
    "    print(f'current max_depth: {i}')\n",
    "    # fit Random Forest with Gini criterion, current max_depth and RANDOM_STATE_RF as random_state\n",
    "    # store its validation accuracy (call \"accuracy_score\" function to calculate it)\n",
    "    # your code is here\n",
    "    \n",
    "print (validation_acc)\n",
    "tree_depth = np.argmax(validation_acc) + 1\n",
    "print(\"Best tree depth\", tree_depth)\n",
    "\n",
    "# fit the best performing classifier here, we will use it further\n",
    "# make sure to set RANDOM_STATE_RF\n",
    "# your code is here\n",
    "clf = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db20774a",
   "metadata": {},
   "source": [
    "### Report the best tree depth:\n",
    "\n",
    "you answer is here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ef9395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the method of synthetic data generation:\n",
    "method = 'GMM'\n",
    "X_syn = sample_generation(X_train, X_validation, method)\n",
    "#Labeling the synthetic data basesd on the most accurate random forest model on the validation set\n",
    "y_syn = np.zeros((X_syn.shape[0]))\n",
    "y_syn = clf.predict(X_syn)\n",
    "print(y_syn.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653468fc",
   "metadata": {},
   "source": [
    "### Report the best number of components:\n",
    "\n",
    "you answer is here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2454db",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Selecting the method of synthetic data generation:\n",
    "method = 'KDE'\n",
    "X_syn = sample_generation(X_train, X_validation, method)\n",
    "\n",
    "#Labeling the synthetic data based on the most accurate random forest model on the validation set\n",
    "y_syn = np.zeros((X_syn.shape[0]))\n",
    "y_syn = clf.predict(X_syn)\n",
    "print (y_syn.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8427095c",
   "metadata": {},
   "source": [
    "### Report the best bandwidth:\n",
    "\n",
    "you answer is here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decdbc6e-b786-4718-9a82-710425244a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_synthetic_data = 'synthetic_data_generated'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1129e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# save synthetic datasets\n",
    "for method in ('GMM', 'KDE'):\n",
    "    path_to_save = os.path.join(path_synthetic_data, method)\n",
    "    os.makedirs(path_to_save, exist_ok=True)\n",
    "    np.save(os.path.join(path_to_save, 'X-syn.npy'), X_syn)\n",
    "    np.save(os.path.join(path_to_save, 'y-syn.npy'), y_syn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d577334",
   "metadata": {},
   "source": [
    "# Part 2. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e1307d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# definig the device variable to use GPU if it is available\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2362b2a6",
   "metadata": {},
   "source": [
    "### Task 2a: define two missing network layers and metrcis (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba28a4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network and function definitions\n",
    "\n",
    "class DynamicNet5(torch.nn.Module):\n",
    "    def __init__(self, D_in, H_1=256, H_2=128, H_3=128, D_out=4):\n",
    "        super(DynamicNet5, self).__init__()\n",
    "        # your code is here\n",
    "        self.fc2 = ...\n",
    "        ...\n",
    "\n",
    "        self.fc4 = nn.Linear(H_3, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "        x = F.leaky_relu(self.fc2(x))\n",
    "        x = F.leaky_relu(self.fc3(x))\n",
    "        y_pred = (self.fc4(x))\n",
    "        return y_pred\n",
    "    \n",
    "    \n",
    "def metrics (TP, TN, FP, FN):\n",
    "    # your code is here\n",
    "\n",
    "    return F1, FPR, FNR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7d9b32",
   "metadata": {},
   "source": [
    "### Reload real data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cb4acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_validation, y_validation, X_test, y_test = (\n",
    "        np.load(os.path.join(path_to_real_data, 'X_train.npy')),\n",
    "        np.load(os.path.join(path_to_real_data, 'y_train.npy')),\n",
    "        np.load(os.path.join(path_to_real_data, 'X_validation.npy')),\n",
    "        np.load(os.path.join(path_to_real_data, 'y_validation.npy')),\n",
    "        np.load(os.path.join(path_to_real_data, 'X_test.npy')),\n",
    "        np.load(os.path.join(path_to_real_data, 'y_test.npy')),\n",
    ")\n",
    "\n",
    "# Copying the data into new variables\n",
    "# this is done since we later want to different feature sets of our data\n",
    "X_train_original = X_train\n",
    "X_validation_original = X_validation\n",
    "X_test_original = X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b3cfc8",
   "metadata": {},
   "source": [
    "### Load synthetic data\n",
    "In this asssignment, we will use only KDE generated data (you should have generated them on the previous step)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa150a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'KDE'\n",
    "\n",
    "X_syn = np.load (f'{path_synthetic_data}/{method}/X-syn.npy')\n",
    "y_syn = np.load (f'{path_synthetic_data}/{method}/y-syn.npy')\n",
    "\n",
    "X_syn_original = X_syn\n",
    "\n",
    "#feature dictinary for synthetic data|\n",
    "featdict_syn = {\n",
    "    \"GSR\": list(np.arange(0,60)),\n",
    "    \"Temp\": list(np.arange(60,120)),\n",
    "    \"IBI\" : list(np.arange(120,180)),\n",
    "    \"Ox\" : list(np.arange(180,181)),\n",
    "    \"BP\" : list(np.arange(181,183)),\n",
    "    \"Q\": list(np.arange(183,194))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d838b083",
   "metadata": {},
   "source": [
    "### Generate all the subsets of feature categories\n",
    "Since covidDeep has 6 feature categories, we generate all the subsets of these 6 categories here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75efb0e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get feature combinations for training\n",
    "\n",
    "combs_array = [\n",
    "    ('GSR', 'Temp', 'IBI', 'Ox', 'BP'),\n",
    "    ('Temp', 'IBI', 'Ox', 'BP', 'Q'),\n",
    "    ('GSR', 'Temp', 'Ox', 'BP', 'Q'),\n",
    "    ('GSR', 'Temp', 'IBI', 'Ox', 'BP', 'Q'),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc612908",
   "metadata": {},
   "source": [
    "## Train DNN-1 models (models trained on real data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6737a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_save_models = 'trained_models'\n",
    "\n",
    "# DNN-1 models are ones trained on real data\n",
    "path_dnn1 = os.path.join(path_to_save_models, 'DNN-1')\n",
    "os.makedirs(path_dnn1, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5fbd9c",
   "metadata": {},
   "source": [
    "### Task 2b: define optimizer and classification loss (2 points)\n",
    "\n",
    "optimizer: Adam, learning rate: 1e-3\n",
    "classification loss: Cross Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25b8372",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# ==============\n",
    "# Random states:\n",
    "np.random.seed(RANDOM_STATE)\n",
    "random.seed(RANDOM_STATE)\n",
    "\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "torch.cuda.manual_seed(RANDOM_STATE)\n",
    "# ==============\n",
    "\n",
    "saved_metrics = {\n",
    "    key: {} for key in combs_array\n",
    "}\n",
    "\n",
    "\n",
    "for cmb in combs_array:\n",
    "    indices = list()\n",
    "\n",
    "    print(\"For features:\", cmb)\n",
    "    for feat in cmb:\n",
    "        indices += featdict[feat]\n",
    "    # corresponding indices in the real data\n",
    "    indices = np.array(indices).reshape(-1)\n",
    "\n",
    "    # considering a subset of features y slicing the feature space\n",
    "    X_train  = X_train_original[:, indices]\n",
    "    X_validation = X_validation_original[:, indices]\n",
    "    X_test = X_test_original[:, indices]\n",
    "\n",
    "    # converting the data into torch tensors\n",
    "    x = torch.from_numpy(X_train).float()\n",
    "    y = torch.from_numpy(y_train.reshape(-1)).long()\n",
    "\n",
    "    # putting the data on 'device'\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "\n",
    "    m = X_train.shape[1]\n",
    "    print (\"Number of features: \", m)\n",
    "    model = DynamicNet5(m)\n",
    "    model = model.to(device)\n",
    "\n",
    "    \n",
    "    # set ADAM optimizer with lr= 1e-3 and loss\n",
    "    # your code is here\n",
    "    criterion = ...\n",
    "    optimizer = ...\n",
    "\n",
    "    # training loop (the number of epochs can be explored)\n",
    "    print(\"Training loop...\")\n",
    "    for t in range(1000):\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(model(x), y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # define confusion matrix\n",
    "    cm = np.zeros((4, 4), dtype=int)\n",
    "    TP = TN = FP = FN = 0\n",
    "\n",
    "    # evaluation on test set without computing gradients (with torch.no_grad())\n",
    "    y_pred_array = []\n",
    "    correct = 0\n",
    "    total = X_test.shape[0]\n",
    "    print('Running evaluation...')\n",
    "    with torch.no_grad():\n",
    "        for i in range(total):\n",
    "            # Convert single test sample and label to tensors, move to device\n",
    "            x_t = torch.from_numpy(X_test[i, :]).float().to(device)\n",
    "            y_t = torch.tensor(y_test[i], dtype=torch.long).to(device)\n",
    "\n",
    "            # Forward pass through the model\n",
    "            output = model(x_t)\n",
    "            # Compute predicted class\n",
    "            y_pred = torch.argmax(output.cpu(), dim=-1).item()\n",
    "            y_pred_array.append(y_pred)\n",
    "            label = int(y_test[i].item())\n",
    "\n",
    "            # Update the confusion matrix\n",
    "            cm[label, y_pred] += 1\n",
    "\n",
    "            # Update correct predictions and binary classification counters\n",
    "            if y_pred == label:\n",
    "                correct += 1\n",
    "                if y_pred == 0:\n",
    "                    TN += 1\n",
    "                else:\n",
    "                    TP += 1\n",
    "            else:\n",
    "                if y_pred == 0:\n",
    "                    FN += 1\n",
    "                else:\n",
    "                    FP += 1\n",
    "\n",
    "    # After processing all samples, extract confusion matrix values:\n",
    "    T1, T2, T3, T4 = cm[0, 0], cm[1, 1], cm[2, 2], cm[3, 3]\n",
    "    F12, F13, F14 = cm[0, 1], cm[0, 2], cm[0, 3]\n",
    "    F21, F23, F24 = cm[1, 0], cm[1, 2], cm[1, 3]\n",
    "    F31, F32, F34 = cm[2, 0], cm[2, 1], cm[2, 3]\n",
    "    F41, F42, F43 = cm[3, 0], cm[3, 1], cm[3, 2]\n",
    "\n",
    "    test_accuracy_original = 100 * correct/total\n",
    "    print ('Test Accuracy:', round(test_accuracy_original, 3))\n",
    "\n",
    "    print(f\"T1: {T1}, T2: {T2}, T3: {T3}, T4: {T4}\")\n",
    "    print(f\"F12: {F12}, F13: {F13}, F14: {F14}\")\n",
    "    print(f\"F21: {F21}, F23: {F23}, F24: {F24}\")\n",
    "    print(f\"F31: {F31}, F32: {F32}, F34: {F34}\")\n",
    "    print(f\"F41: {F41}, F42: {F42}, F43: {F43}\")\n",
    "\n",
    "    print (\"True positive: \", TP)\n",
    "    print (\"False positive: \", FP)\n",
    "    print (\"True negative: \", TN)\n",
    "    print (\"False negative: \", FN)\n",
    "\n",
    "    # computing the performance metrics\n",
    "    F1, FPR, FNR = metrics(TP, TN, FP, FN)\n",
    "    print(\"Aggregated metrics:\")\n",
    "    print (\"False positve rate: \",  round(FPR, 3))\n",
    "    print (\"False negative rate: \",  round(FNR, 3))\n",
    "    print (\"F1 Score: \",  round(F1, 3))\n",
    "    y_pred_array = np.array(y_pred_array)\n",
    "\n",
    "    F1_original = 100 * f1_score(y_test, y_pred_array, average='macro')\n",
    "    FPR_original = 100 * ((F12+F13+F14)/(F12+F13+F14+T1)) if (F12+F13+F14+T1) != 0 else np.nan\n",
    "    FNR2_original = 100 * ((F21+F23+F24)/(F21+F23+F24+T2)) if (F21+F23+F24+T2) != 0 else np.nan\n",
    "    FNR3_original = 100 * ((F31+F32+F34)/(F31+F32+F34+T3)) if (F31+F32+F34+T3) != 0 else np.nan\n",
    "    FNR4_original = 100 * ((F41+F42+F43)/(F41+F42+F43+T4)) if (F41+F42+F43+T4) != 0 else np.nan\n",
    "\n",
    "    print(\"Per class metrics:\")\n",
    "    print (\"False positve rate: 1\",  round(FPR_original, 3))\n",
    "    print (\"False negative rate 2: \",  round(FNR2_original, 3))\n",
    "    print (\"False negative rate 3\",  round(FNR3_original, 3))\n",
    "    print (\"False negative rate 4: \",  round(FNR4_original, 3))\n",
    "    print (\"F1 Score: \",  round(F1_original, 3))\n",
    "\n",
    "    saved_metrics[cmb] = {\n",
    "        'acc': round(test_accuracy_original, 3),\n",
    "        'FPR': round(FPR, 3),\n",
    "        'FNR': round(FNR, 3),\n",
    "        'F1': round(F1_original, 3),\n",
    "    }\n",
    "\n",
    "    # Saving the nodel for DNN 1, using the name of features used in naming the saved file\n",
    "    save_to = os.path.join(path_dnn1, '_'.join(cmb) + '.pth')\n",
    "    torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict()\n",
    "            }, save_to)\n",
    "\n",
    "    print(f'saved to {save_to}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006c5b6b",
   "metadata": {},
   "source": [
    "### Task 2c: class 4 metrics (1 point)\n",
    "Why do you think model never misclassifies class 4 as other classes (neither FP, neither FN)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859f40e1",
   "metadata": {},
   "source": [
    "**your answer is here**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f0cb0f",
   "metadata": {},
   "source": [
    "## Train DNN-2 models (models pretrained on synthetic data and trained on real data)\n",
    "\n",
    "In this part, the model is pretrained on synthetic data generated in the first part for small number of epochs, and then trained on real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5da41a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DNN-2 models are ones pretrained with synthetic and then trained on real data\n",
    "path_dnn2 = os.path.join(path_to_save_models, 'DNN-2')\n",
    "os.makedirs(path_dnn2, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4765e8ca",
   "metadata": {},
   "source": [
    "### Task 2d: define training loops (3 points)\n",
    "\n",
    "(1) Define optimizer and loss as in the previous part; \n",
    "\n",
    "(2) Define pretraining on synthetic data and then (3) training or real data (fill in 3 gaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32776459-21a4-4814-9e57-a3aae831e45e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ==============\n",
    "# Random states:\n",
    "np.random.seed(RANDOM_STATE)\n",
    "random.seed(RANDOM_STATE)\n",
    "\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "torch.cuda.manual_seed(RANDOM_STATE)\n",
    "# ==============\n",
    "\n",
    "saved_metrics2 = {\n",
    "    key: {} for key in combs_array\n",
    "}\n",
    "\n",
    "for cmb in combs_array:\n",
    "    indices = list()\n",
    "    indices_syn = list()\n",
    "    \n",
    "    print(\"For features:\", cmb)\n",
    "    for feat in cmb:\n",
    "        indices += featdict[feat]\n",
    "        indices_syn += featdict_syn[feat]\n",
    "    # corresponding indices in the real data\n",
    "    indices = np.array(indices).reshape(-1)\n",
    "    # corresponding indices in the synthetic data\n",
    "    indices_syn = np.array(indices_syn).reshape(-1)\n",
    "\n",
    "    # considering a subset of features y slicing the feature space\n",
    "    X_train  = X_train_original[:, indices]\n",
    "    X_validation = X_validation_original[:, indices]\n",
    "    X_test = X_test_original[:, indices]\n",
    "\n",
    "    X_syn = X_syn_original  [:, indices_syn]\n",
    "\n",
    "    print(\"Unique lables in train:\", np.unique(y_train))\n",
    "    print(\"Unique lables in validation:\", np.unique(y_validation))\n",
    "\n",
    "    # converting the data into torch tensors\n",
    "    x = torch.from_numpy(X_syn).float()\n",
    "    y = torch.from_numpy(y_syn.reshape(-1)).long()\n",
    "\n",
    "    # putting the data on 'device'\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    \n",
    "    m = X_train.shape[1]\n",
    "    print (\"Number of features: \", m)\n",
    "    model = DynamicNet5(m)\n",
    "    model = model.to(device)\n",
    "\n",
    "    # (1) again, set ADAM optimizer with lr= 1e-3  and CrossEntropy loss\n",
    "    # your code is here\n",
    "\n",
    "    # (2) After setting optimizer and loss, define pretraining with synthetic data for 50 epochs. \n",
    "    # Hint: check the train loop for DNN1\n",
    "    # (we do pretraining usually for fewer epochs compared to final training with real data)\n",
    "    # your code is here\n",
    "\n",
    "    m, n = X_train.shape\n",
    "    m2, n2 = X_validation.shape\n",
    "    X_all = np.zeros((m + m2, n))\n",
    "    y_all = np.zeros((m + m2))\n",
    "\n",
    "    X_all[0:m, :] = X_train\n",
    "    X_all[m: m + m2, :] = X_validation\n",
    "    y_all[0:m] = y_train.reshape(-1)\n",
    "    y_all[m:m + m2] = y_validation.reshape(-1)\n",
    "\n",
    "    x = torch.from_numpy(X_all).float()\n",
    "    y = torch.from_numpy(y_all.reshape(-1)).long()\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    \n",
    "    \n",
    "    # (3) define training with real data for 1000 epochs. Hint: check the train loop for DNN1\n",
    "    # your code is here\n",
    "        \n",
    "    # define confusion matrix\n",
    "    cm = np.zeros((4, 4), dtype=int)\n",
    "    TP = TN = FP = FN = 0\n",
    "    \n",
    "    # evaluation on test data\n",
    "    y_pred_syn_array = list()\n",
    "    correct = 0\n",
    "    total = X_test.shape[0]\n",
    "    print('Running evaluation...')\n",
    "    with torch.no_grad():\n",
    "        for i in range(X_test.shape[0]):\n",
    "            x_t = torch.from_numpy(X_test[i, :]).float().to(device)\n",
    "            y_t = torch.tensor(y_test[i], dtype=torch.long, device=device)\n",
    "            \n",
    "            output = model.forward(x_t)\n",
    "            np_output = (output.cpu()).numpy()\n",
    "            y_pred = np.argmax(np_output)\n",
    "            y_pred_syn_array.append(y_pred)\n",
    "            label = int(y_test[i].item())\n",
    "            \n",
    "            # Update the confusion matrix\n",
    "            cm[label, y_pred] += 1\n",
    "\n",
    "            # Update correct predictions and binary classification counters\n",
    "            if y_pred == label:\n",
    "                correct += 1\n",
    "                if y_pred == 0:\n",
    "                    TN += 1\n",
    "                else:\n",
    "                    TP += 1\n",
    "            else:\n",
    "                if y_pred == 0:\n",
    "                    FN += 1\n",
    "                else:\n",
    "                    FP += 1\n",
    "                    \n",
    "        # After processing all samples, extract confusion matrix values:\n",
    "        T1, T2, T3, T4 = cm[0, 0], cm[1, 1], cm[2, 2], cm[3, 3]\n",
    "        F12, F13, F14 = cm[0, 1], cm[0, 2], cm[0, 3]\n",
    "        F21, F23, F24 = cm[1, 0], cm[1, 2], cm[1, 3]\n",
    "        F31, F32, F34 = cm[2, 0], cm[2, 1], cm[2, 3]\n",
    "        F41, F42, F43 = cm[3, 0], cm[3, 1], cm[3, 2]\n",
    "        \n",
    "        test_accuracy_syn = 100 * correct / total\n",
    "        print ('Test Accuracy:', round(test_accuracy_syn, 3))\n",
    "\n",
    "        print(f\"T1: {T1}, T2: {T2}, T3: {T3}, T4: {T4}\")\n",
    "        print(f\"F12: {F12}, F13: {F13}, F14: {F14}\")\n",
    "        print(f\"F21: {F21}, F23: {F23}, F24: {F24}\")\n",
    "        print(f\"F31: {F31}, F32: {F32}, F34: {F34}\")\n",
    "        print(f\"F41: {F41}, F42: {F42}, F43: {F43}\")\n",
    "\n",
    "        print(\"Aggregated metrics:\")\n",
    "        print (\"True positive: \", TP)\n",
    "        print (\"False positive: \", FP)\n",
    "        print (\"True negative: \", TN)\n",
    "        print (\"False negative: \", FN)\n",
    "        \n",
    "        F1, FPR, FNR = metrics(TP, TN, FP, FN)\n",
    "        print(\"False positve rate: \", round(FPR, 3))\n",
    "        print(\"False negative rate: \", round(FNR, 3))\n",
    "        print(\"F1 Score: \", round(F1, 3))\n",
    "        y_pred_syn_array = np.array(y_pred_syn_array)\n",
    "\n",
    "        # computing the performance metrics\n",
    "        F1_syn = 100 * f1_score(y_test, y_pred_syn_array, average='macro') # .item()\n",
    "        FPR_syn = 100 * ((F12 + F13 + F14) / (F12 + F13 + F14 + T1)) if (F12 + F13 + F14 + T1) else np.nan\n",
    "        FNR2_syn = 100 * ((F21 + F23 + F24) / (F21 + F23 + F24 + T2)) if (F21 + F23 + F24 + T2) else np.nan\n",
    "        FNR3_syn = 100 * ((F31 + F32 + F34) / (F31 + F32 + F34 + T3)) if (F31 + F32 + F34 + T3) else np.nan\n",
    "        FNR4_syn = 100 * ((F41 + F42 + F43) / (F41 + F42 + F43 + T4)) if (F41 + F42 + F43 + T4) else np.nan\n",
    "\n",
    "        print(\"Per-class metrics:\")\n",
    "        print (\"False positve rate: 1\",  round(FPR_syn, 3))\n",
    "        print (\"False negative rate 2: \",  round(FNR2_syn, 3))\n",
    "        print (\"False negative rate 3\",  round(FNR3_syn, 3))\n",
    "        print (\"False negative rate 4: \",  round(FNR4_syn, 3))\n",
    "        print (\"F1 Score: \",  round(F1_syn, 3))\n",
    "\n",
    "        saved_metrics2[cmb] = {\n",
    "            'acc': round(test_accuracy_syn, 3),\n",
    "            'FPR': round(FPR, 3),\n",
    "            'FNR': round(FNR, 3),\n",
    "            'F1': round(F1_syn, 3),\n",
    "        }\n",
    "\n",
    "        #saving the DNN 2 models, with the name of the features used in the name of the file        \n",
    "        save_to = os.path.join(path_dnn2, '_'.join(cmb) + '.pth')\n",
    "        torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict()\n",
    "                }, save_to)\n",
    "\n",
    "        print(f'saved to {save_to}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab21fe2c",
   "metadata": {},
   "source": [
    "### Task 2e: find the best feature (2 points)\n",
    "Write code that finds the feature combinatinons (like ('GSR', 'IBI', 'Ox', 'BP')) with:\n",
    "\n",
    "1) best accuracy\n",
    "2) best F1 score\n",
    "\n",
    "Note that your results may differ from the CovidDeep paper, since we changed hyperparameters to make training faster for educational purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec96e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "hint: look where we saved metrics in the code above\n",
    "\n",
    "def find_best_features(saved_metrics: dict) -> None:\n",
    "    # your code is here\n",
    "    \n",
    "    print(\"Features with max accuracy:\", max_acc)\n",
    "    print(\"Maximum accuracy:\", max_acc)\n",
    "    \n",
    "    # your code is here\n",
    "    print(\"Features with max F1 score:\", max_acc)\n",
    "    print(\"Maximum F1 score:\", max_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb423eb",
   "metadata": {},
   "source": [
    "### Task 2f: Report accuracies for both DNN1 and DNN2 (1 point)\n",
    "\n",
    "Using find_best_features, report best F1 and accuracy scores, and feature combination that performs the best for both DNN1 and DNN2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e7a737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer is here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cf611e",
   "metadata": {},
   "source": [
    "# Part 3. DNN3: Grow and Prune approach.\n",
    "\n",
    "The starting point can be pretrained DNN 1 and 2 models to apply grow and prune synthetic to improve the classification performance. However, we can also leverage grow and prune synthesis from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdb875a-e69b-4ec5-ac1d-555b34b2123c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_train(data_path='./data/sonar/', mode='all', fold=None):\n",
    "    if fold != None:\n",
    "        X_train = np.load(data_path + \"X_train\" + str(fold) + '.npy')\n",
    "        y_train = np.load(data_path + \"y_train\" + str(fold) + '.npy')\n",
    "        X_test = np.load(data_path + \"X_validation\"+ str(fold) + '.npy')\n",
    "        y_test = np.load(data_path + \"y_validation\"+ str(fold) + '.npy')\n",
    "         \n",
    "    else:\n",
    "        if mode == 'all':\n",
    "            X_train = np.load(data_path + \"X_train.npy\")\n",
    "            y_train = np.load(data_path + \"y_train.npy\")\n",
    "        elif mode == 'easy':\n",
    "            X_train = np.load(data_path + \"X_easy.npy\")\n",
    "            y_train = np.load(data_path + \"y_easy.npy\")\n",
    "        elif mode == 'hard':\n",
    "            X_train = np.load(data_path + \"X_hard.npy\")\n",
    "            y_train = np.load(data_path + \"y_hard.npy\")\n",
    "        \n",
    "        X_test = np.load(data_path + \"X_validation.npy\")\n",
    "        y_test = np.load(data_path + \"y_validation.npy\")\n",
    "    \n",
    "\n",
    "    X_train = X_train.astype(float)\n",
    "    X_test = X_test.astype(float)\n",
    "    y_train = y_train.astype(int)\n",
    "    y_test = y_test.astype(int)\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "def load_data_test(data_path='./data/sonar/', fold=None):\n",
    "    if fold != None:\n",
    "        X_train = np.load(data_path + \"X_train\" + str(fold) + '.npy')\n",
    "        y_train = np.load(data_path + \"y_train\" + str(fold) + '.npy')\n",
    "        X_test = np.load(data_path + \"X_test\"+ str(fold) + '.npy')\n",
    "        y_test = np.load(data_path + \"y_test\"+ str(fold) + '.npy')\n",
    "        \n",
    "         \n",
    "    else:\n",
    "        X_train = np.load(data_path + \"X_train_total.npy\")\n",
    "        X_test = np.load(data_path + \"X_test.npy\")\n",
    "        y_train = np.load(data_path + \"y_train_total.npy\")\n",
    "        y_test = np.load(data_path + \"y_test.npy\")\n",
    "\n",
    "    X_train = X_train.astype(float)\n",
    "    X_test = X_test.astype(float)\n",
    "    y_train = y_train.astype(int)\n",
    "    y_test = y_test.astype(int)\n",
    "    \n",
    "\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277e7c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData(path_to_dataset, indexes, mode='train', batch_size=64, generator=None):\n",
    "    if mode == 'train':\n",
    "        X_train, y_train, X_validation, y_validation = load_data_train(f\"{path_to_dataset}/\")\n",
    "    elif mode == 'test':\n",
    "        X_train, y_train, X_validation, y_validation = load_data_test(f\"{path_to_dataset}/\")\n",
    "    \n",
    "    print('number of features:', len(indexes))\n",
    "    \n",
    "    #Only using these features\n",
    "    X_train  = X_train[:, indexes]\n",
    "    X_validation = X_validation[:, indexes]\n",
    "    \n",
    "    print('number of labels in train:', np.unique(y_train))\n",
    "    print('number of labels in validation:', np.unique(y_validation))\n",
    "\n",
    "    # changing to torch tensors\n",
    "    X_train_tensor = torch.from_numpy(X_train).float()\n",
    "    y_train_tensor = torch.from_numpy(y_train.reshape(-1)).long()\n",
    "    X_validation_tensor = torch.from_numpy(X_validation).float()\n",
    "    y_validation_tensor = torch.from_numpy(y_validation.reshape(-1)).long()\n",
    "\n",
    "    # Create TensorDatasets\n",
    "    train_data = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    validation_data = TensorDataset(X_validation_tensor, y_validation_tensor)\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, generator=generator)\n",
    "    validation_loader = DataLoader(validation_data, batch_size=batch_size, shuffle=False, generator=generator)\n",
    "\n",
    "    return train_loader, validation_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a36ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dnn3 = os.path.join(path_to_save_models, 'DNN-3')\n",
    "os.makedirs(path_dnn3, exist_ok=True)\n",
    "\n",
    "\n",
    "def save_checkpoint(state, path_to_save, is_best, name, filename='_checkpoint.pth.tar'):\n",
    "    torch.save(state, os.path.join(path_to_save, name + filename))\n",
    "    print(f\"saved to {os.path.join(path_to_save, name + filename)}\")\n",
    "    if is_best:\n",
    "        shutil.copyfile(os.path.join(path_to_save, name + filename), os.path.join(path_dnn3, name + '_model_best.pth.tar'))\n",
    "        print(f\"also saved as the best checkpoint to {os.path.join(path_dnn3, name + '_model_best.pth.tar')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aedf317",
   "metadata": {},
   "outputs": [],
   "source": [
    "def disp_acc():\n",
    "    global model, trainloader, validationloader\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    \n",
    "    # For the CovidDeep Confusion matrix, and FPR, FNR, and F1 calculations\n",
    "    #Ti: the cohort i instances that are correcly classified\n",
    "    #Fij: The incorrect predictions, label: i, prediction: j\n",
    "    cm = np.zeros((3, 3), dtype=int)\n",
    "\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model.myforward(inputs, cw1, cw2, cw3, cw4)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct += (predicted == labels.data).sum()\n",
    "        total += labels.size(0)\n",
    "    print('Train: %d/%d' %(correct, total))\n",
    "    train_acc = correct * 1. / total\n",
    "    \n",
    "    total = 0\n",
    "    correct = 0\n",
    "    \n",
    "    y_test = np.zeros((1))\n",
    "    y_pred_array = np.zeros((1))\n",
    "    \n",
    "    for i, data in enumerate(validationloader, 0):\n",
    "        inputs, targets = data\n",
    "        inputs, labels = inputs.to(device), targets.to(device)\n",
    "        outputs = model.myforward(inputs, cw1, cw2, cw3, cw4)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "        # Convert GPU tensors to CPU and detach before converting to NumPy\n",
    "        labels_np = labels.cpu().detach().numpy()\n",
    "        predicted_np = predicted.cpu().detach().numpy()\n",
    "\n",
    "        y_test = np.concatenate((y_test, labels_np), axis=0)\n",
    "        y_pred_array = np.concatenate((y_pred_array, predicted_np), axis=0)\n",
    "\n",
    "        # Computing the confusion matrix\n",
    "        for j in range(labels_np.shape[0]):\n",
    "            cm[labels_np[j], predicted_np[j]] += 1\n",
    "\n",
    "    T1, T2, T3 = cm[0, 0], cm[1, 1], cm[2, 2]\n",
    "    F12, F13 = cm[0, 1], cm[0, 2]\n",
    "    F21, F23 = cm[1, 0], cm[1, 2]\n",
    "    F31, F32 = cm[2, 0], cm[2, 1]\n",
    "\n",
    "    \n",
    "    y_test = np.array(y_test)\n",
    "    y_pred_array = np.array(y_pred_array)\n",
    "    y_test = np.delete(y_test, 0, 0)\n",
    "    y_pred_array = np.delete(y_pred_array, 0, 0)\n",
    "    \n",
    "    # Computing the F1 score and FNR and FPR values\n",
    "    F1_original = 100 * f1_score(y_test, y_pred_array, average='macro')\n",
    "    FPR_original = 100 * ((F12+F13)/(F12+F13+T1))\n",
    "    FNR2_original = 100 * ((F21+F23)/(F21+F23+T2))\n",
    "    FNR3_original = 100 * ((F31+F32)/(F31+F32+T3))\n",
    "    test_accuracy_original = 100 * correct/total\n",
    "\n",
    "    print(\"T1: \", T1)\n",
    "    print(\"T2: \", T2)\n",
    "    print(\"T3: \", T3)\n",
    "\n",
    "    print(\"F12: \", F12)\n",
    "    print(\"F13: \", F13)\n",
    "\n",
    "    print(\"F21: \", F21)\n",
    "    print(\"F23: \", F23)\n",
    "\n",
    "    print(\"F31: \", F31)\n",
    "    print(\"F32: \", F32)\n",
    "\n",
    "    print(\"False positive rate: \", round(FPR_original, 3))\n",
    "    print(\"False negative rate 2: \", round(FNR2_original, 3))\n",
    "    print(\"False negative rate 3: \", round(FNR3_original, 3))\n",
    "    print(\"F1 Score: \", round(F1_original, 3))\n",
    "\n",
    "\n",
    "    print('Train: %d/%d' %(correct, total))\n",
    "    validation_acc = correct * 1. /total\n",
    "    print('Training accuracy: %f, Validation accuracy: %f' \n",
    "              % (train_acc, validation_acc))\n",
    "    return validation_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db35444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the model alongside the learned masks\n",
    "# if the number of layers change, the number of masks need to change and this function can be changed accordingly\n",
    "def load_model(path_to_load: str, mode='best'):\n",
    "    global model, name, best_acc_test, global_epoch, cw1, cw2, cw3, cw4   \n",
    "    if mode == 'best':\n",
    "        checkpoint = torch.load(path_to_load + '_model_best.pth.tar', weights_only=True)\n",
    "    elif mode == 'checkpoint':\n",
    "        checkpoint = torch.load(path_to_load + '_checkpoint.pth.tar', weights_only=True)\n",
    "    else:\n",
    "        checkpoint = torch.load(path_to_load + '_prune_checkpoint.pth.tar', weights_only=True)\n",
    "        \n",
    "    best_acc_test = checkpoint['best_acc']\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    global_epoch = checkpoint['epoch']\n",
    "    cw1 = checkpoint['cw1']\n",
    "    cw2 = checkpoint['cw2']\n",
    "    cw3 = checkpoint['cw3']\n",
    "    cw4 = checkpoint['cw4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12adb2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(duration=10):    \n",
    "    global best_acc_train, best_acc_test, name, trainloader, validationloader, global_epoch, flagc, best_acc_prune, validation_acc_list\n",
    "    for epoch in range(global_epoch, global_epoch+duration):  # loop over the dataset multiple times\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs\n",
    "            inputs, targets = data\n",
    "            # move tensors to the configured device\n",
    "            inputs, labels = inputs.to(device), targets.to(device)\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            # forward + backward + optimize\n",
    "            outputs = model.myforward(inputs, cw1, cw2, cw3, cw4)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            inputs, targets = data\n",
    "            inputs, labels = inputs.to(device), targets.to(device)\n",
    "            outputs = model.myforward(inputs, cw1, cw2, cw3, cw4)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct += (predicted == labels.data).sum()\n",
    "            total += labels.size(0)\n",
    "        train_acc = correct * 1. / total\n",
    "        \n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for i, data in enumerate(validationloader, 0):\n",
    "            inputs, targets = data\n",
    "            inputs, labels = inputs.to(device), targets.to(device)\n",
    "            outputs = model.myforward(inputs, cw1, cw2, cw3, cw4)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct += (predicted == labels.data).sum()\n",
    "            total += labels.size(0)\n",
    "        validation_acc = correct * 1. /total\n",
    "        print('Epoch: %d, Training accuracy: %f, Validation accuracy: %f' \n",
    "                  % (epoch, train_acc, validation_acc))\n",
    "        \n",
    "        # saving the model with the best validation performance\n",
    "        if validation_acc > best_acc_test:\n",
    "            best_acc_test = validation_acc\n",
    "            best_acc_train = train_acc\n",
    "            save_checkpoint({\n",
    "                'epoch': epoch,\n",
    "                'state_dict': model.state_dict(),\n",
    "                'best_acc': best_acc_test,\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'cw1': cw1,\n",
    "                'cw2': cw2,\n",
    "                'cw3': cw3,\n",
    "                'cw4': cw4,\n",
    "            }, path_dnn3, True, name=name)\n",
    "        else:\n",
    "            save_checkpoint({\n",
    "                'epoch': epoch,\n",
    "                'state_dict': model.state_dict(),\n",
    "                'best_acc': best_acc_test,\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'cw1': cw1,\n",
    "                'cw2': cw2,\n",
    "                'cw3': cw3,\n",
    "                'cw4': cw4,\n",
    "            }, path_dnn3, False, name=name)\n",
    "\n",
    "        if (flag == 1) and (validation_acc > best_acc_prune):\n",
    "            best_acc_prune = validation_acc\n",
    "            state = {\n",
    "                     'epoch': epoch,\n",
    "                     'best_acc': best_acc_prune,\n",
    "                     'state_dict': model.state_dict(),\n",
    "                     'optimizer': optimizer.state_dict(),\n",
    "                     'cw1': cw1,\n",
    "                     'cw2': cw2,\n",
    "                     'cw3': cw3,\n",
    "                     'cw4': cw4,\n",
    "                    }\n",
    "            save_checkpoint(state, path_dnn3, False, name = name + '_prune')\n",
    "            \n",
    "    global_epoch += duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ce334d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function: prune connections\n",
    "Arguments:\n",
    "    weight_list: weight matrix to prune\n",
    "    mask_list: corresponding mask to prune\n",
    "    mode: 'small' prune small weights, 'large' prune large weights, 'rand' random, we normally use it to prune small weights\n",
    "    num: number of connections left\n",
    "    percetile: percentile of connections left\n",
    "'''\n",
    "\n",
    "def pruneMask(weight_list, mask_list, mode='small', num=100, percentile=None):\n",
    "    for i, weight in enumerate(weight_list):\n",
    "        weight_cpu = weight.cpu()\n",
    "        if i == 0:\n",
    "            weight_arr = weight_cpu.view(-1).numpy()\n",
    "        else:\n",
    "            weight_arr = np.concatenate((weight_arr, weight_cpu.view(-1).numpy()))\n",
    "\n",
    "    if mode == 'small':\n",
    "        if percentile:\n",
    "            threshold = np.percentile(np.absolute(weight_arr), percentile)\n",
    "        else:\n",
    "            threshold = np.sort(np.absolute(weight_arr))[-num]\n",
    "        for i, mask in enumerate(mask_list):\n",
    "            # Use CPU absolute for threshold comparison; update mask in-place on original device\n",
    "            mask[torch.abs(weight_list[i]) < float(threshold)] = 0\n",
    "\n",
    "    elif mode == 'large':\n",
    "        if percentile:\n",
    "            threshold = np.percentile(np.absolute(weight_arr), percentile)\n",
    "        else:\n",
    "            threshold = np.sort(np.absolute(weight_arr))[-num]\n",
    "        for i, mask in enumerate(mask_list):\n",
    "            mask[torch.abs(weight_list[i]) > float(threshold)] = 0\n",
    "                \n",
    "\n",
    "'''\n",
    "Function: add connections\n",
    "Arguments:\n",
    "    mask_list: corresponding mask to prune\n",
    "    mode: 'partial' add part of connections, 'full' restore all connections\n",
    "    ratio: ratio of active connections after adding connections \n",
    "'''\n",
    "        \n",
    "def growMask(mask_list, mode='partial', ratio=[0.1,], generator=None):\n",
    "    if mode == 'partial':\n",
    "        for i, mask in enumerate(mask_list):\n",
    "            rand_mat = torch.rand(mask.size(), device=mask.device, generator=generator)\n",
    "            mask[rand_mat > ratio[i]] = 1\n",
    "    elif mode == 'full':\n",
    "        for i, mask in enumerate(mask_list):\n",
    "            mask.fill_(1)\n",
    "            \n",
    "            \n",
    "def displayConnection(mask_list):\n",
    "    nonzero_connections = []\n",
    "    for name, mask in mask_list:\n",
    "        nonzero_count = np.count_nonzero(mask.cpu().numpy()) if mask.device.type != 'cpu' else np.count_nonzero(mask.numpy())\n",
    "        nonzero_connections.append(nonzero_count)\n",
    "        print(f\"{name}: {nonzero_count}\", end=\"  \")\n",
    "    print('\\n')\n",
    "    return nonzero_connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8aab66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Three hidden layers\n",
    "# This can be changed based on the loaded architecture \n",
    "class Net3(nn.Module):\n",
    "    def __init__(self, in_num, out_num, hidden_num1, hidden_num2, hidden_num3):\n",
    "        super(Net3, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_num, hidden_num1)\n",
    "        self.fc2 = nn.Linear(hidden_num1, hidden_num2)\n",
    "        self.fc3 = nn.Linear(hidden_num2, hidden_num3)\n",
    "        self.fc4 = nn.Linear(hidden_num3, out_num)\n",
    "    \n",
    "    # Simple forward pass in a DNN\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "    \n",
    "    # forward pass that considers binary masks (cw1 to cw4)\n",
    "    # The number of these masks change as the number of layers change\n",
    "    def myforward(self, x, cw1, cw2, cw3, cw4):\n",
    "        self.fc1.weight.data = self.fc1.weight.data * cw1\n",
    "        self.fc2.weight.data = self.fc2.weight.data * cw2\n",
    "        self.fc3.weight.data = self.fc3.weight.data * cw3\n",
    "        self.fc4.weight.data = self.fc4.weight.data * cw4\n",
    "       \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8957f2c9",
   "metadata": {},
   "source": [
    "## Load the DNN 2 models\n",
    "The SCANN grow-and-prune is applied to further improve DNN models 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc4ff26",
   "metadata": {},
   "source": [
    "#### We will work with GSR_Temp_IBI_Ox_BP_Q DNN2 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa10539",
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn2_for_grow_prune = 'GSR_Temp_IBI_Ox_BP_Q.pth'\n",
    "checkpoint = torch.load(os.path.join(path_dnn2, dnn2_for_grow_prune), map_location=torch.device(device), weights_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03a04ce",
   "metadata": {},
   "source": [
    "### Task 3a: set Net3 model configuration based on the loaded model parameters (2 points)\n",
    "\n",
    "\n",
    "set in_num, hidden_num1, hidden_num2, hidden_num3, out_num\n",
    "\n",
    "in_num: shows the number of input features\n",
    "\n",
    "hidden_num1, hidden_num2, hidden_num3: neurons in hidden layers\n",
    "\n",
    "out_num: the number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1a3244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # your code is here\n",
    "\n",
    "\n",
    "# print(in_num, hidden_num1, hidden_num2, out_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe3770b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing the masks\n",
    "cw1 = torch.ones([hidden_num1, in_num])\n",
    "cw2 = torch.ones([hidden_num2, hidden_num1])\n",
    "cw3 = torch.ones([hidden_num3, hidden_num2])\n",
    "cw4 = torch.ones([out_num, hidden_num3])\n",
    "\n",
    "cw1, cw2, cw3, cw4 = cw1.to(device), cw2.to(device), cw3.to(device), cw4.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a91d366",
   "metadata": {},
   "outputs": [],
   "source": [
    "featdict_grow_and_prune = {\n",
    "    \"GSR\": list(np.arange(3900,3960)),\n",
    "    \"Temp\": list(np.arange(3960,4020)),\n",
    "    \"IBI\" : list(np.arange(4080,4140)),\n",
    "    \"Ox\" : list(np.arange(4146,4147)),\n",
    "    \"BP\" : list(np.arange(4160,4162)),\n",
    "    \"Q\": list(np.arange(4147,4158))\n",
    "}\n",
    "\n",
    "indexes_grow_prune = np.array(list(chain.from_iterable(featdict_grow_and_prune.values()))).reshape(-1)\n",
    "print('number of features:', len(indexes_grow_prune))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e4c2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading train, and validation sets\n",
    "dataset = 'KDE'\n",
    "batch_size = 256\n",
    "global_epoch = 0\n",
    "name = dataset + '_mlp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85619f9d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ==============\n",
    "generator2 = torch.Generator(device=device)\n",
    "generator2.manual_seed(RANDOM_STATE)\n",
    "\n",
    "generator3 = torch.Generator(device='cpu')\n",
    "generator3.manual_seed(RANDOM_STATE)\n",
    "\n",
    "np.random.seed(RANDOM_STATE)\n",
    "random.seed(RANDOM_STATE)\n",
    "\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "torch.cuda.manual_seed(RANDOM_STATE)\n",
    "# ==============\n",
    "\n",
    "trainloader, validationloader = loadData(path_to_real_data, indexes_grow_prune, mode='train', batch_size=batch_size, generator=generator3)\n",
    "\n",
    "model = Net3(in_num, out_num, hidden_num1, hidden_num2, hidden_num3)\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "flag = 0\n",
    "best_acc_prune = 0\n",
    "\n",
    "best_acc_test = 0\n",
    "best_acc_train = 0\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=5e-3, momentum=0.9)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "for i in range(1): # number of iterations\n",
    "    print('====== Prune ======')\n",
    "    # num in pruneMas shows the final number of connections in model\n",
    "    # it needs to be decided by trial and error process\n",
    "    pruneMask([model.fc1.weight.data, model.fc2.weight.data, model.fc3.weight.data, model.fc4.weight.data],\n",
    "              [cw1, cw2, cw3, cw4], num = 25000)\n",
    "    flag = 1\n",
    "    displayConnection([('cw1', cw1), ('cw2', cw2), ('cw3', cw3), ('cw4', cw4)])\n",
    "    train(20)\n",
    "    print('====== Grow ======')\n",
    "    growMask([cw1, cw2, cw3, cw4], mode='partial', ratio=[0.2, 0.2, 0.5, 0.5], generator=generator2)\n",
    "    flag = 0\n",
    "    displayConnection([('cw1', cw1), ('cw2', cw2), ('cw3', cw3), ('cw4', cw4)])\n",
    "    train(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c84ab76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the model, and evaluating on the validation set\n",
    "path_to_load = f'{path_dnn3}/KDE_mlp'\n",
    "load_model(path_to_load, mode='best')\n",
    "nonzero_connections = displayConnection([('cw1', cw1), ('cw2', cw2), ('cw3', cw3), ('cw4', cw4)])\n",
    "disp_acc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37a6bd7",
   "metadata": {},
   "source": [
    "### Task 3b: compare the DNN3 size vs orignal DNN2 (1 point)\n",
    "\n",
    "As a result of grow and prune, you get a compact network with higher accuracy. \n",
    "\n",
    "NOTE: your results may be different from CovidDeep paper, since we decreased number of training epochs for faster training. But the DNN3 accuracy should be higher than DNN2.\n",
    "\n",
    "Calculate the compression ratio for the number of parameters vs the intial network (the number should be > 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d222ea14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code is here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e556acd-79b9-4961-bebb-eca4d734a2dd",
   "metadata": {},
   "source": [
    "### Task 3c: compare the DNN3 accuracy vs orignal DNN2 accuracy (1 point)\n",
    "\n",
    "Display the result for DNN2.\n",
    "\n",
    "In this code, use saved_metrics corresponding DNN2 to display the results for DNN2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d999cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code is here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scann",
   "language": "python",
   "name": "scann"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
