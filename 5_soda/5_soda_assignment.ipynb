{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ddd0092-943f-4a57-acf5-221aced10184",
   "metadata": {
    "id": "rVC50o5iupx8"
   },
   "source": [
    "# Important notice: any use of generative AI for completing the assignment is strictly prohibited."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6641b82-4789-4937-b74d-35e5655fdbab",
   "metadata": {
    "id": "qf-AMSR2UH4n"
   },
   "source": [
    "## Prepare environment & load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483963d6-4d45-47ed-a56f-0aeecda97e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3659f07-9f13-44fe-b242-bf122867db2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use that if working in colab\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10c900d-387c-482e-aa16-1d97f1a45946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you should be added as viewer to shared Google drive \"ECE477 datasets\"\n",
    "#  at https://drive.google.com/drive/u/0/folders/0ABIZHKB-QPnRUk9PVA\n",
    "\n",
    "!unzip \"/content/drive/Shared drives/ECE477 datasets/Assignment5/features_extracted_threshold8.zip\" -d \".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a71f82-90d9-47c3-af75-ac94c2ef958d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read patient data\n",
    "\n",
    "path_train = 'features_extracted_threshold8/train/'\n",
    "path_test = 'features_extracted_threshold8/test/'\n",
    "\n",
    "p1_train = patient = pd.read_csv(os.path.join(path_train, \"p1.csv\"), header=None)\n",
    "p1_test = patient = pd.read_csv(os.path.join(path_test, \"p1.csv\"), header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a08e0a5-0164-4177-b290-ddfc544e00c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f037a0d1-85f8-4fee-aec9-b1eacff2989f",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fab0ae-02f7-46db-a85c-f656424d33ca",
   "metadata": {},
   "source": [
    "# Part 1: Principal Component Analysis\n",
    "\n",
    "PCA is standard practice to reduce the feature set. We aid of PCA, one can preserve the most significant part of data while omitting insignificant components that don't add a real value to the data.\n",
    "\n",
    "PCA allows to decrease computational complexity and may be especailly helpful in case of prohibitively large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b414d0f1-5846-46b6-99f6-f3f4c7afe8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497c9614-8fc3-4056-884d-e8f2f0feac2e",
   "metadata": {},
   "source": [
    "### Task 1a: Preprocess data for PCA (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f66ba73e-aae4-4ee3-9ec9-40d6e8baf31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target for training data\n",
    "features_train = p1_train.iloc[:, :-1]\n",
    "# your code is here\n",
    "target_train = ...\n",
    "\n",
    "# Separate features and target for test data\n",
    "# your code is here\n",
    "\n",
    "# Standardize data features using StandardScaler;\n",
    "# your code is here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300a2e00-5246-4699-9c77-3e027a048a18",
   "metadata": {},
   "source": [
    "### Task 1b: Find the optimal number of components over the preprocessed training set (4 points)\n",
    "\n",
    "After PCA, the main components explain the most of the dataset variance. The rest don't contibute into explained variance much, thus can be omitted. In this part we are looking for the optimal number of components.\n",
    "\n",
    "\n",
    "Fill in the missing code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0fd61d-5062-4488-a5dd-a0742d6b5e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA with the maximum number of components (all features)\n",
    "# your code is here\n",
    "\n",
    "# Calculate the cumulative explained variance\n",
    "# hint: use numpy cumsum()\n",
    "# your code is here\n",
    "\n",
    "\n",
    "# Plot the cumulative explained variance to visualize the optimal number of components\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(1, len(explained_variance_ratio) + 1), cumulative_variance, marker='o', linestyle='--')\n",
    "plt.xlabel('Number of Principal Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Explained Variance vs Number of Components')\n",
    "plt.axhline(y=0.95, color='r', linestyle='--')  # 95% variance line\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Find the number of components that explain at least 95% of the variance\n",
    "# your code is here\n",
    "\n",
    "print(f\"Optimal number of PCA components: {optimal_components}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20221593-95bd-4eb9-9ce1-c64e02e3671c",
   "metadata": {},
   "source": [
    "### Task 1c: Perform PCA over the preprocessed training and test set (3 points)\n",
    "Fill in the missing code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0fbd04c4-3fbf-495b-ae13-c25deb7fb39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA with the optimal number of components over the train set\n",
    "# your code is here\n",
    "\n",
    "# Convert the principal components of the train set into a DataFrame and combine with target\n",
    "# your code is here\n",
    "\n",
    "# Display the PCA results for the training set\n",
    "print(\"\\nPCA for Training Set:\")\n",
    "print(pca_df_train)\n",
    "\n",
    "print()\n",
    "\n",
    "# Prepare the test data in a similar fashion \n",
    "# (PCA, conveting to pandas dataframe)\n",
    "# your code is here\n",
    "\n",
    "# Display the PCA results for the test set\n",
    "print(\"\\nPCA for Test Set:\")\n",
    "print(pca_df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98653880-3786-4aae-8e1e-2b737532ec63",
   "metadata": {},
   "source": [
    "# Part 2: KNN\n",
    "\n",
    "Now, we a going to apply ML classifiers to the reduced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4d8d6a-ea16-44df-9f53-cf346648b3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7e659f-d442-4370-97b4-f4361f45864c",
   "metadata": {},
   "source": [
    "### Task 2a: Prepare data for KNN\n",
    "Separate features and target for training and test data in the PCA-transformed dataframe  (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7dd44794-50f5-404f-9798-33a906f7da27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare X_train, y_train, X_test, y_test from PCA preprocessed dataframes (pca_df_train, pca_df_test)\n",
    "# your code is here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44634e91-a5ef-4b91-8ada-6f0e6ad72f68",
   "metadata": {},
   "source": [
    "### Task 2b: Perform KNN on the PCA-transformed data (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f4f940-4a17-4268-9f38-b2dbfadadbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform KNN for number of neighbors (2, 10, 30, 50)\n",
    "# report train and test accuracy for all of them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1667b9-c829-4154-8188-c1f67a46fa6d",
   "metadata": {},
   "source": [
    "# Part 3: SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea00500-0eb7-41a7-83ed-933e5f2a189b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec68d1f3-be74-4dd8-817b-7090af4a13e0",
   "metadata": {},
   "source": [
    "### Task 3a: Train SVM (5 points)\n",
    "Train SVM with on PCA transformed data \n",
    "1) with linear kernel\n",
    "2) with radial basis functions as the kernel. \n",
    "\n",
    "Report train and test accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6019ba-7378-4847-9c2c-924d87428331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, fit SVM with linear kernel function (use dataset prepared in 2a)\n",
    "# your code is here\n",
    "\n",
    "# Print the results\n",
    "print(f\"SVM - Train Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"SVM - Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "\n",
    "# Second, fit SVM with radial basis functions as kernel\n",
    "# your code is here\n",
    "\n",
    "# Print the results\n",
    "print(f\"SVM with RBF kernel - Train Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"SVM with RBF kernel - Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acbba13-620a-4624-b5da-2ab4e928fb3b",
   "metadata": {},
   "source": [
    "### Task 3b: Compare models (1 point)\n",
    "\n",
    "Which model -- KNN, SVM with linear kernel, SVM with RBF -- is the best?\n",
    "\n",
    "Note your results may be different from the paper since we work only with the data of one patient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688527ca-ea55-420c-98a3-4cde8afaec7f",
   "metadata": {},
   "source": [
    "your answer is here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scann",
   "language": "python",
   "name": "scann"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
