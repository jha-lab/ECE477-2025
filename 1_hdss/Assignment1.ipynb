{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Important notice: any use of generative AI for completing the assignment is strictly prohibited."
      ],
      "metadata": {
        "id": "rVC50o5iupx8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare environment"
      ],
      "metadata": {
        "id": "qf-AMSR2UH4n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RfyECEizQ8ZH"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler, KBinsDiscretizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Warning: to ensure the reproducibility of your results and to achieve the full grade, do not change or remove RANDOM_STATE variables and setting random seed statements. If you remove or change them, you may not get the full grade."
      ],
      "metadata": {
        "id": "FYEFY3seuxES"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random_state=5 # use this to control randomness across runs e.g., dataset partitioning"
      ],
      "metadata": {
        "id": "ble2euAcRYac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use Diagnostic Wisconsin Breast Cancer dataset from UCI machine learning repository. Details of this data can be found [here](https://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic)"
      ],
      "metadata": {
        "id": "gXa5noG9Ucf5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install ucimlrepo"
      ],
      "metadata": {
        "id": "pYTza6ngSuMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ucimlrepo import fetch_ucirepo\n",
        "\n",
        "# fetch dataset\n",
        "breast_cancer_wisconsin_diagnostic = fetch_ucirepo(id=17)\n",
        "\n",
        "df_features_original = pd.DataFrame(dict(breast_cancer_wisconsin_diagnostic.data.features))\n",
        "df_target_original = pd.DataFrame(dict(breast_cancer_wisconsin_diagnostic.data.targets))\n",
        "\n",
        "# convert target features to boolean values\n",
        "df_target = df_target_original.copy()\n",
        "df_target['Diagnosis'] = df_target_original['Diagnosis'].map({'B': 0, 'M': 1})"
      ],
      "metadata": {
        "id": "JS63qPN9RarT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop unnecessary features\n",
        "features_to_drop = [\n",
        "  'radius2',\n",
        "  'texture2',\n",
        "  'perimeter2',\n",
        "  'area2',\n",
        "  'smoothness2',\n",
        "  'compactness2',\n",
        "  'concavity2',\n",
        "  'concave_points2',\n",
        "  'symmetry2',\n",
        "  'fractal_dimension2',\n",
        "  'radius3',\n",
        "  'texture3',\n",
        "  'perimeter3',\n",
        "  'area3',\n",
        "  'smoothness3',\n",
        "  'compactness3',\n",
        "  'concavity3',\n",
        "  'concave_points3',\n",
        "  'symmetry3',\n",
        "  'fractal_dimension3'\n",
        " ]\n",
        "\n",
        "df_features = df_features_original.drop(columns=features_to_drop)"
      ],
      "metadata": {
        "id": "BftEVcrTq2YE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_features.head())\n",
        "print('\\nInfo')\n",
        "print(df_features.info())"
      ],
      "metadata": {
        "id": "kpVwVn83yVHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# These are the names of the columns in the dataset. They includes all features of the data and the label.\n",
        "col_names = df_features.columns.tolist() + ['Diagnosis']\n",
        "col_names"
      ],
      "metadata": {
        "id": "piOARFoUS2Zi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decision tree (5 points)"
      ],
      "metadata": {
        "id": "XjgpHy_QcCvu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "EKOeexGacTw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tree visualization helper function\n",
        "from sklearn.tree import export_graphviz\n",
        "from six import StringIO\n",
        "from IPython.display import Image\n",
        "import pydotplus\n",
        "\n",
        "\"\"\"\n",
        "clf: DecisionTreeClassifier\n",
        "\n",
        "Returns a bytes object representing the image of the tree\n",
        "\"\"\"\n",
        "def get_tree_image(clf):\n",
        "    dot_data = StringIO()\n",
        "    feature_names=data.drop('Type',axis=1).columns\n",
        "    class_names=[\"building_windows_float_processed\", \"building_windows_non_float_processed\", \"vehicle_windows_float_processed\",\n",
        "            \"containers\", \"tableware\", \"headlamps\"]\n",
        "    export_graphviz(clf, out_file=dot_data,\n",
        "                    filled=True, rounded=True,\n",
        "                    special_characters=True,\n",
        "                    feature_names=feature_names,\n",
        "                    class_names=class_names)\n",
        "    graph = pydotplus.graph_from_dot_data(dot_data.getvalue())\n",
        "\n",
        "\n",
        "    return graph.create_png()"
      ],
      "metadata": {
        "id": "b75GM8VWEFfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise a: Fit and interpret a decision tree.\n",
        "\n",
        "Fit Decision trees using the Gini index and entropy-based impurity measure.\n",
        "\n",
        "Set the random_state to the value defined above. Keep all other parameters at their default values.\n",
        "\n",
        "Report the training and validation set accuracies for each classifier."
      ],
      "metadata": {
        "id": "VT4-6a3bEIut"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert data to numpy array\n",
        "\n",
        "# your code is here\n",
        "...\n",
        "\n",
        "print(X.shape)\n",
        "print(y.shape)\n",
        "\n",
        "# split into training (80%) and validation (20%) with train_test_split\n",
        "# your code is here\n",
        "\n",
        "\n",
        "# fit entropy based Decision Tree classifier with random_state defined above\n",
        "# report the train and validation accuracies\n",
        "# your code is here\n",
        "entropy_clf = ...\n",
        "\n",
        "\n",
        "# fit gini based Decision Tree classifier with random_state defined above\n",
        "# report the train and validation accuracies\n",
        "# your code is here\n",
        "gini_clf = ..."
      ],
      "metadata": {
        "id": "_A2KJPpM4lSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tree_image(clf, data): # Pass data as an argument\n",
        "    \"\"\"\n",
        "    clf: DecisionTreeClassifier\n",
        "    data: pandas DataFrame containing the features and target\n",
        "\n",
        "    Returns a bytes object representing the image of the tree\n",
        "    \"\"\"\n",
        "    dot_data = StringIO()\n",
        "    # Use the provided data argument to get feature names\n",
        "    feature_names = data.drop(columns=['Diagnosis']).columns\n",
        "    class_names = ['Benign', 'Malignant']  # actual class names\n",
        "    export_graphviz(clf, out_file=dot_data,\n",
        "                    filled=True, rounded=True,\n",
        "                    special_characters=True,\n",
        "                    feature_names=feature_names,\n",
        "                    class_names=class_names)\n",
        "    graph = pydotplus.graph_from_dot_data(dot_data.getvalue())\n",
        "    return graph.create_png()"
      ],
      "metadata": {
        "id": "cxzSAyE50p5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_clf = entropy_clf\n",
        "\n",
        "all_data = pd.concat([df_features, df_target], axis=1)\n",
        "tree_image = get_tree_image(best_clf, all_data)\n",
        "Image(tree_image)"
      ],
      "metadata": {
        "id": "i1bKqffO4Ayz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_clf = gini_clf\n",
        "\n",
        "all_data = pd.concat([df_features, df_target], axis=1)\n",
        "tree_image = get_tree_image(best_clf, all_data)\n",
        "Image(tree_image)"
      ],
      "metadata": {
        "id": "_i47R3MAFr-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Indicate the most informative descriptive feature (with the threshold) and briefly explain why this is the most informative (from an algorithmic viewpoint) for both GINI and entropy based classifiers (1 point).\n",
        "\n",
        "\n",
        "your answer is here\n",
        "\n",
        "### Show how one can interpret the tree by specifying the rule from its left most branch (1 point).\n",
        "\n",
        "your answer is here\n"
      ],
      "metadata": {
        "id": "8wy3rJj74G3j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise b: Preprune a decision tree\n",
        "\n",
        "Next, let's try pruning the tree to see if we can improve the classifier's generalization performance.\n",
        "\n",
        "Preprune a decision tree by varying the max_depth among {None (no depth control), 1,3,5,7}.\n",
        "\n",
        "Set the criterion to entropy and the random_state to the value defined above. Keep all other parameters at their default values.\n",
        "\n",
        "Report the training and validation set accuracies for each classifier **(1 point)**."
      ],
      "metadata": {
        "id": "y8gLjldd_YA8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# don't forget to specify random_state\n",
        "# your code is here"
      ],
      "metadata": {
        "id": "xRF-vaSx_ojM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analyze the effect of increasing tree depth on training and validation performance (1 point)\n",
        "\n",
        "your answer is here"
      ],
      "metadata": {
        "id": "08vv9dPzACBw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise C: Learning an Ensemble of Decision Trees\n",
        "\n",
        "Fit different Random Forest classifiers by varying the number of trees among [5, 7, 10, 50, 100].\n",
        "\n",
        "Set the criterion to entropy and set the random_state to the value defined above. Keep all other parameters at their default values.\n",
        "\n",
        "Report the validation set accuracies for each classifier."
      ],
      "metadata": {
        "id": "gG0shQx62Ijh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "metadata": {
        "id": "1fDN8_qa2RGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# don't forget to specify random_state\n",
        "# your code is here"
      ],
      "metadata": {
        "id": "kKNqLibw29X0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comment on the effect of increasing the number of trees on validation performance. Compare the performance of the best performing Random Forest classifier against the Decision Tree Classifier trained with entropy and explain any difference. (1 point)\n",
        "\n",
        "your answer is here"
      ],
      "metadata": {
        "id": "1PAHcgr82VwV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KNN model (5 points)"
      ],
      "metadata": {
        "id": "8qBgbzwQcYUE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# convert data to numpy\n",
        "\n",
        "X = df_features.to_numpy()\n",
        "y = df_target.to_numpy().ravel()\n",
        "\n",
        "X.shape, y.shape"
      ],
      "metadata": {
        "id": "nnOk6jo0cdV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create training and validation datasets\n",
        "\n",
        "Split the data into training and validation set using train_test_split. See [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) for details. To get consistent result while splitting, set random_state to the value defined earlier. We use 80% of the data for training and 20% of the data for validation."
      ],
      "metadata": {
        "id": "BhbHwJsqhq3A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X,y,test_size=.2,random_state=random_state)"
      ],
      "metadata": {
        "id": "uaJxN25Zg8-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training K-nearest neighbor models\n",
        "\n",
        "We will use the sklearn library to train a K-nearest neighbors (kNN) classifier. See [here](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier) for more details."
      ],
      "metadata": {
        "id": "qcXJ9YWRkZuR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 1: Learning a kNN classifier"
      ],
      "metadata": {
        "id": "LU8p-6m1lCh8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "9aNupsiNkhYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 1a: Evaluate the effect of the number of neighbors (1 point)\n",
        "* Preprocess the dataset\n",
        "\n",
        "Preprocess the data by normalizing each feature to have zero mean and unit standard deviation. This can be done using the `StandardScaler()` function. See [here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) for more details.\n",
        "\n",
        "* Train kNN classifiers with different number of neighbors among {1, 5, 25, 100, length(X_train)}.\n",
        "\n",
        "* Keep all other parameters at their default values.\n",
        "\n",
        "* Report the model's accuracy on the training and validation sets."
      ],
      "metadata": {
        "id": "fKyGT7MRlW_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the data by normalizing each feature to have zero mean and unit standard deviation. This can be done using the StandardScaler() function.\n",
        "# Define the scaler for scaling the data\n",
        "scaler = StandardScaler()\n",
        "# Normalize the training data with scaler\n",
        "# your code is here\n",
        "\n",
        "# Use the scaler defined above to standardize the validation data by applying the same transformation to the validation data.\n",
        "# your code is here\n",
        "\n",
        "\n",
        "# fit and predict K neighbors classifiers with k = 1, 5, 25, 100, len(X_train).\n",
        "# report train and validation accuracies for each.\n",
        "# your code is here"
      ],
      "metadata": {
        "id": "6lNO2VU1lhYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Explain the effect of increasing the number of neighbors on the performance over the training and validation sets. (1 point)**\n",
        "\n",
        "\n",
        "your answer is here"
      ],
      "metadata": {
        "id": "nGnEOBPslmuq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 1b: Evaluate the effect of a weighted kNN"
      ],
      "metadata": {
        "id": "LbEyKx_Oods7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (0.5 points)\n",
        "\n",
        "Train kNN classifiers with different numbers of neighbors from the list {1, 5, 25, 100, len(X_train)} and use the distance-weighted kNN (p and metric remains default). Report the model's accuracy on both the training and validation sets."
      ],
      "metadata": {
        "id": "4MpUA01iohUt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code is here"
      ],
      "metadata": {
        "id": "MsBSpEaQoq5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Compare the effect of the number of neighbors on model performance (train and validation) under the distance-weighted kNN (0.5 points)**\n",
        "\n",
        "\n",
        "your answer is here"
      ],
      "metadata": {
        "id": "IgsCCIh1pSJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 2: Feature Importance Analysis\n",
        "\n",
        "\n",
        "## (1 point)\n",
        "In this exercise you will implement a function to calculate feature importance for KNN classifier."
      ],
      "metadata": {
        "id": "SBdP6CRYp9eH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def knn_feature_importance(X, y, n_neighbors):\n",
        "    # Split the data into training and validation sets; use random_state\n",
        "    # your code is here\n",
        "\n",
        "    # Initialize KNN classifier and feature importance array\n",
        "    knn = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
        "    feature_importance = np.zeros(X.shape[1])\n",
        "\n",
        "    # Calculate baseline accuracy\n",
        "    knn.fit(X_train, y_train)\n",
        "    baseline_accuracy = accuracy_score(y_val, knn.predict(X_val))\n",
        "\n",
        "    # Calculate feature importance by deleting one feature at a time and\n",
        "    # subtracting resulted reduced_accuracy from baseline_accuracy\n",
        "    for i in range(X.shape[1]):\n",
        "        # your code is here\n",
        "        ...\n",
        "        feature_importance[i] = baseline_accuracy - reduced_accuracy\n",
        "\n",
        "    # Normalization\n",
        "    return feature_importance / np.sum(feature_importance)\n",
        "\n",
        "\n",
        "def plot_feature_importance(importance, col_names=col_names):\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.bar(col_names[:-1], importance)\n",
        "    plt.title('KNN Feature Importance')\n",
        "    plt.xlabel('Features')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.ylabel('Importance')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    for feature, imp in zip(col_names[:-1], importance):\n",
        "        print(f\"{feature}: {imp:.2f}\")"
      ],
      "metadata": {
        "id": "WjVPUwhlqAC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then you can use your function to calculate the feature importance when $n_{neighbors} = 1, 2, 5$, and you can plot or print them for better visualization."
      ],
      "metadata": {
        "id": "BQLlnNeCzsL7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_neighbors = [1, 2, 5]\n",
        "\n",
        "for n in num_neighbors:\n",
        "    print(f\"Feature importance for n_neighbors={n}:\")\n",
        "    importance = knn_feature_importance(X, y, n)\n",
        "    plot_feature_importance(importance)"
      ],
      "metadata": {
        "id": "0STj-2YEqVKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Discuss the questions: (1 point)**\n",
        "\n",
        "* For every given number of neighbors, identify the two top most important features for the KNN classifier.\n",
        "* Why the different number of neighbors yields different most important features?\n",
        "\n",
        "\n",
        "your answer is here"
      ],
      "metadata": {
        "id": "_3AXxVMJ1EGL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Naive Bayes Model (7 points)\n"
      ],
      "metadata": {
        "id": "zoz6H2EzUuMn"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcEaYMzl6t69"
      },
      "source": [
        "## Exercise 1: Learning a Naive Bayes Model (2 points)\n",
        "\n",
        "#### We will use the `pomegranate` library to train a Naive Bayes Model. Review ch.6 and see [here](https://pomegranate.readthedocs.io/en/v0.8.1/NaiveBayes.html) for more details.\n",
        "\n",
        "**Note:** The specified version of pomegranate is necessary, so please do not modify it. It may take more than 5-10 mins to install these packages, so please be patient."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Installs required packages\n",
        "!apt install libgraphviz-dev\n",
        "!pip install pomegranate==0.15.0\n",
        "!pip install matplotlib pygraphviz\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "K-7NzYOJsAw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pomegranate.distributions import NormalDistribution, ExponentialDistribution, DiscreteDistribution\n",
        "from pomegranate.NaiveBayes import NaiveBayes\n",
        "from pomegranate.BayesClassifier import BayesClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import KBinsDiscretizer\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "np.random.seed(random_state)"
      ],
      "metadata": {
        "id": "Ky01lMJyq4LC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create training and validation datasets\n",
        "\n",
        "Split the data into training and validation sets using `train_test_split`.  See [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) for details. To get consistent result while splitting, set `random_state` to the value defined earlier. We use 80% of the data for training and 20% of the data for validation."
      ],
      "metadata": {
        "id": "K1oudA3Ct0t9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# convert data to numpy\n",
        "\n",
        "X = df_features.to_numpy()\n",
        "y = df_target.to_numpy().ravel()\n",
        "\n",
        "# split the data into training and validation sets\n",
        "X_train,X_val,y_train,y_val = train_test_split(X,y,test_size=.2,random_state=random_state)"
      ],
      "metadata": {
        "id": "DCNg1LywFylJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 1a: Fit naive bayes model using a single distribution type (1 point)\n",
        "\n",
        "#### Train one naive bayes model using a normal distribution per feature. Train another naive bayes model using an exponential distribution per feature. Hint: use NormalDistribution or ExponentialDistribution and NaiveBayes.from_samples() to fit the model to the data.\n",
        "\n",
        "#### Report the training and validation set accuracies for each model. Hint: use accuracy_score()\n",
        "\n"
      ],
      "metadata": {
        "id": "Zo1LeAUMrYWn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for distribution_obj in [NormalDistribution, ExponentialDistribution]:\n",
        "  print(distribution_obj)\n",
        "  pom_model= # your code here\n",
        "  # report training acc and validation acc\n",
        "  # your code here\n"
      ],
      "metadata": {
        "id": "zWuRrIrWGPu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 1b: Fit a naive bayes model using different feature distributions (2 points)\n",
        "\n",
        "#### Visualize the feature distributions (done for you below) to determine which distribution (normal or exponential) better models a specific feature.\n",
        "\n",
        "#### Train a Naive Bayes classifier using this set of feature-specific distributions. Hint: use NormalDistribution or ExponentialDistribution and NaiveBayes.from_samples() to fit the model to the data.\n",
        "\n",
        "#### Report the training and validation set accuracies for the model. Hint: use accuracy_score()"
      ],
      "metadata": {
        "id": "crzjARjrszy3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# visualization code\n",
        "FEATURE_NAMES=df_features.columns\n",
        "\n",
        "num_cols=3\n",
        "num_rows=int(len(FEATURE_NAMES)/num_cols) if len(FEATURE_NAMES)%num_cols == 0 else int(math.ceil(len(FEATURE_NAMES)/num_cols))\n",
        "fig,ax=plt.subplots(num_rows,num_cols,figsize=(15, 10))\n",
        "\n",
        "for ft_index in np.arange(X_train.shape[1]):\n",
        "  ax[ft_index//num_cols,ft_index%num_cols].hist(X_train[:,ft_index], color='blue')\n",
        "  ax[ft_index//num_cols,ft_index%num_cols].hist(X_val[:,ft_index], color='red')\n",
        "  ax[ft_index//num_cols,ft_index%num_cols].set_title(FEATURE_NAMES[ft_index])\n",
        "\n",
        "fig.tight_layout()"
      ],
      "metadata": {
        "id": "-o5uAQKTx4mr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# insert your code here: train a classifier, report training and validation accuracies"
      ],
      "metadata": {
        "id": "1hG3pn3sG2er"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Comment on any performance difference between this model and the models trained in Ex. a. (1 point)"
      ],
      "metadata": {
        "id": "NjCAytQ27wnu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "your answer here"
      ],
      "metadata": {
        "id": "YApcBNMX7_mt"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQH02U516t7B"
      },
      "source": [
        "## Exercise 2: Learning a Bayes Net (5 points)\n",
        "\n",
        "#### We will use the `pomegranate` library to train a Bayes Net to assess whether relaxing the assumption in Naive bayes (i.e., all features are independent given the target feature) could improve the classification model. Review ch.6 and see [here](https://pomegranate.readthedocs.io/en/v0.8.1/BayesianNetwork.html) for more details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pb8hVFNg6t7B"
      },
      "source": [
        "### Exercise 2a: Create a categorical version of the dataset\n",
        "\n",
        "#### Create categorical versions of the training and validation sets by using equal-frequency binning with the number of bins set to 3 (as in Ex. 1c).\n",
        "\n",
        "#### Use these datasets for training and evaluating the bayes net models in the following exercises.\n",
        "\n",
        "**Note:** This is done because pomegranate currently only supports bayes net over categorical features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v53ZeSlY6t7B"
      },
      "source": [
        "discretizer=KBinsDiscretizer(n_bins=3,encode='ordinal',strategy='quantile')\n",
        "\n",
        "X_train_binned=discretizer.fit_transform(X_train)\n",
        "X_val_binned=discretizer.transform(X_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3PXVTAz6t7B"
      },
      "source": [
        "### Exercise 2b: Construct a Bayes net (2 points)\n",
        "\n",
        "#### Construct and train a Bayes net in which the **texture1** feature node is a parent of the **breast cancer** feature node (only these 2 nodes should be in the net). Use construct_and_train_bayes_net (defined below) by passing in the binned training dataset and specifying the index of the parent feature node.\n",
        "\n",
        "#### Construct and train another Bayes net in which the **perimeter1** feature node is a parent of the **breast cancer** feature node (only these 2 nodes should be in the net). Use construct_and_train_bayes_net (defined below) by passing in the binned training dataset and specifying the index of the parent feature node.\n",
        "\n",
        "#### Report the training and validation accuracies of each Bayes Net. Use get_performance (defined below) by passing in the trained bayes net, binned datasets, and specifying the index of the parent feature node."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6w5LBLj6t7C"
      },
      "source": [
        "from pomegranate import *\n",
        "\n",
        "\"\"\"\n",
        "X_train_binned: ndarray (# instances, # features) This is the binned version of the training set\n",
        "y_train: 1darray (# instances,)\n",
        "ind_chosen_parent_features: 1d numpy array encodes the indices of the features relative to FEATURE_NAMES.\n",
        "                            These indices correspond to features that are parent nodes of the heart disease node.\n",
        "ind_chosen_child_features: 1d numpy array encodes the indices of the features relative to FEATURE_NAMES.\n",
        "                            These indices correspond to features that are children nodes of the heart disease node.\n",
        "\n",
        "Returns a BayesianNetwork representing the trained bayes net\n",
        "\"\"\"\n",
        "def construct_and_train_bayes_net(X_train_binned,\n",
        "                                  y_train,\n",
        "                                  ind_chosen_parent_features=np.array([]),\n",
        "                                  ind_chosen_child_features=np.array([]),\n",
        "                                ):\n",
        "    # parent nodes of heart disease\n",
        "\n",
        "    dist_by_parent_feature=[]\n",
        "    state_by_parent_feature=[]\n",
        "    if len(ind_chosen_parent_features)>0:\n",
        "        parent_feature_names_chosen=FEATURE_NAMES[ind_chosen_parent_features]\n",
        "\n",
        "        for ft_index in ind_chosen_parent_features:\n",
        "            ft_dist=DiscreteDistribution.from_samples(X_train_binned[:,ft_index])\n",
        "            dist_by_parent_feature.append(ft_dist)\n",
        "            state_by_parent_feature.append(State(ft_dist, str(FEATURE_NAMES[ft_index])))\n",
        "        dist_by_parent_feature=np.array(dist_by_parent_feature)\n",
        "        state_by_parent_feature=np.array(state_by_parent_feature)\n",
        "\n",
        "\n",
        "    # heart disease node\n",
        "    if len(ind_chosen_parent_features)>0:\n",
        "        X_train_parent_features_binned_with_labels=np.concatenate((X_train_binned[:,ind_chosen_parent_features],\n",
        "                                                                   np.expand_dims(y_train,axis=1)),axis=1)\n",
        "        heartdisease_dist=ConditionalProbabilityTable.from_samples(X_train_parent_features_binned_with_labels)\n",
        "        # temporary workaround to properly initialize the distribution\n",
        "        heartdisease_dist=ConditionalProbabilityTable(heartdisease_dist.parameters[0],dist_by_parent_feature.tolist())\n",
        "    else:\n",
        "        heartdisease_dist=DiscreteDistribution.from_samples(y_train)\n",
        "    heartdisease_state=State(heartdisease_dist, \"breast cancer\")\n",
        "\n",
        "    # children node of heart disease\n",
        "\n",
        "    dist_by_child_feature=[]\n",
        "    state_by_child_feature=[]\n",
        "    if len(ind_chosen_child_features)>0:\n",
        "        child_feature_names_chosen=FEATURE_NAMES[ind_chosen_child_features]\n",
        "\n",
        "        for ft_index in ind_chosen_child_features:\n",
        "            X_train_child_features_binned_with_labels=np.concatenate((np.expand_dims(y_train,axis=1),\n",
        "                                                                        np.expand_dims(X_train_binned[:,ft_index],axis=1)),\n",
        "                                                                     axis=1)\n",
        "            ft_dist=ConditionalProbabilityTable.from_samples(X_train_child_features_binned_with_labels)\n",
        "            ft_dist=ConditionalProbabilityTable(ft_dist.parameters[0],[heartdisease_dist])\n",
        "            dist_by_child_feature.append(ft_dist)\n",
        "            state_by_child_feature.append(State(ft_dist, str(FEATURE_NAMES[ft_index])))\n",
        "        dist_by_child_feature=np.array(dist_by_child_feature)\n",
        "        state_by_child_feature=np.array(state_by_child_feature)\n",
        "\n",
        "\n",
        "    pom_model = BayesianNetwork()\n",
        "    pom_model.add_states(*list(state_by_parent_feature))\n",
        "    pom_model.add_states(heartdisease_state)\n",
        "    pom_model.add_states(*list(state_by_child_feature))\n",
        "\n",
        "    for parent_index in np.arange(len(ind_chosen_parent_features)):\n",
        "        pom_model.add_edge(state_by_parent_feature[parent_index],heartdisease_state)\n",
        "\n",
        "    for child_index in np.arange(len(ind_chosen_child_features)):\n",
        "        pom_model.add_edge(heartdisease_state, state_by_child_feature[child_index])\n",
        "\n",
        "    pom_model.bake()\n",
        "\n",
        "    return pom_model\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "pom_model: BayesianNetwork represents the trained bayes net model\n",
        "X_train_binned: ndarray (# instances, # features) This is the binned training set\n",
        "y_train: 1darray (# instances,)\n",
        "X_val_binned: ndarray (# instances, # features) This is the binned validation set\n",
        "y_val: 1darray (# instances,)\n",
        "ind_chosen_parent_features: 1d numpy array encodes the indices of the features relative to FEATURE_NAMES.\n",
        "                            These indices correspond to features that are parent nodes of the heart disease node.\n",
        "ind_chosen_child_features: 1d numpy array encodes the indices of the features relative to FEATURE_NAMES.\n",
        "                            These indices correspond to features that are children nodes of the heart disease node.\n",
        "\n",
        "Returns the training and validation set accuracies attained by the bayes net model (pom_model)\n",
        "\"\"\"\n",
        "def get_performance(pom_model, X_train_binned, y_train, X_val_binned, y_val,\n",
        "                    ind_chosen_parent_features=np.array([]), ind_chosen_child_features=np.array([])):\n",
        "    nones_array=np.expand_dims(np.array([None]*len(X_train_binned)),axis=1)\n",
        "    ind_heartdisease_node=len(ind_chosen_parent_features)\n",
        "    if len(ind_chosen_parent_features)>0:\n",
        "        X_train_binned_with_none=X_train_binned[:,ind_chosen_parent_features]\n",
        "        X_train_binned_with_none=np.concatenate((X_train_binned_with_none,nones_array),axis=1)\n",
        "    else:\n",
        "        X_train_binned_with_none=nones_array\n",
        "\n",
        "    if len(ind_chosen_child_features)>0:\n",
        "        X_train_binned_with_none=np.concatenate((X_train_binned_with_none,\n",
        "                                                X_train_binned[:,ind_chosen_child_features]),\n",
        "                                               axis=1)\n",
        "    pred_labels=np.array(pom_model.predict(X_train_binned_with_none),dtype='int64')[:,ind_heartdisease_node]\n",
        "    train_acc=accuracy_score(y_train, pred_labels)\n",
        "\n",
        "    nones_array=np.expand_dims(np.array([None]*len(X_val_binned)),axis=1)\n",
        "    if len(ind_chosen_parent_features)>0:\n",
        "        X_val_binned_with_none=X_val_binned[:,ind_chosen_parent_features]\n",
        "        X_val_binned_with_none=np.concatenate((X_val_binned_with_none,nones_array),axis=1)\n",
        "    else:\n",
        "        X_val_binned_with_none=nones_array\n",
        "\n",
        "    if len(ind_chosen_child_features)>0:\n",
        "        X_val_binned_with_none=np.concatenate((X_val_binned_with_none,\n",
        "                                               X_val_binned[:,ind_chosen_child_features]),\n",
        "                                               axis=1)\n",
        "    pred_labels=np.array(pom_model.predict(X_val_binned_with_none),dtype='int64')[:,ind_heartdisease_node]\n",
        "    val_acc=accuracy_score(y_val, pred_labels)\n",
        "\n",
        "    return train_acc, val_acc\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# insert your code here"
      ],
      "metadata": {
        "id": "CF4uSqLCHYiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88TK3MDm6t7D"
      },
      "source": [
        "#### Comment on which feature seems more informative for predicting the presence of breast cancer. (1 point)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUXF_J8i6t7D"
      },
      "source": [
        "your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlB83Bqj6t7D"
      },
      "source": [
        "### Exercise 2c: Construct a Bayes net with parent and children nodes (3 points)\n",
        "\n",
        "#### Here, we'll implement a Bayes net with both parent nodes and children nodes.\n",
        "\n",
        "#### Construct and train a Bayes net in which:\n",
        "#### -the following features are all parents of the breast cancer feature node (radius, texture1, perimeter1).  \n",
        "#### -the following features are all children of the breast cancer feature node (area1, smoothness1, compactness1).\n",
        "#### Use construct_and_train_bayes_net by passing in the binned training dataset and specifying the indices of the parent feature nodes and indices of the children feature nodes.\n",
        "\n",
        "#### Report the training and validation accuracy of the Bayes Net using get_performance by passing in the trained bayes net, binned datasets, and indices of the parent and children feature nodes."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# insert your code here"
      ],
      "metadata": {
        "id": "rm1br4_7HbhX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccWkBvd96t7D"
      },
      "source": [
        "#### Compare the performance of this Bayes net against the Bayes nets from Ex. 2b. (1 point)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eudq5MXw6t7E"
      },
      "source": [
        "your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression (2 point)"
      ],
      "metadata": {
        "id": "60txjiNSU9KG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create training and validation datasets\n",
        "\n",
        "Split the data into training and validation set using train_test_split. See [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) for details. To get consistent result while splitting, set random_state to the value defined earlier. We use 80% of the data for training and 20% of the data for validation.\n",
        "\n",
        "### Preprocess the dataset\n",
        "\n",
        "Preprocess the data by normalizing each feature to have zero mean and unit standard deviation. This can be done using the `StandardScaler()` function. See [here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) for more details.\n"
      ],
      "metadata": {
        "id": "YfvRuHPmABcG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# convert data to numpy\n",
        "\n",
        "X = df_features.to_numpy()\n",
        "y = df_target.to_numpy().ravel()\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X,y,test_size=.2,random_state=random_state)\n",
        "\n",
        "# Preprocess the data by normalizing each feature to have zero mean and unit standard deviation. This can be done using the StandardScaler() function.\n",
        "# Define the scaler for scaling the data\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Normalize the training data\n",
        "# your code here\n",
        "\n",
        "# Use the scaler defined above to standardize the validation data by applying the same transformation to the validation data.\n",
        "# your code here"
      ],
      "metadata": {
        "id": "0f7pNFlJBVPt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZY5Qfz36rW_"
      },
      "source": [
        "### Use `sklearn`'s `SGDClassifier` to train a multinomial logistic regression classifier (i.e., using a one-versus-rest scheme) with Stochastic Gradient Descent. Review ch.7 and see [here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier) for more details.\n",
        "\n",
        "#### Set the `random_state` as defined above,  increase the `n_iter_no_change` to 100 and `max_iter` to 1000 to facilitate better convergence.  \n",
        "\n",
        "#### Report the model's accuracy over the training and validation sets.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# your code here\n"
      ],
      "metadata": {
        "id": "mqT0vtGBHxNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compare different methods (1 point)\n",
        "\n",
        "####Discuss advantages and disadvantages of every method. Which one you think works the best for this dataset and why?\n",
        "\n",
        "\n",
        "your answer here"
      ],
      "metadata": {
        "id": "cMCIYw1dDM_g"
      }
    }
  ]
}