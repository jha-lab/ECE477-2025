{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g3ivv9pWqOKZ"
   },
   "source": [
    "#Put your Google Colab link here:\n",
    "*your link here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yCS3Ib5GhmFP"
   },
   "source": [
    "## Important notice: any use of generative AI for completing the assignment is strictly prohibited."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r9W5mZDlxa9K"
   },
   "source": [
    "# DOCTOR: A Multi-Disease Detection Continual Learning Framework Based on Wearable Medical Sensors\n",
    "\n",
    "In this exercise, we will use PyTorch to recreate some of the experiments done in the [DOCTOR](https://dl.acm.org/doi/full/10.1145/3679050) paper. We will perform domain-, class-, and task-incremental learning for a multilayer perceptron (MLP) model detecting diabetes and mental health disorders using replay-based continual learning methods.\n",
    "\n",
    "*   Navigate to the tabs above. Click `Runtime` -> `Change runtime type` -> select `GPU` as the hardware accelerator to enable GPU, which will allow you to train faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9FJNIcUXtnR4"
   },
   "source": [
    "# Part 1: Prepare data (0.5 pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xuaKG1SVzJHk"
   },
   "source": [
    "##Import useful libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qy208qs7wWk8"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from torch.optim import SGD\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import Module, ReLU, Linear, LogSoftmax, CrossEntropyLoss\n",
    "\n",
    "from sys import getsizeof\n",
    "from itertools import cycle\n",
    "from scipy.stats import kstest\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.mixture import GaussianMixture as GMM\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-yKoHry41pcF"
   },
   "source": [
    "##Get access to a GPU:\n",
    "To gain access to the GPUs on Colab, navigate to the `Runtime` tab above and select `Change runtime type`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "STrgYrSs1r61"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "54vFnhPvIr_I"
   },
   "source": [
    "## Prepare the experimental datasets:\n",
    "\n",
    "### Experimental datasets:\n",
    "\n",
    "For this exercise, we use the DiabDeep dataset collected for the [DiabDeep](https://ieeexplore.ieee.org/abstract/document/8935429) project. We will use the DiabDeep dataset for the domain- and class-incremental learning experiments.\n",
    "\n",
    "*  **The DiabDeep Dataset:**\n",
    "The DiabDeep dataset contains physiological signals and environmental information collected from 25 non-diabetic individuals, 14 Type-I diabetic patients, and 13 Type-II diabetic patients with a smartwatch and a smartphone.\n",
    "\n",
    "The datasets used here are their streamlined versions. The DiabDeep dataset contains 20,957 data instances with 4,485 features. Refer to Table 2 in [DOCTOR](https://dl.acm.org/doi/full/10.1145/3679050) for the data features included in these datasets.\n",
    "\n",
    "### Data preprocessing:\n",
    "\n",
    "It is crucial and a good practice to [preprocess](https://neptune.ai/blog/data-preprocessing-guide) your dataset before you jump into model training. Data preprocessing includes handling missing values, data nomalization, feature selection, dimensionality reduction, etc. This part has been done for you. See Section 5.2 in [DOCTOR](https://dl.acm.org/doi/full/10.1145/3679050) for the details about the dataset preprocessing that had been done.\n",
    "\n",
    "### Prepare data for experiments:\n",
    "\n",
    "* **Domain-incremental learning:**\n",
    "For the domain-incremental learning experiment, we split the DiabDeep dataset into two missions (domains) in a stratified fashion. Mission-1 contains data from 80% of the patient files (20 healthy, 11 Type-I, and 10 Type-II), while Mission-2 contains data from the other 20% of the patient files (5 healthy, 3 Type-I, and 3 Type-II). Then, we take the first 70%, the next 10%, and the last 20% of each patient's sequential time series data to construct the training, validation, and test sets with no time overlap.\n",
    "\n",
    "* **Class-incremental learning:**\n",
    "For the class-incremental learning experiment, we split the dataset into two missions. Mission-1 contains data from healthy individuals and Type-I diabetic patients, while Mission-2 includes solely the Type-II diabetic patients. Similarly, we take the first 70%, the next 10%, and the last 20% of each patient's sequential time series data to construct the training, validation, and test sets with no time overlap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b39RDi_62n0J"
   },
   "source": [
    "##Create a custom dataset class: (0.5 pt)\n",
    "To prepare our datasets, we create a `CustomDataset` class that inherits PyTorch's [Dataset](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7_rOx9El3ZFG"
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        # Transform x, y to torch tensors\n",
    "        \"\"\"TO DO\"\"\"\n",
    "        self.x =\n",
    "        self.y =\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the len of the dataset\n",
    "        \"\"\"TO DO\"\"\"\n",
    "        return\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index, :], self.y[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Grnmn8J769nx"
   },
   "source": [
    "# Part 2: Domain-Incremental Learning Experiment (14 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DZufgx0c3ibS"
   },
   "source": [
    "##Load the experimental data and create the datasets and dataloaders: (1 pt)\n",
    "The dataset has been processed into the training, validation, and test sets for the two domains for you. You just need to load the corresponding experimental data.\n",
    "\n",
    "First, we use Numpy to load the experimental data. Then, we create the training, validation, and test sets using the `CustomDataset` class. Finally, we instantiate dataloaders with PyTorch's [DataLoader](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hPcYSdbK5rxI"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EMX3MVXH3rwu"
   },
   "outputs": [],
   "source": [
    "# Unzip the dataset\n",
    "!unzip \"/content/drive/Shared drives/ECE477 datasets/Assignment6/diabdeep_DIL_data.zip\" -d diabdeep_DIL_data\n",
    "\n",
    "# Check out the experimental data\n",
    "print('\\n', os.listdir('diabdeep_DIL_data/'))\n",
    "\n",
    "# Load the data features x from Mission-1 (Domain-1) as type \"float32\"\n",
    "\"\"\"TO DO\"\"\"\n",
    "x_train_1 =\n",
    "x_valid_1 =\n",
    "x_test_1 =\n",
    "\n",
    "# Load the data features x from Mission-2 (Domain-2) as type \"float32\"\n",
    "\"\"\"TO DO\"\"\"\n",
    "x_train_2 =\n",
    "x_valid_2 =\n",
    "x_test_2 =\n",
    "\n",
    "# Load the labels y from Mission-1 (Domain)-1 as type \"int64\"\n",
    "\"\"\"TO DO\"\"\"\n",
    "y_train_1 =\n",
    "y_valid_1 =\n",
    "y_test_1 =\n",
    "\n",
    "# Load the labels y from Mission-2 (Domain)-2 as type \"int64\"\n",
    "\"\"\"TO DO\"\"\"\n",
    "y_train_2 =\n",
    "y_valid_2 =\n",
    "y_test_2 =\n",
    "\n",
    "# Inspect the labels for both missions\n",
    "print(f\"Mission-1 labels: {np.unique(y_test_1)}\")\n",
    "print(f\"Mission-2 labels: {np.unique(y_test_2)}\")\n",
    "\n",
    "# check dimensions\n",
    "print(f\"x_train_1 shape: {x_train_1.shape}\")\n",
    "\n",
    "# Instantiate the datasets with CustomDataset\n",
    "\"\"\"TO DO\"\"\"\n",
    "train_1 =\n",
    "valid_1 =\n",
    "test_1 =\n",
    "train_2 =\n",
    "valid_2 =\n",
    "test_2 =\n",
    "\n",
    "# Set the batch size to 128\n",
    "batch_size = 128\n",
    "\n",
    "# Instantiate the dataloaders\n",
    "# Note that we shuffle all of our datasets except for train_loader_1\n",
    "# since we need to record each training data instance's training loss\n",
    "# over all epochs for data selection (details later)\n",
    "# Note that we halve the batch size for train_loader_2\n",
    "# since we will train the model with a balanced amount of data from both domains\n",
    "# in each batch for continual learning\n",
    "\"\"\"TO DO\"\"\"\n",
    "train_loader_1 =\n",
    "valid_loader_1 =\n",
    "test_loader_1 =\n",
    "train_loader_2 =\n",
    "valid_loader_2 =\n",
    "test_loader_2 ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n2-1GRFL94cQ"
   },
   "source": [
    "##Create an MLP model: (1.5 pts)\n",
    "\n",
    "Here, we will build a four-layer MLP model. See [here](https://pytorch.org/tutorials/recipes/recipes/defining_a_neural_network.html) for an example of building a deep neural network with PyTorch.\n",
    "\n",
    "\n",
    "\n",
    "*   Use [Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) to define a fully-connected feed-forward layer\n",
    "*   Use [ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html) for the activation function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8JLeV60B465H"
   },
   "outputs": [],
   "source": [
    "class MLP(Module):\n",
    "    def __init__(self, in_num, out_num, hidden_num1, hidden_num2, hidden_num3):\n",
    "        super(MLP, self).__init__()\n",
    "        # Add the first hidden layer and ReLU activation\n",
    "        \"\"\"TO DO\"\"\"\n",
    "        self.hidden1 =\n",
    "        self.relu1 =\n",
    "\n",
    "        # Add the second hidden layer and ReLU activation\n",
    "        \"\"\"TO DO\"\"\"\n",
    "        self.hidden2 =\n",
    "        self.relu2 =\n",
    "\n",
    "        # Add the third hidden layer and ReLU activation\n",
    "        \"\"\"TO DO\"\"\"\n",
    "        self.hidden3 =\n",
    "        self.relu3 =\n",
    "\n",
    "        # Add the output layer\n",
    "        \"\"\"TO DO\"\"\"\n",
    "        self.out =\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass x forward the first hidden layer and ReLU activation\n",
    "        \"\"\"TO DO\"\"\"\n",
    "\n",
    "        # Pass x forward the second hidden layer and ReLU activation\n",
    "        \"\"\"TO DO\"\"\"\n",
    "\n",
    "        # Pass x forward the third hidden layer and ReLU activation\n",
    "        \"\"\"TO DO\"\"\"\n",
    "\n",
    "        # Pass x forward the output layer\n",
    "        \"\"\"TO DO\"\"\"\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t-hOEiXykzmk"
   },
   "source": [
    "##Define the accuracy evaluation function: (1 pt)\n",
    "\n",
    "Here, we define the helper function `acc_eval()` that evaluates the model's accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zpS2tYSNlU3d"
   },
   "outputs": [],
   "source": [
    "def acc_eval(mlp, loader, set_len):\n",
    "    '''\n",
    "        Evaluate accuracy.\n",
    "\n",
    "        Args:\n",
    "            mlp:        The MLP model\n",
    "            loader:     DataLoader\n",
    "            set_len:    Number of data in the dataset\n",
    "        Returns:\n",
    "            accuracy:   Accuracy\n",
    "    '''\n",
    "    correct = 0\n",
    "    # Set mlp model to evaluation state\n",
    "    \"\"\"TO DO\"\"\"\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (x, y) in loader:\n",
    "            # Cast the features x to `device`\n",
    "            \"\"\"TO DO\"\"\"\n",
    "\n",
    "            # Cast the labels y to `device`\n",
    "            \"\"\"TO DO\"\"\"\n",
    "\n",
    "            # Use the MLP model to predict on x and get the predicted labels y_hat (which has the max probability)\n",
    "            \"\"\"TO DO\"\"\"\n",
    "            y_hat =\n",
    "            correct += (y_hat == y).int().sum()\n",
    "    accuracy = correct / set_len\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wnzMe3DHlzV_"
   },
   "source": [
    "##Initialize parameters:\n",
    "We initialize some of the parameters we will use in this exercise here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ofx59zTlwIw"
   },
   "outputs": [],
   "source": [
    "random_state = 0      # Set random state to 0\n",
    "learning_rate = 5e-3  # Initial learning rate for the SGD optimizer\n",
    "momentum = 9e-1       # Momentum for the SGD optimizer\n",
    "epoch = 30           # Number of epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vgj3JPt_VqsQ"
   },
   "source": [
    "##Define the training pipeline: (2 pts)\n",
    "\n",
    "We define the training function `train_model()` that we'll use to train our MLP model here.\n",
    "\n",
    "*   Use the [stochastic gradient descent optimizer](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html) with momentum\n",
    "*   Use the [cross entropy loss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) as our loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fmp4rkLfVphD"
   },
   "outputs": [],
   "source": [
    "def train_model(mlp, epoch, train_loader, valid_loader, train_set_len, valid_set_len,\n",
    "                file_name, verbose=True, save_loss=False, labels=None, pre_loader=None):\n",
    "    '''\n",
    "        Train the neural network.\n",
    "\n",
    "        Args:\n",
    "            mlp:            The MLP model\n",
    "            epoch:          Number of epochs\n",
    "            train_loader:   DataLoader for training data\n",
    "            valid_loader:   DataLoader for validation data\n",
    "            train_set_len:  Number of data in the training dataset\n",
    "            valid_set_len:  Number of data in the validation dataset\n",
    "            file_name:      File name to store the state dict\n",
    "            verbose:        See the training progress or not\n",
    "            save_loss:      Save the loss matrix or not\n",
    "            labels:         Labels of the training data if save_loss == True\n",
    "            pre_loader:     DataLoader for synthetic data representing previous task\n",
    "        Returns:\n",
    "            tra_acc:        Training accuracy throughout all epochs\n",
    "            val_acc:        Validation accuracy throughout all epochs\n",
    "            _loss_matrix:   Loss matrix storing the average training loss\n",
    "    '''\n",
    "    # Initialize the SGD optimizer\n",
    "    \"\"\"TO DO\"\"\"\n",
    "    opt =\n",
    "    # Initialize the cross entropy loss\n",
    "    \"\"\"TO DO\"\"\"\n",
    "    lossF =\n",
    "    # Initialize two lists to record training accuracy and validation accuracy, respectively\n",
    "    tra_acc, val_acc = [], []\n",
    "\n",
    "    # Perform generative replay continual learning with data from pre_loader\n",
    "    if pre_loader:\n",
    "        for e in range(1, epoch + 1):\n",
    "            # Set the mlp model to training state\n",
    "            \"\"\"TO DO\"\"\"\n",
    "\n",
    "            # Initialize the running loss of the current epoch to 0.0\n",
    "            running_loss = 0.0\n",
    "            # Get training data from both the train_loader and pre_loader\n",
    "            for i, (data1, data2) in enumerate(zip(train_loader, cycle(pre_loader)), 1):\n",
    "                # Zero out the gradients\n",
    "                \"\"\"TO DO\"\"\"\n",
    "\n",
    "                x1, y1 = data1\n",
    "                x2, y2 = data2\n",
    "                # Concatenate features (x1, x2) from both DataLoaders along the correct dimension\n",
    "                \"\"\"TO DO\"\"\"\n",
    "                x =\n",
    "                # Concatenate labels (y1, y2) from both DataLoaders along the correct dimension\n",
    "                \"\"\"TO DO\"\"\"\n",
    "                y =\n",
    "                # Cast the features to `device`\n",
    "                \"\"\"TO DO\"\"\"\n",
    "\n",
    "                # Cast the labels to `device`\n",
    "                \"\"\"TO DO\"\"\"\n",
    "\n",
    "\n",
    "                # Calculate the loss\n",
    "                \"\"\"TO DO\"\"\"\n",
    "                loss =\n",
    "                # Back propagate the gradient\n",
    "                \"\"\"TO DO\"\"\"\n",
    "\n",
    "\n",
    "                # Update the model\n",
    "                \"\"\"TO DO\"\"\"\n",
    "\n",
    "                # Accumulate the running loss\n",
    "                running_loss += loss.item()\n",
    "\n",
    "                # Save training loss for data selection later\n",
    "                if save_loss:\n",
    "                    _lossF = CrossEntropyLoss(reduction='none')\n",
    "                    _loss_tr = _lossF(mlp(x1.to(device)), y1.to(device)).cpu().detach().numpy().reshape((-1, 1))\n",
    "                    _loss_pr = _lossF(mlp(x2.to(device)), y2.to(device)).cpu().detach().numpy().reshape((-1, 1))\n",
    "                    if i == 1:\n",
    "                        tmp_loss_tr_matrix = np.copy(_loss_tr)\n",
    "                        tmp_loss_pr_matrix = np.copy(_loss_pr)\n",
    "                    else:\n",
    "                        tmp_loss_tr_matrix = np.concatenate((tmp_loss_tr_matrix, _loss_tr), axis=0)\n",
    "                        tmp_loss_pr_matrix = np.concatenate((tmp_loss_pr_matrix, _loss_pr), axis=0)\n",
    "\n",
    "            # Save training loss for data selection later\n",
    "            if save_loss:\n",
    "                if e == 1:\n",
    "                    _loss_tr_matrix = np.copy(tmp_loss_tr_matrix)\n",
    "                    _loss_pr_matrix = np.copy(tmp_loss_pr_matrix)\n",
    "                else:\n",
    "                    _loss_tr_matrix += tmp_loss_tr_matrix     # Accumulate loss\n",
    "                    _loss_pr_matrix += tmp_loss_pr_matrix     # Accumulate loss\n",
    "                if e == epoch:\n",
    "                    _loss_tr_matrix = _loss_tr_matrix / e     # Get the average loss value over epcoh\n",
    "                    _loss_pr_matrix = _loss_pr_matrix / e     # Get the average loss value over epoch\n",
    "                    _loss_tr_matrix = np.concatenate((_loss_tr_matrix[:len(labels[0]), :], labels[0].reshape((-1, 1))), axis=1)\n",
    "                    _loss_pr_matrix = np.concatenate((_loss_pr_matrix[:len(labels[1]), :], labels[1].reshape((-1, 1))), axis=1)\n",
    "                    _loss_matrix = np.concatenate((_loss_tr_matrix, _loss_pr_matrix), axis=0)\n",
    "\n",
    "            tra_accuracy = acc_eval(mlp, train_loader, train_set_len)\n",
    "            val_accuracy = acc_eval(mlp, valid_loader, valid_set_len)\n",
    "            tra_acc.append(tra_accuracy.cpu())\n",
    "            val_acc.append(val_accuracy.cpu())\n",
    "\n",
    "            if verbose:\n",
    "                print(\n",
    "                    f'[{e}] loss: {running_loss / i:.3f} \\t training_accuracy: {tra_accuracy:.3f} \\t validation_accuracy: {val_accuracy:.3f}')\n",
    "\n",
    "            # Save parameter weights if they achieve optimal validation accuracy\n",
    "            if val_accuracy >= max(val_acc):\n",
    "                torch.save(mlp.state_dict(), file_name)\n",
    "\n",
    "    # Regular training pipeline without data from pre_loader for generative replay\n",
    "    else:\n",
    "        for e in range(1, epoch + 1):\n",
    "            # Set the model to train state\n",
    "            mlp.train()\n",
    "            # Initialize the running loss of the current epoch to 0.0\n",
    "            running_loss = 0.0\n",
    "\n",
    "            for i, (x, y) in enumerate(train_loader, 1):\n",
    "                # Zero out the gradients\n",
    "                \"\"\"TO DO\"\"\"\n",
    "\n",
    "                # Cast the features to `device`\n",
    "                \"\"\"TO DO\"\"\"\n",
    "\n",
    "                # Cast the labels to `device`\n",
    "                \"\"\"TO DO\"\"\"\n",
    "\n",
    "\n",
    "                # Calculate the loss\n",
    "                \"\"\"TO DO\"\"\"\n",
    "                loss =\n",
    "                # Back propagate the gradient\n",
    "                \"\"\"TO DO\"\"\"\n",
    "\n",
    "\n",
    "                # Update the model\n",
    "                \"\"\"TO DO\"\"\"\n",
    "\n",
    "                # Accumulate the running loss\n",
    "                running_loss += loss.item()\n",
    "\n",
    "                # Save training loss for data selection later\n",
    "                if save_loss:\n",
    "                    _lossF = CrossEntropyLoss(reduction='none')\n",
    "                    _loss = _lossF(mlp(x), y).cpu().detach().numpy().reshape((-1, 1))\n",
    "                    if i == 1:\n",
    "                        tmp_loss_matrix = np.copy(_loss)\n",
    "                    else:\n",
    "                        tmp_loss_matrix = np.concatenate((tmp_loss_matrix, _loss), axis=0)\n",
    "\n",
    "            # Save training loss for data selection later\n",
    "            if save_loss:\n",
    "                if e == 1:\n",
    "                    _loss_matrix = np.copy(tmp_loss_matrix)\n",
    "                else:\n",
    "                    _loss_matrix += tmp_loss_matrix     # Accumulate loss\n",
    "                if e == epoch:\n",
    "                    _loss_matrix = _loss_matrix / e     # Get the average loss value over epoch\n",
    "                    _loss_matrix = np.concatenate((_loss_matrix, labels.reshape((-1, 1))), axis=1)\n",
    "\n",
    "            tra_accuracy = acc_eval(mlp, train_loader, train_set_len)\n",
    "            val_accuracy = acc_eval(mlp, valid_loader, valid_set_len)\n",
    "            tra_acc.append(tra_accuracy.cpu())\n",
    "            val_acc.append(val_accuracy.cpu())\n",
    "\n",
    "            if verbose:\n",
    "                print(\n",
    "                    f'[{e}] loss: {running_loss / i:.3f} \\t training_accuracy: {tra_accuracy:.3f} \\t validation_accuracy: {val_accuracy:.3f}')\n",
    "\n",
    "            # Save parameter weights if they achieve optimal validation accuracy\n",
    "            if val_accuracy >= max(val_acc):\n",
    "                torch.save(mlp.state_dict(), file_name)\n",
    "    if save_loss:\n",
    "        return tra_acc, val_acc, _loss_matrix\n",
    "    else:\n",
    "        return tra_acc, val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OlhJXK104ncB"
   },
   "source": [
    "##Define some helper functions: (1 pt)\n",
    "\n",
    "We define some helper functions here for plotting accuracy curves and printing  metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cm8HO6BRmrYo"
   },
   "outputs": [],
   "source": [
    "# Plot accuracy curves for training and validation accuracy\n",
    "def plot_acc(t_acc, v_acc, file_name):\n",
    "    e = range(1, len(t_acc) + 1)\n",
    "    plt.figure()\n",
    "    plt.plot(e, t_acc, 'b', label='Training Accuracy')\n",
    "    plt.plot(e, v_acc, 'r', label='Validation Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.legend()\n",
    "    plt.savefig(file_name)\n",
    "    plt.show()\n",
    "\n",
    "# Print accuracy metrics\n",
    "def print_acc(mlp, train_loader, train_set_len, valid_loader, valid_set_len, mission1_test_loader, mission1_test_set_len, mission2_test_loader, mission2_test_set_len):\n",
    "    '''\n",
    "        Print accuracy metrics.\n",
    "\n",
    "        Args:\n",
    "            mlp:                     The MLP model\n",
    "            train_loader:            DataLoader for training data\n",
    "            train_set_len:           Number of data in the training set\n",
    "            valid_loader:            DataLoader for validation data\n",
    "            valid_set_len:           Number of data in the validation set\n",
    "            mission1_test_loader:    DataLoader for Mission-1 test data\n",
    "            mission1_test_set_len:   Number of data in the Mission-1 test set\n",
    "            mission2_test_loader:    DataLoader for Mission-2 test data\n",
    "            mission2_test_set_len:   Number of data in the Mission-2 test set\n",
    "        Returns:\n",
    "            accuracy1:               Accuracy on the Mission-1 test set (for BWT calculation)\n",
    "    '''\n",
    "    # Use acc_eval() to print training accuracy\n",
    "    \"\"\"TO DO\"\"\"\n",
    "    accuracy =\n",
    "    print(f'Training accuracy:        {accuracy:.3f}')\n",
    "\n",
    "    # Use acc_eval() to print validation accuracy\n",
    "    \"\"\"TO DO\"\"\"\n",
    "    accuracy =\n",
    "    print(f'Validation accuracy:      {accuracy:.3f}')\n",
    "\n",
    "    # Use acc_eval() to print Mission-1 test accuracy\n",
    "    \"\"\"TO DO\"\"\"\n",
    "    accuracy1 =\n",
    "    print(f'Mission-1 test accuracy:   {accuracy1:.3f}')\n",
    "\n",
    "    # Use acc_eval() to print Mission-2 test accuracy\n",
    "    \"\"\"TO DO\"\"\"\n",
    "    accuracy2 =\n",
    "    print(f'Mission-2 test accuracy:   {accuracy2:.3f}')\n",
    "\n",
    "    # Calculate the average test accuracy for both Mission-1 and Mission-2\n",
    "    \"\"\"TO DO\"\"\"\n",
    "    avg_accuracy =\n",
    "    print(f'Average test accuracy:    {avg_accuracy:.3f}')\n",
    "\n",
    "    return accuracy1.cpu().item()\n",
    "\n",
    "# Print average F1-score\n",
    "def avg_F_metrics(mlp, experiment):\n",
    "    '''\n",
    "        Print F1-score metrics.\n",
    "\n",
    "        Args:\n",
    "            mlp:            The MLP model\n",
    "            experiment:     Specify the experiment conducted:\n",
    "                            ['DIL': for domain-incremental learning,\n",
    "                             'CIL': for class-incremental learning]\n",
    "        Returns:\n",
    "            None\n",
    "    '''\n",
    "    mlp.eval()\n",
    "    with torch.no_grad():\n",
    "        x1 = torch.as_tensor(x_test_1).to(device)\n",
    "        x2 = torch.as_tensor(x_test_2).to(device)\n",
    "        y_pred_1 = torch.argmax(mlp(x1), 1).cpu().view(-1, 1)\n",
    "        y_pred_2 = torch.argmax(mlp(x2), 1).cpu().view(-1, 1)\n",
    "        y_real_1 = torch.as_tensor(y_test_1).view(-1, 1)\n",
    "        y_real_2 = torch.as_tensor(y_test_2).view(-1, 1)\n",
    "    print(f\"Classification Report for Mission-1: \\n {classification_report(y_real_1, y_pred_1, labels=[0, 1, 2], target_names=['class 0', 'class 1', 'class 2'], zero_division=0.0)}\")\n",
    "    print(f\"Classification Report for Mission-2: \\n {classification_report(y_real_2, y_pred_2, labels=[0, 1, 2], target_names=['class 0', 'class 1', 'class 2'], zero_division=0.0)}\")\n",
    "    mission1_report = classification_report(y_real_1, y_pred_1, labels=[0, 1, 2], target_names=['class 0', 'class 1', 'class 2'], zero_division=0.0, output_dict=True)\n",
    "    mission2_report = classification_report(y_real_2, y_pred_2, labels=[0, 1, 2], target_names=['class 0', 'class 1', 'class 2'], zero_division=0.0, output_dict=True)\n",
    "    if experiment == 'CIL':\n",
    "        print(f\"Average F1-Score:   {(mission1_report['weighted avg']['f1-score'] + mission2_report['weighted avg']['f1-score']) / 2:.3f}\")\n",
    "    else:\n",
    "        print(f\"Average F1-Score:   {(mission1_report['macro avg']['f1-score'] + mission2_report['macro avg']['f1-score']) / 2:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nGb7Mq2TmcrH"
   },
   "source": [
    "## Initialize model parameters:  (0.5 pt)\n",
    "\n",
    "We initialize the hyperparameters for our MLP model here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jGRl1gTJyAOG"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters for our MLP model\n",
    "\"\"\"TO DO\"\"\"\n",
    "in_num =           # The number of input features\n",
    "hidden_num1 = 256     # Number of neurons in hidden layer 1\n",
    "hidden_num2 = 128     # Number of neurons in hidden layer 2\n",
    "hidden_num3 = 128     # Number of neurons in hidden layer 3\n",
    "out_num =            # Number of output classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c1wQvvEVm7Oq"
   },
   "source": [
    "## Build the baseline model: (1 pt)\n",
    "\n",
    "Here, we train our MLP model with data from Mission-1 (Domain-1) only. This is our baseline model that has only seen Mission-1 (Domain-1).\n",
    "\n",
    "Use the `train_model()` function defined above to train our MLP model.\n",
    "\n",
    "In addition, we set `save_loss = True` and assign `labels = y_train_1` to obtain a `loss_matrix` that records the average training loss values of all training data instances for our data preservation continual learning method later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iyGHtC8Klprj"
   },
   "outputs": [],
   "source": [
    "# Random states\n",
    "random.seed(random_state)\n",
    "np.random.seed(random_state)\n",
    "torch.manual_seed(random_state)\n",
    "if device == 'cuda':\n",
    "  torch.cuda.manual_seed(random_state)\n",
    "  torch.cuda.manual_seed_all(random_state)\n",
    "  torch.backends.cudnn.deterministic = True\n",
    "  torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Instansiate the model and cast it to `device`\n",
    "\"\"\"TO DO\"\"\"\n",
    "mlp =\n",
    "\n",
    "# Use train_model() to train the mlp model with data from Mission-1 (Domain-1)\n",
    "# Store the optimal model weights to the file 'domain_1.pt'\n",
    "# Specify save_loss=True and labels=y_train_1 to record the average training\n",
    "# loss values of the training data in loss_matrix\n",
    "\"\"\"TO DO\"\"\"\n",
    "tra_acc, val_acc, loss_matrix ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dBuuu1vioDxJ"
   },
   "source": [
    "## Evaluate performance: (1 pt)\n",
    "\n",
    "Now, we evaluate the MLP model's performance on both domains. You should see that our baseline model does a good job on the test set from Mission-1 (Domain-1), but it performs poorly on the test set from Mission-2 (Domain-2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M9DMDuLI6T7O"
   },
   "outputs": [],
   "source": [
    "# Load the optimal weights that gives the highest validation accuracy during training\n",
    "\"\"\"TO DO\"\"\"\n",
    "\n",
    "\n",
    "# Make a plot for the training and validation accuracy\n",
    "plot_acc(tra_acc, val_acc, 'domain_1_acc')\n",
    "\n",
    "print()\n",
    "print(\"#### Results from Training on Domain 1 Only ####\")\n",
    "# Use print_acc() to print the accuracy metrics on the training, validation, and test sets from Mission-1 (Domain-1), and the test set from Mission-2 (Domain-2)\n",
    "\"\"\"TO DO\"\"\"\n",
    "baseline_acc_domain1 =\n",
    "\n",
    "# Use avg_F_metrics() to print the F1-score metric\n",
    "\"\"\"TO DO\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0vy4naVuNYDg"
   },
   "source": [
    "## Naive fine-tuning: (1 pt)\n",
    "\n",
    "Now, we naively fine-tune our baseline model with data from Mission-2 (Domain-2). You should observe that the model performs very well on the test set from Mission-2 (Domain-2) now, but performs very badly on the test set from Mission-1 (Domain-1). The performance on data from Mission-1 (Domain-1) deteriorate greatly. This is ***catastrophic forgetting***.\n",
    "\n",
    "In addition, we report an additional metric, called **backward transfer**, to evaluate the performance of the continual learning algorithm. When learning a new mission, backward transfer measures how much the continual learning algorithm impacts the performance of the model on previous missions. See Section 3.2 in [DOCTOR](https://dl.acm.org/doi/full/10.1145/3679050) for details about backward transfer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qCkoipvLNVv6"
   },
   "outputs": [],
   "source": [
    "# Random states\n",
    "random.seed(random_state)\n",
    "np.random.seed(random_state)\n",
    "torch.manual_seed(random_state)\n",
    "if device == 'cuda':\n",
    "  torch.cuda.manual_seed(random_state)\n",
    "  torch.cuda.manual_seed_all(random_state)\n",
    "  torch.backends.cudnn.deterministic = True\n",
    "  torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# load baseline model\n",
    "\"\"\"TO DO\"\"\"\n",
    "\n",
    "# Train the model with data from Mission-2 (Domain-2)\n",
    "# Make sure to give a different file_name to store the optimal weights for this naively fine-tuned model\n",
    "# We will use our baseline model's optimal weights, domain_1.pt, in the latter experiments\n",
    "# Store the optimal model weights to the file 'domain_2.pt'\n",
    "# Keep default values for verbose=True, save_loss=False, labels=None, pre_loader=None\n",
    "\"\"\"TO DO\"\"\"\n",
    "tra_acc, val_acc =\n",
    "\n",
    "# Load the optimal weights that gives the highest validation accuracy during naive fine-tuning\n",
    "\"\"\"TO DO\"\"\"\n",
    "\n",
    "print()\n",
    "print(\"#### Results from Naively Fine-tune on Domain 2 Only ####\")\n",
    "# Use print_acc() to print the accuracy metrics on the training, validation, and test sets from Mission-2 (Domain-2), and the test set from Mission-1 (Domain-1)\n",
    "naive_acc_domain1 =\n",
    "print()\n",
    "# Use avg_F_metrics() to print the F1-score metric\n",
    "\"\"\"TO DO\"\"\"\n",
    "\n",
    "print()\n",
    "# Calculate and print the backward transfer metric\n",
    "\"\"\"TO DO\"\"\"\n",
    "naive_bwt =\n",
    "print(f\"Naive Fine-Tuning Backward Transfer:    {naive_bwt:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DkXUg_tPys9R"
   },
   "source": [
    "## Domain-incremental learning using the data preservation (DP) method:\n",
    "\n",
    "Here, we will use the DP method to preserve training data from Mission-1 (Domain-1) for replay. When the model learns about a new domain (Domain-2), we will replay the preserved data from Mission-1 (Domain-1) to help our model retain the learned knowledge from Mission-1 (Domain-1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RpRzm_pkIx-D"
   },
   "source": [
    "## Define functions for the DP method: (2 pts)\n",
    "\n",
    "Here, we define the functions for the DP algorithm. It preserves training data instances whose average training loss values are above a user-defined percentile in a stratified fashion.\n",
    "\n",
    "Note:\n",
    "\n",
    "Please read train_model() function to see what is returned in loss\\_matix.\n",
    "\n",
    "loss\\_matix has 2 columns (column refers to dim=1): The first column is the average training loss, the last column is the y labels.\n",
    "\n",
    "You will pass loss\\_matix into preserve_data()\n",
    "\n",
    "Hint: You may need to use Boolean indexing in Numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1ISy4PCkI4UW"
   },
   "outputs": [],
   "source": [
    "def get_boundary(loss):\n",
    "    '''\n",
    "    Find the threshold value based on the given loss matrix.\n",
    "\n",
    "    Args:\n",
    "        loss:         Loss matrix\n",
    "    Returns:\n",
    "        threshold:    The threshold value of a user-defined percentile\n",
    "    '''\n",
    "    # Find the value of the 70-th percentile average training loss values\n",
    "    threshold = np.percentile(loss[:, -2], 70)\n",
    "    return threshold\n",
    "\n",
    "\n",
    "def preserve_data(x_data, y_data, _loss_matrix):\n",
    "    '''\n",
    "    Preserve data that have higher average training loss than the threshold value for replay.\n",
    "\n",
    "    Args:\n",
    "        x_data:         x data\n",
    "        y_data:         y labels\n",
    "        _loss_matrix:   The average training loss matrix\n",
    "                  It has 2 columns (column refers to dim=1): The first column is the average training loss, the last column is the y labels.\n",
    "                  Please check the returned value of train_model()\n",
    "    Returns:\n",
    "        x_preserve:     The preserved x data\n",
    "        y_preserve:     The preserved y labels\n",
    "    '''\n",
    "    index_0, index_1, index_2 = [], [], []\n",
    "\n",
    "    # For class 0:\n",
    "    if sum(_loss_matrix[:, -1] == 0) > 0:\n",
    "        # Find the threshold value based on a user-defined percentile for class 0 with the get_boundary() function\n",
    "        # Pass the rows that belong to class 0 to get_boundary()\n",
    "        # hint: You can achieve this using Boolean indexing in Numpy. Google for more information\n",
    "        threshold_0 = get_boundary(\"\"\"TO DO\"\"\")\n",
    "        # For data instances belonging to class 0,\n",
    "        # get a list of indices of data instances whose average training loss values are above the threshold value\n",
    "        # Find the row indices that satisfy two conditions:\n",
    "        # 1. belong to class 0\n",
    "        # 2. average training loss values are greater than or equal to the threshold value\n",
    "        index_0 = list(np.where(\"\"\"TO DO\"\"\")[0])\n",
    "\n",
    "    # For class 1:\n",
    "    if sum(_loss_matrix[:, -1] == 1) > 0:\n",
    "        # Find the threshold value based on a user-defined percentile for class 1 with the get_boundary() function\n",
    "        # Pass the rows that belong to class 1 to get_boundary()\n",
    "        threshold_1 = get_boundary(\"\"\"TO DO\"\"\")\n",
    "        # For data instances belonging to class 1,\n",
    "        # get a list of indices of data instances whose average training loss values are above the threshold value\n",
    "        # Find the row indices that satisfy two conditions:\n",
    "        # 1. belong to class 1\n",
    "        # 2. average training loss values are greater than or equal to the threshold value\n",
    "        index_1 = list(np.where(\"\"\"TO DO\"\"\")[0])\n",
    "\n",
    "    # For class 2:\n",
    "    if sum(_loss_matrix[:, -1] == 2) > 0:\n",
    "        # Find the threshold value based on a user-defined percentile for class 2 with the get_boundary() function\n",
    "        # Pass the rows that belong to class 2 to get_boundary()\n",
    "        threshold_2 = get_boundary(\"\"\"TO DO\"\"\")\n",
    "        # For data instances belonging to class 2,\n",
    "        # get a list of indices of data instances whose average training loss values are above the threshold value\n",
    "        # Find the row indices that satisfy two conditions:\n",
    "        # 1. belong to class 2\n",
    "        # 2. average training loss values are greater than or equal to the threshold value\n",
    "        index_2 = list(np.where(\"\"\"TO DO\"\"\")[0])\n",
    "\n",
    "    x_preserve = x_data[index_0 + index_1 + index_2, :]\n",
    "    y_preserve = y_data[index_0 + index_1 + index_2]\n",
    "    return x_preserve, y_preserve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fBF5f4mDLKOC"
   },
   "source": [
    "## Preserve data: (0.5 pt)\n",
    "\n",
    "Now, we will use the `preserve_data()` function defined above to preserve data from our Mission-1 (Domain-1) training set `(x_train_1, y_train_1)` based on the `loss_matrix` obtained from training the baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tFHQphzSE9rg"
   },
   "outputs": [],
   "source": [
    "# Use preserve_data() to preserve data for replay\n",
    "x_train_1_preserve, y_train_1_preserve = preserve_data(x_train_1, y_train_1, loss_matrix)\n",
    "\n",
    "# Create CustomDataset for the preserved training data\n",
    "\"\"\"TO DO\"\"\"\n",
    "domain1_preserve =\n",
    "# Create DataLoader for domain1_preserve\n",
    "# Halve the batch_size for this DataLoader so that the model will be trained with\n",
    "# 50% of the data from domain1_preserve and 50% of the data from train_2 in each batch\n",
    "# Set shuffle=True\n",
    "\"\"\"TO DO\"\"\"\n",
    "domain1_preserve_loader ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qiR7UR16ygj7"
   },
   "source": [
    "## Continual learning with the DP method: (1.5 pts)\n",
    "\n",
    "Our baseline model is the model that has only learned from Mission-1 (Domain-1). Now, we will apply the DP continual learning algorithm to it to learn data from Mission-2 (Domain-2). You should observe that now our model performs well on both test sets from the two domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sKtZIwfXKXuS"
   },
   "outputs": [],
   "source": [
    "# Random states\n",
    "random.seed(random_state)\n",
    "np.random.seed(random_state)\n",
    "torch.manual_seed(random_state)\n",
    "if device == 'cuda':\n",
    "  torch.cuda.manual_seed(random_state)\n",
    "  torch.cuda.manual_seed_all(random_state)\n",
    "  torch.backends.cudnn.deterministic = True\n",
    "  torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Load the weights of the baseline model to ensure that we apply continual learning\n",
    "# to our model that has only seen data from Mission-1 (Domain-1) (domain_1.pt)\n",
    "\"\"\"TO DO\"\"\"\n",
    "\n",
    "\n",
    "# Train the MLP model with Mission-2 (Domain-2) data and the preserved data\n",
    "tra_acc, val_acc = train_model(mlp, epoch, train_loader_2, valid_loader_2, len(train_2), len(valid_2),\n",
    "                               file_name=f'dp.pt', pre_loader=domain1_preserve_loader)\n",
    "\n",
    "# Load the optimal weights that gives the highest validation accuracy during continual learning\n",
    "\"\"\"TO DO\"\"\"\n",
    "\n",
    "\n",
    "print()\n",
    "print(\"######### Results from Continual Learning Using the DP Method #########\")\n",
    "# Use print_acc() to print the accuracy metrics on the training, validation, and test sets from Mission-2 (Domain-2), and the test set from Mission-1 (Domain-1)\n",
    "\"\"\"TO DO\"\"\"\n",
    "dp_acc_domain1 =\n",
    "\n",
    "print()\n",
    "# Use avg_F_metrics() to print the F1-score metric\n",
    "\"\"\"TO DO\"\"\"\n",
    "\n",
    "\n",
    "print()\n",
    "# Calculate and print the backward transfer metric\n",
    "\"\"\"TO DO\"\"\"\n",
    "dp_bwt =\n",
    "print(f\"DP Backward Transfer:    {dp_bwt:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SHqqLpXNkxz9"
   },
   "source": [
    "# Part 3: Class-Incremental Learning Experiment (5.5 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vQ2oxwfzkxz-"
   },
   "source": [
    "##Load the experimental data and create the datasets and dataloaders:\n",
    "The dataset has been processed into the training, validation, and test sets for the two missions for you. You just need to load the corresponding experimental data.\n",
    "\n",
    "First, we use Numpy to load the experimental data. Then, we create the training, validation, and test sets using the `CustomDataset` class. Finally, we instantiate dataloaders with PyTorch's [DataLoader](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IdOuuaIMkxz-"
   },
   "outputs": [],
   "source": [
    "# Unzip the dataset\n",
    "!unzip \"/content/drive/Shared drives/ECE477 datasets/Assignment6/diabdeep_CIL_data.zip\" -d diabdeep_CIL_data\n",
    "\n",
    "# Check out the experimental data\n",
    "print('\\n', os.listdir('diabdeep_CIL_data/'), '\\n')\n",
    "\n",
    "# Load the data features x from Mission-1 as type \"float32\"\n",
    "\"\"\"TO DO\"\"\"\n",
    "x_train_1 =\n",
    "x_valid_1 =\n",
    "x_test_1 =\n",
    "\n",
    "# Load the data features x from Mission-2 as type \"float32\"\n",
    "\"\"\"TO DO\"\"\"\n",
    "x_train_2 =\n",
    "x_valid_2 =\n",
    "x_test_2 =\n",
    "\n",
    "# Load the labels y from Mission-1 as type \"int64\"\n",
    "\"\"\"TO DO\"\"\"\n",
    "y_train_1 =\n",
    "y_valid_1 =\n",
    "y_test_1 =\n",
    "\n",
    "# Load the labels y from Mission-2 as type \"int64\"\n",
    "\"\"\"TO DO\"\"\"\n",
    "y_train_2 =\n",
    "y_valid_2 =\n",
    "y_test_2 =\n",
    "\n",
    "# Inspect the labels for both missions\n",
    "print(f\"Mission-1 labels: {np.unique(y_test_1)}\")\n",
    "print(f\"Mission-2 labels: {np.unique(y_test_2)}\")\n",
    "\n",
    "# Instantiate the datasets with CustomDataset\n",
    "\"\"\"TO DO\"\"\"\n",
    "train_1 =\n",
    "valid_1 =\n",
    "test_1 =\n",
    "train_2 =\n",
    "valid_2 =\n",
    "test_2 =\n",
    "\n",
    "# Set the batch size to 128\n",
    "batch_size = 128\n",
    "\n",
    "# Instantiate the dataloaders\n",
    "# Note that we shuffle all of our datasets except for train_loader_1\n",
    "# since we need to record each training data instance's training loss\n",
    "# over all epochs for data selection (details later)\n",
    "# Note that we halve the batch size for train_loader_2\n",
    "# since we will train the model with a balanced amount of data from both domains\n",
    "# in each batch for continual learning\n",
    "\"\"\"TO DO\"\"\"\n",
    "train_loader_1 =\n",
    "valid_loader_1 =\n",
    "test_loader_1 =\n",
    "train_loader_2 =\n",
    "valid_loader_2 =\n",
    "test_loader_2 ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DksOWwGzahhg"
   },
   "source": [
    "## Build the baseline model: (1 pt)\n",
    "\n",
    "Here, we train our MLP model with data from Mission-1 only. This is our baseline model that has only seen Mission-1. There are only two classes in Mission-1 (healthy and Type-I diabetic), so we will set the `out_num = 2` for our baseline model.\n",
    "\n",
    "Use the `train_model()` function defined above to train our MLP model.\n",
    "\n",
    "This time, we leave `save_loss = False` since we will use the synthetic data generation (SDG) module to perform generative replay continual learning. Therefore, we won't need a `loss_matrix`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aFi0NCNlahhg"
   },
   "outputs": [],
   "source": [
    "# Random states\n",
    "random.seed(random_state)\n",
    "np.random.seed(random_state)\n",
    "torch.manual_seed(random_state)\n",
    "if device == 'cuda':\n",
    "  torch.cuda.manual_seed(random_state)\n",
    "  torch.cuda.manual_seed_all(random_state)\n",
    "  torch.backends.cudnn.deterministic = True\n",
    "  torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Hyperparameters for our MLP model\n",
    "\"\"\"TO DO\"\"\"\n",
    "in_num =          # The number of input features\n",
    "hidden_num1 = 256     # Number of neurons in hidden layer 1\n",
    "hidden_num2 = 128     # Number of neurons in hidden layer 2\n",
    "hidden_num3 = 128     # Number of neurons in hidden layer 3\n",
    "out_num =            # Number of output classes\n",
    "\n",
    "# Instansiate the model and cast it to `device`\n",
    "\"\"\"TO DO\"\"\"\n",
    "mlp =\n",
    "\n",
    "# Use train_model() to train the mlp model with data from Mission-1\n",
    "# Store the optimal model weights to the file 'mission_1.pt'\n",
    "# Keep default values for verbose=True, save_loss=False, labels=None, pre_loader=None\n",
    "\"\"\"TO DO\"\"\"\n",
    "tra_acc, val_acc ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "onWB02s5ahhh"
   },
   "source": [
    "## Evaluate performance: (1 pt)\n",
    "\n",
    "Now, we evaluate the MLP model's performance on both missions. You should see that our baseline model does a good job on the test set from Mission-1, but it cannot perform on the test set from Mission-2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GD3qxF_7ahhh"
   },
   "outputs": [],
   "source": [
    "# Load the optimal weights that gives the highest validation accuracy during baseline training\n",
    "\"\"\"TO DO\"\"\"\n",
    "\n",
    "\n",
    "# Make a plot for the training and validation accuracy\n",
    "plot_acc(tra_acc, val_acc, 'mission_1_acc')\n",
    "\n",
    "print()\n",
    "print(\"#### Results from Training on Mission 1 Only ####\")\n",
    "# Use print_acc() to print the accuracy metrics on the training, validation, and test sets from Mission-1, and the test set from Mission-2\n",
    "\"\"\"TO DO\"\"\"\n",
    "baseline_acc_mission1 =\n",
    "\n",
    "# Use avg_F_metrics() to print the F1-score metric\n",
    "\"\"\"TO DO\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9QLY45JlDlLi"
   },
   "source": [
    "## Class-incremental learning using the SDG module with Gaussian mixture model estimation (GMME):\n",
    "\n",
    "Here, we will use the SDG module to generate synthetic data representing Mission-1 for replay. We will use the GMME method to model the probability distribution of Mission-1 and sample synthetic data from the learned distribution. When the model learns about a new mission (Mission-2), we will replay the synthetic data to help our model retain the learned knowledge from Mission-1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K8HwXLRZpmO0"
   },
   "source": [
    "## GMME Synthetic data generation function: (1 pt)\n",
    "\n",
    "We use [Gaussian mixture model](https://scikit-learn.org/1.5/modules/mixture.html) to build our synthetic data generation function. This section, we'll build our synthetic data generation function with `sklearn`'s [GaussianMixture](https://scikit-learn.org/1.5/modules/generated/sklearn.mixture.GaussianMixture.html#sklearn.mixture.GaussianMixture)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s2VKWjHlpPs3"
   },
   "outputs": [],
   "source": [
    "def GMME_sample_generation (X_train, X_validation, n_components=None):\n",
    "    '''\n",
    "    Use GMM to generate synthetic data.\n",
    "\n",
    "    Args:\n",
    "        X_train:        Training data\n",
    "        X_validation:   Validation data\n",
    "        n_components:   Number of Gaussian mixture components\n",
    "    Returns:\n",
    "        X_syn:          Generated synthetic data\n",
    "    '''\n",
    "    if not n_components:\n",
    "        # Set a range of various n_components\n",
    "        n_components = np.arange(5, 16, 1)\n",
    "        score_array = np.zeros((len(n_components)))\n",
    "\n",
    "        print(\"fitting GMM models with various components\")\n",
    "        for i, n in enumerate(n_components):\n",
    "            # Fit a GMM to the training data with n mixture components\n",
    "            # Use covariance_type='full' and random_state=random_state\n",
    "            \"\"\"TO DO\"\"\"\n",
    "            gmm =\n",
    "            # Compute the per-sample average log-likelihood of the validation data to find the optimal n_components, check documentation for available functions\n",
    "            \"\"\"TO DO\"\"\"\n",
    "            score_array[i] =\n",
    "\n",
    "        # Set the n_components to the one leading to max log-likelihood\n",
    "        \"\"\"TO DO\"\"\"\n",
    "        n_components =\n",
    "\n",
    "    print(f\"Number of components: {n_components}\")\n",
    "    # Fit the GMM to the training data with n_components mixture components, 'full' covariance_type, and random_state=random_state\n",
    "    \"\"\"TO DO\"\"\"\n",
    "    gmm =\n",
    "\n",
    "    # Sample synthetic data from the GMM\n",
    "    # The amount of synthetic data should be the same as the training data\n",
    "    \"\"\"TO DO\"\"\"\n",
    "    X_syn, y_syn =\n",
    "\n",
    "    return X_syn.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bV8vh8sgMSe0"
   },
   "source": [
    "## Generate synthetic data: (1 pt)\n",
    "\n",
    "Now, we will use the GMME function to generate the synthetic data we need to perform generative replay. The synthetic data represents the probability distribution of Mission-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kWF_MY8C9nwo"
   },
   "outputs": [],
   "source": [
    "# Load the weights of the baseline model (mission_1.pt)\n",
    "\"\"\"TO DO\"\"\"\n",
    "\n",
    "\n",
    "# Use x_train_1 and x_valid_1 to generate synthetic training data gmm_x_syn\n",
    "\"\"\"TO DO\"\"\"\n",
    "gmm_x_syn =\n",
    "\n",
    "# Use the baseline model to give gmm_x_syn their pseudo-labels gmm_y_syn\n",
    "mlp.eval()\n",
    "with torch.no_grad():\n",
    "    x = torch.from_numpy(gmm_x_syn).to(device)\n",
    "    # Get pseudo-labels gmm_y_syn as torch long integers\n",
    "    # Transform the model's output prediction to 0, 1, or 2\n",
    "    \"\"\"TO DO\"\"\"\n",
    "    gmm_y_syn =\n",
    "\n",
    "# Create CustomDataset for the synthetic training data\n",
    "\"\"\"TO DO\"\"\"\n",
    "train_syn =\n",
    "\n",
    "# Create DataLoader for train_syn\n",
    "# Halve the batch_size for this DataLoader so that the model will be trained with\n",
    "# 50% of the data from train_syn and 50% of the data from train_2 in each batch\n",
    "# Set shuffle=True\n",
    "\"\"\"TO DO\"\"\"\n",
    "train_loader_syn ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HcEFg3nXXtB2"
   },
   "source": [
    "## Create a new MLP model:\n",
    "\n",
    "Next, we will apply generative replay continual learning to our baseline model. But first of all, we need to add one more neuron to the output layer so the model can learn a new class. Then, we will train our baseline model with data from Mission-2 while replaying the synthetic data we just generated.\n",
    "\n",
    "Here, we will use the same MLP structure, but we will add one more neuron to the output layer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z3WM3EDvXtB2"
   },
   "outputs": [],
   "source": [
    "class NewMLP(Module):\n",
    "    def __init__(self, in_num, out_num, hidden_num1, hidden_num2, hidden_num3):\n",
    "        super(NewMLP, self).__init__()\n",
    "\n",
    "        # Add the first hidden layer and ReLU activation\n",
    "        \"\"\"TO DO\"\"\"\n",
    "        self.hidden1 =\n",
    "        self.relu1 =\n",
    "\n",
    "        # Add the second hidden layer and ReLU activation\n",
    "        \"\"\"TO DO\"\"\"\n",
    "        self.hidden2 =\n",
    "        self.relu2 =\n",
    "\n",
    "        # Add the third hidden layer and ReLU activation\n",
    "        \"\"\"TO DO\"\"\"\n",
    "        self.hidden3 =\n",
    "        self.relu3 =\n",
    "\n",
    "        # Add the new output layer\n",
    "        \"\"\"TO DO\"\"\"\n",
    "        self.out_new =\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass x forward the first hidden layer and ReLU activation\n",
    "        \"\"\"TO DO\"\"\"\n",
    "\n",
    "        # Pass x forward the second hidden layer and ReLU activation\n",
    "        \"\"\"TO DO\"\"\"\n",
    "\n",
    "        # Pass x forward the third hidden layer and ReLU activation\n",
    "        \"\"\"TO DO\"\"\"\n",
    "\n",
    "        # Pass x forward the new output layer\n",
    "        \"\"\"TO DO\"\"\"\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5HZHYwZIGBnK"
   },
   "source": [
    "## Continual learning with the GMME SDG: (1.5 pts)\n",
    "\n",
    "Our baseline model is the model that has only learned from Mission-1. Now, we will apply the GMME SDG continual learning algorithm to it to learn data from Mission-2. You should observe that now our model performs well on both test sets from the two missions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6T5npGUFWVDB"
   },
   "outputs": [],
   "source": [
    "# Random states:\n",
    "random.seed(random_state)\n",
    "np.random.seed(random_state)\n",
    "torch.manual_seed(random_state)\n",
    "if device == 'cuda':\n",
    "  torch.cuda.manual_seed(random_state)\n",
    "  torch.cuda.manual_seed_all(random_state)\n",
    "  torch.backends.cudnn.deterministic = True\n",
    "  torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Add one more neuron to the output layer\n",
    "\"\"\"TO DO\"\"\"\n",
    "out_num =            # Number of output classes\n",
    "\n",
    "# Re-instansiate the model as the NewMLP class and cast it to `device`\n",
    "\"\"\"TO DO\"\"\"\n",
    "mlp_new =\n",
    "\n",
    "# Load the weights of the baseline model to mlp_new (mission_1.pt)\n",
    "# Note that mlp_new includes one additional output neuron. When loading the weights, ensure that you ignore the additional neuron in mlp_new to avoid errors\n",
    "# Please check the official PyTorch documentation on load_state_dict here:\n",
    "# https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.load_state_dict\n",
    "\"\"\"TO DO\"\"\"\n",
    "\n",
    "\n",
    "# Note that we will plug train_loader_syn in pre_loader and leave save_loss=False\n",
    "tra_acc, val_acc = train_model(mlp_new, epoch, train_loader_2, valid_loader_2, len(train_2), len(valid_2), file_name=f'gmm.pt', pre_loader=train_loader_syn)\n",
    "\n",
    "# Load the optimal weights that gives the highest validation accuracy during continual learning\n",
    "\"\"\"TO DO\"\"\"\n",
    "\n",
    "\n",
    "print()\n",
    "print(\"#### Results from Continual Learning Using the SDG Module with GMME ####\")\n",
    "# Use print_acc() to print the accuracy metrics on the training, validation, and test sets from Mission-2, and the test set from Mission-1\n",
    "\"\"\"TO DO\"\"\"\n",
    "gmme_acc_mission1 =\n",
    "\n",
    "# Use avg_F_metrics() to print the F1-score metric\n",
    "\"\"\"TO DO\"\"\"\n",
    "\n",
    "\n",
    "print()\n",
    "# Calculate and print the backward transfer metric\n",
    "\"\"\"TO DO\"\"\"\n",
    "gmme_bwt =\n",
    "\n",
    "print(f\"GMME Backward Transfer:    {gmme_bwt:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
